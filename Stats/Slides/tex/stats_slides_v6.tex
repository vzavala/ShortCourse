%------------------------------------------------
%     THEMES
%------------------------------------------------

\documentclass[9pt]{beamer}
\usefonttheme{professionalfonts} % using non standard fonts for beamer

\setbeamersize{text margin left=30mm,text margin right=30mm} 

\usetheme{Wisconsin}

\bibliographystyle{ieeetr}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{mathtools}
\usepackage{stackengine}
\usepackage{algorithmic}
\usepackage{epsfig}
\usepackage{lipsum}
\usepackage{hyperref}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{} \footnotesize{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%% Colors
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{green2}{rgb}{0, 0.8, 0}
\definecolor{UWGray}{RGB}{90,90,90}
\definecolor{UWRed}{RGB}{183,1,0}
\setbeamerfont{block title}{size={}}
\setbeamercolor{block title}{fg=UWRed}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\green}{\color{green2}}
\newcommand{\UWgray}{\color{UWGray}}
\newcommand{\UWred}{\color{UWRed}}
\graphicspath{{../figs/}}
%% \graphicspath{{../../FIGS/old_figs/}}               

%% Beamer macrosa
\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}

%% Sungho Macros
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bbeta}{\mathbf{\beta}}
\newcommand{\bepsilon}{\mathbf{\epsilon}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bLambda}{\mathbf{\Lambda}}
\newcommand{\blambda}{\mathbf{\Lambda}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bht}{\hat{\mathbf{t}}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bS}{\mathbf{\Sigma}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bxi}{\boldsymbol{{\xi}}}
\newcommand{\bup}{\boldsymbol{\upsilon}}
\newcommand{\rank}{\mathop{\textrm{\textup{rank}}}}
\newcommand{\diag}{\mathop{\textrm{\textup{diag}}}}
\newcommand{\st}{\mathop{\textrm{\textup{s.t.}}}}
\newtheorem{proposition}{Proposition}

\usepackage{media9}
\usepackage{movie15}
\usepackage{animate}

%-----------------------------------------------------------
%    TITLE PAGE
%-----------------------------------------------------------

\title{\LARGE Fundamentals of Statistics: \\ {\large From Data to Models to Decision-Making}}
\author{Victor M. Zavala} 
\institute[UW-Madison] 
{\small
  Department of Chemical and Biological Engineering\\
  University of Wisconsin-Madison\\
\medskip
\textit{victor.zavala@wisc.edu}
}
\date{} 

\begin{document}

\begin{frame}
  \titlepage
\end{frame}


%%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%%------------------------------------------------
\begin{frame}{Motivation}
As engineers, we often use {\em laws} of physics and chemistry to make {\em decisions}:
      \begin{block}{}
        \begin{itemize}
      \item The discovery of these governing laws has been the result of extensive collection and analysis of observations (data) 
      \item A governing law is often expressed in the form of a mechanistic model
      \item A mechanistic model provides a concise summary of observations (knowledge) that allow us to predict and generalize
      \end{itemize}
      \end{block}
These laws are powerful but only provide limited descriptions of phenomena:
      \begin{block}{}
      \begin{itemize}
      \item Laws are applicable under specific settings (e.g., continuum vs. atomistic) 
      \item Discovering laws and new mechanistic models might be challenging or cost-prohibitive (e.g., climate)
      \end{itemize}
      \end{block}
      \begin{itemize}
      \item Mechanistic predictions will {\em always} face a certain degree of {\em uncertainty} (due to our limited knowledge of the world). 
      \item Despite these limitations, we still want to be able to {\em make decisions}. In fact, we as humans make decisions  in our daily lives with limited use of mechanistic models and (somehow) by accounting for uncertainty. 
      \end{itemize}
\end{frame}
%%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Motivation}



\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/phen_mech_rand}
\end{figure}
\
\end{frame}
%%------------------------------------------------


%
%%------------------------------------------------
\begin{frame}{Motivation}

{\em Statistics} is the branch of mathematics that offers tools to: 
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item Collect, analyze, and extract knowledge (models) from data 
\item Characterize and model the unknown (uncertainty) 
\item Systematically make decisions in the face of uncertainty
\end{itemize}

\begin{block}{}
For an engineering perspective, {\em statistics} aids the discovery and development of mechanistic models and provides complementary (data-driven) modeling capabilities.  
\end{block}

\begin{block}{}
From a scientific perspective, {\em statistics} provides a framework for thinking about the world that can help us understand how humans naturally process data to extract knowledge and to ultimately make decisions. 
\end{block}
\end{frame}
%%------------------------------------------------


%%%------------------------------------------------
\begin{frame}{Random Variables}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item In statistics, we use random variables (RVs) to {\em model} uncertainty. 

\item An RV (denoted as $X$) does not have a known value, often exhibits {\em variability}, and has the following properties: 
\end{itemize}
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item An RV is characterized by a realization set $\omega \in \Omega$ with associated values $x_\omega\in \mathcal{D}_X$ (a.k.a. realizations of $X$).  

\item Here, $\mathcal{D}_X$ is the domain of $X$ (domain covered by realizations $x_\omega$).

\item An RV is characterized by a measure $\mathbb{P}:\Omega\to [0,1]$, which assigns probability to events (combinations of realizations); e.g., $\mathbb{P}(a\leq X\leq b)$.
\end{itemize}
\end{block}
\end{frame}
%%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Random Variables}
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item Measure $\mathbb{P}$ has an associated cumulative density function (cdf) $F_X:\mathcal{D}_X\to [0,1]$. 

\item Cdf assigns a probability to the event that $X$ is below a certain threshold value $x$; i.e., $F_X(x)=\mathbb{P}(X\leq x)$.
\item Cdf has an associated probability density function (pdf) $f_X:\mathcal{D}_X\to \mathbb{R}_+$. 

\item The pdf assigns a probability to the event that $X$ takes a specific value $x$; i.e., $f_X(x)=\mathbb{P}(X=x)$.  
\end{itemize}
\end{block}
\begin{itemize}
\item An RV that has a unique (exhibits no variability) and known value is called a {\em deterministic variable}. 
\item When convenient, we will simplify notation and write $F(x)$ and $f(x)$ to denote cdf and pdf. 
\end{itemize}
\end{frame}
%%------------------------------------------------


%%%------------------------------------------------
\begin{frame}{Random Variables}
\begin{block}{}
{\bf Don't Forget:} A random variable is a {\em model} of a unknown phenomenon. 
\end{block}
\end{frame}
%%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}

\begin{itemize}
 \setlength{\itemsep}{5pt}
\item Consider a reactor under which the reaction $CO+2H_2\leftrightarrow CH_3OH$ takes place
\item Reaction favored (achieves higher conversion $C$) at high pressure ($P$) and low temperature ($T$)
\item Control system maintains $P$ and $T$ at desired conditions
\end{itemize}

\begin{columns}
\begin{column}{0.3\textwidth}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{figstats/gibbs_diagram}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
\mu_{out}^k&=\mu_{in}^k+\gamma^kC,\;k\in K\\
\mu_{tot}&=\sum_{k\in K}\mu_{out}^k\\
a^k&=\left(P\,\frac{\mu_{out}^k}{\mu_{tot}}\right)^{\gamma^k},\;k\in K\\
K_{eq}(T)&=\prod_{k\in K}a^k\\
\log K_{eq}(T)&=-\frac{\Delta H}{RT}+\frac{\Delta S}{R}
\end{align*}
\end{column}

\end{columns}


\begin{block}{}
\begin{itemize}
\item What is unknown (what are sources of uncertainty)?
\end{itemize}
\end{block}
\end{frame}
%%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}

\begin{itemize}
 \setlength{\itemsep}{10pt}
\item Assume {\em pressure} ($P$) varies due to malfunction of control and model this as an RV
\item Outcomes, pdf, and cdf of RV are shown below. How do we interpret these? 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/gibbs_press_pdf_cdf}
\end{figure}

\end{frame}
%%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Types of Random Variables}
RVs are categorized as multivariate vs. univariate and continuous vs. discrete:

\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item A {\em multivariate} RV $X=(X_{1},X_{2},...,X_n)$ has realizations that are vector values $x_{\omega}=(x_{\omega,1},x_{\omega,2},...,x_{\omega,n})\in \mathbb{R}^n$; e.g., temperature, pressure, conversion.
\item A {\em univariate} RV $X$ is a multivariate with $n=1$ and has realizations that are scalar values $x_\omega\in \mathbb{R}$; e.g., temperature.
\item A {\em continuous} RV $X$ is that in which the domain $\mathcal{D}_X$ is continuous; e.g., $X=(X_1,X_2)$ has realizations satisfying $0\leq x_{\omega,1}\leq 1$ and $0\leq x_{\omega,2}\leq 1$.
\item A {\em discrete} RV $X$ is that in which the domain $\mathcal{D}_X$ is discrete; e.g., $X=(X_1,X_2)$ has realizations satisfying $x_{\omega,1}\in \{0,1\}$ and $x_{\omega,2}\in \{0,1\}$.
\end{itemize}
\end{block}
There is a wide range of models of random variables that apply to different categories (e.g., Gaussian is for continuous and Poisson for discrete). We will explore these later.  
\end{frame}
%%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Probability Density of Discrete and Continuous RVs}

A discrete $X$ has a discrete domain $\mathcal{D}_X$ and, as such, its pdf $f_X(x)$ is not a continuous function. The pdf has the following properties:
\begin{block}{}

\begin{align*}
f(x)&\geq 0,\quad x\in \mathcal{D}_X\\
\sum_{x\in D_X}f(x)&=1\\
\mathbb{P}(X\in \mathcal{A})&=\sum_{x\in \mathcal{A}}f(x),\quad  \mathcal{A}\subseteq \mathcal{D}_X.
\end{align*}
\end{block}

A continuous $X$ has a continuous domain $\mathcal{D}_X$ and its pdf $f_X(x)$ is a continuous function. The pdf has the following properties:
\begin{block}{}
\begin{align*}
f(x)&\geq 0,\quad x\in \mathcal{D}_X\\
\int_{x\in \mathcal{D}_X}f(x)dx&=1\\
\mathbb{P}(X\in \mathcal{A})&=\int_{x\in \mathcal{A}}f(x)dx,\quad  \mathcal{A}\subseteq \mathcal{D}_X.
\end{align*}
\end{block}



\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Probability Density of Discrete and Continuous RVs}

\begin{itemize}
\item A discrete RV is easy to handle computationally  (involves summations):
\vspace{0.1in}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item If $\mathcal{D}_X$ is discrete then $\mathbb{P}(X\leq a)=F_X(a)=\sum_{x\in \mathcal{D}_X}f_X(x)\mathbf{1}[x\leq a]$. 
\end{itemize}
\vspace{0.1in}
Here, we use  indicator  function: $\mathbf{1}[x\leq a]=1$ if $x\leq a$ and $\mathbf{1}[x\leq a]=0$ if $x> a$. 
\vspace{0.1in}
\item A continuous RV is difficult to handle computationally (involves integrals) but has useful properties that facilitate analysis:
\vspace{0.1in}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item If $\mathcal{D}_X$ continuous then $\mathbb{P}(X\leq a)=F_X(a)=\int_{x\in \mathcal{D}_X}f_X(x)dx$. 
\item The cdf and pdf are related as $\frac{dF_X(x)}{dx}=f_X(x)$ and thus $\int_{x\in \mathcal{A}}f_X(x)dx=\int_{x\in \mathcal{A}}dF_X(x)$.
\end{itemize}
\vspace{0.1in}
\item Continuous RVs are often approximated using discrete RVs (discretization). This is analogous to approximating continuous time using discrete times. 

\end{itemize}


\end{frame}
%------------------------------------------------

%%------------------------------------------------
\begin{frame}{From Data to Random Variables}
\begin{block}{}
We begin our discussion by considering RVs that are {\em univariate}.
\end{block}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item In practice, we often count with observations ({\em data}) $x_\omega,\, \omega \in \mathcal{S}$. Our objective is to use the data to create a theoretical RV model $X$. 
\item We assume the observation set $\mathcal{S}$ (a.k.a. sample set) is a subset of realization set $\Omega$ (which is usually extremely large). 

\item We can construct a data-driven approximation (a.k.a. empirical or sample approximation) of the domain, cdf, and pdf of $X$:

\begin{itemize}
 \setlength{\itemsep}{10pt}
\item Empirical domain $\hat{D}_X$ is domain covered by observations $x_\omega,\, \omega \in \mathcal{S}$.
\item Theoretical pdf is approximated using empirical pdf: 
\begin{align*}
\hat{f}_X(x)=\frac{1}{S}\sum_{\omega \in \mathcal{S}}\mathbf{1}[x_\omega= x],\; x\in \hat{D}_X
\end{align*}
i.e., this is frequency at which $X$ takes a value $x$ (normalized by $S=|\mathcal{S}|$). 
\item Theoretical cdf is approximated using empirical cdf:
\begin{align*}
\hat{F}_X(x)=\frac{1}{S}\sum_{\omega \in \mathcal{S}}\mathbf{1}[x_\omega\leq x],\; x\in \hat{D}_X
\end{align*}
i.e., this is frequency at which $X$ takes a value below $x$ (normalized by $S=|\mathcal{S}|$). 
\end{itemize}
\end{itemize}
\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}

\begin{itemize}
\item Comparison of empirical and theoretical pdfs and cdfs for reactor pressure
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/gibbs_press_pdf_cdf_fit}
\end{figure}

\end{frame}
%%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Summarizing Statistics (Basic)}

\begin{itemize}
 \setlength{\itemsep}{5pt}
\item Pdf and cdf are {\em functions} that fully characterize an RV $X$. However, in practice, we might be interested in using values (and not functions) to describe $X$.  
\item This is done by using {\em summarizing statistics} (a.k.a. descriptive statistics).  Popular summarizing statistics are the expected value and variance:
\end{itemize}
\begin{block}{}
For a discrete RV we have:
\begin{itemize}
\item {\em Expected Value (measure of magnitude):} $\mathbb{E}_X:=\sum_{x\in {\mathcal{D}}_X}x{f}_X(x)$

\item {\em Variance and Standard Deviation (measure of variability):} 
\begin{align*}
\mathbb{V}_X=\sum_{x\in {\mathcal{D}}_X}f_X(x)(x-\mathbb{E}_X)^2,\qquad \mathbb{SD}_X:=\sqrt{\mathbb{V}_X}
\end{align*}
\end{itemize}
\end{block}

\begin{block}{}
For a continuous RV we have:
\begin{itemize}
\item {\em Expected Value (measure of magnitude):}  $\mathbb{E}_X:=\int_{x\in {\mathcal{D}}_X}x{f}_X(x)dx$
\item {\em Variance and Standard Deviation (measure of variability):} 
\begin{align*}
\mathbb{V}_X&=\int_{x\in {\mathcal{D}}_X}f_X(x)(x-\mathbb{E}_X)^2dx\\
 \mathbb{SD}_X&=\sqrt{\mathbb{V}_X}
\end{align*}
\end{itemize}
\end{block}
When convenient, we will use notation $\mathbb{E}[X]$, $\mathbb{V}[X]$, $\mathbb{SD}[X]$, and so on. 
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Summarizing Statistics (Sample Approximations)}
If we only have observations $x_\omega,\,\omega in \mathcal{S}$, we can approximate summarizing statistics using their sample approximations: 
\begin{block}{}
\begin{itemize}
\item {\em Sample Mean (measure of magnitude):} 
\begin{align*}
\hat{\mathbb{E}}_X:=\sum_{x\in \hat{\mathcal{D}}_X}x\hat{f}_X(x)=\frac{1}{S}\sum_{\omega\in \mathcal{S}}x_\omega
\end{align*}
\item {\em Sample Variance and Standard Deviation (measure of variability):} 
\begin{align*}
\hat{\mathbb{V}}_X:=\sum_{x\in \hat{\mathcal{D}}_X}(x-\hat{E}_X)^2\hat{f}_X(x)=\frac{1}{S}\sum_{\omega\in \mathcal{S}}(x_\omega-\hat{E}_X)^2,\qquad \hat{\mathbb{SD}}_X=\sqrt{\hat{\mathbb{V}}_X}
\end{align*}
\end{itemize}
\end{block}
Intuition tells us that sample approximations improve as we accumulate observations ($\mathcal{S}$ becomes large). We will see later on that this is indeed the case (under some conditions). 
\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}
\begin{itemize}
\item Behavior of sample mean and standard deviation as we increase sample size $S=|\mathcal{S}|$
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/gibbs_press_exp_var}
\end{figure}

\end{frame}
%%%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Summarizing Statistics (Quantiles)}
An important family of summarizing statistics are the quantiles (a.k.a. percentiles). 
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item The quantile is the inverse function of the cdf and, as such, it might be easier to explain it from this perspective.  Consider the following equation for some $\alpha \in [0,1]$:
\begin{align*}
F_X(x)=\mathbb{P}(X\leq x)=\alpha 
\end{align*}
\item A value $x$ that satisfies this equation is the $\alpha$-quantile of the random variable $X$ and is denoted as $\mathbb{Q}_X(\alpha)$. This means that we can express the quantile as:
\begin{align*}
\mathbb{Q}_X(\alpha)=F^{-1}_X(\alpha) 
\end{align*}
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Summarizing Statistics (Quantiles)}
Some important observations about quantiles:
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item Since the cdf can have a ``staircase" form, there might be multiple values of $x$ satisfying $F_X(x)=\alpha$. Consequently, $\alpha$-quantile might be not be unique. 

\item Typically,  the definition of the quantile is refined by looking for the smallest or center values of $x$ satisfying $F_X(x)\geq \alpha$. 

\item Quantiles are related to other summarizing statistics for interest. For instance: 
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item $\mathbb{Q}_{X}(0.5)$ is the {\em center value} of $X$ (a.k.a. the median and denoted as $\mathbb{M}_X$)
\item $\mathbb{Q}_X(1)=\displaystyle\max_{x\in \mathcal{D}_X }x$ is the maximum value of $X$
\item $\mathbb{Q}_{X}(0)=\displaystyle \min_{x\in \mathcal{D}_X }x$ is the minimum value of $X$ 
\end{itemize}
\item We can use empirical cdf $\hat{F}_X(x)$ to estimate empirical quantiles $\hat{\mathbb{Q}}_X(\alpha)$.
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------

%%------------------------------------------------
%%
\begin{frame}{Summarizing Statistics (Moments)}

\begin{itemize}
 \setlength{\itemsep}{5pt}
\item {\em Central moments} are an important family of summarizing statistics

\item The moments of $X$ with pdf $f(x,\theta)$ are given by:
\begin{block}{}
\begin{align*}
m_k:=\mathbb{E}[(X-\mathbb{E}[X])^k], \qquad k=1,2,3,4,...,
\end{align*}
\end{block}
\item The normalized moments of $X$ with pdf $f(x,\theta)$ are given by:
\begin{block}{}
\begin{align*}
m_k:=\frac{\mathbb{E}[(X-\mathbb{E}[X])^k]}{SD[X]^k}, \qquad k=1,2,3,4,...,
\end{align*}
\end{block}

\item First moment is simply $m_1=0$, second moment $m_2=\mathbb{V}[X]$ is variance, third moment $m_3$ is known as skewness, and fourth moment  $m_4$ is kurtosis. 

\item As with expectation and variance, we can use data to construct sample approximations for moments $\hat{m}_k$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}
\begin{itemize}
\item Sample moment approximations with $S=1000$ are $\hat{m}_1=0,\hat{m}_2=638$, $\hat{m}_3=1539$.
\item Theoretical are $\hat{m}_1=0,\hat{m}_2=639$, $\hat{m}_3=0$. What is going on with third moment?
\item Empirical and theoretical quantiles are shown below. How do we interpret this? 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/gibbs_press_quantile}
\end{figure}
\end{frame}
%%&------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Uncertainty Propagation and Mitigation}
\begin{itemize}
 \setlength{\itemsep}{10pt}
\item Uncertainty (and variability) associated with RVs propagate through systems. 

\item Fortunately, we often have the ability to manipulate a system (e.g., via design or control) in order to mitigate the effects of uncertainty. 

\item Consider propagation of $X$ through system $\varphi(X,u)$:
\begin{align*}
Y=\varphi(X,u)
\end{align*}
where $u\in \mathcal{U}$ is a mitigating action (decision) and $Y$ is the system output. 
\item We make the following observations:
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item Output $Y$ is an RV if the input $X$ is an RV.
\item Nature of $Y$ (its cdf, pdf, and domain) depends on system function $\varphi$. Some systems magnify uncertainty and variability while others might damp it. 
\item Nature of $Y$ depends on action $u$.  Can use action to control uncertainty of $Y$.  
\end{itemize}
\end{block}
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Uncertainty Propagation and Mitigation}
Having data $x_\omega,\; \omega \in \mathcal{S}$ and a system model $\varphi$, we can characterize cdf, pdf, domain, and summarizing statistics of $Y$ using the following simulation procedure:
\begin{block}{}
\begin{itemize}
\item For a given decision $u$, perform simulations of the form:
\begin{align*}
y_\omega=\varphi(x_\omega,u),\; \omega \in \mathcal{S}
\end{align*} 
\item Use $y_\omega$ to compute sample approximations of quantities of interest for $Y$ such as:

\begin{itemize}
\item Sample mean:
\begin{align*}
 \hat{\mathbb{E}}_Y=\frac{1}{S}\sum_{\omega \in \mathcal{S}}y_\omega=\frac{1}{S}\sum_{\omega \in \mathcal{S}}\varphi(x_\omega,u)
 \end{align*}
\item Sample variance: 
\begin{align*}
\hat{\mathbb{V}}_Y=\frac{1}{S}\sum_{\omega \in \mathcal{S}}(y_\omega-\hat{\mathbb{E}}_Y)^2
\end{align*} 
\item Empirical cdf: 
\begin{align*}
\hat{F}_Y(y)=\frac{1}{S}\sum_{\omega \in \mathcal{S}}\mathbf{1}[y_\omega \leq y] 
\end{align*}
\end{itemize}
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}
\begin{itemize}
\item Empirical pdf and cdf for pressure (input $X$) and conversion (output $Y$). 
\item Note change in behavior of $Y$ due to nonlinearity of system. 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/gibbs_press_extent}
\end{figure}

\end{frame}
%%%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Decision-Making under Uncertainty}
Consider now that we would like to find a decision $u\in \mathcal{U}$ that controls $Y(u)=\varphi(X,u)$ in some desirable way. This gives rise to a couple of questions:
\begin{block}{}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item If we have a couple of competing decisions $u$ and $u'$ giving rise to random outputs $Y(u)$ and $Y(u')$. How can we tell which one is better? 

\item How can we find the best possible decision $u$?
\end{itemize}
\end{block}
Some observations: 
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item If we assume a {\em deterministic setting} with no uncertainty, then $Y(u)$ and $Y(u')$ will each take a single value and one would select, {\em unambigously},  the one with larger (or smaller) value. For instance, one would select $u$ if $Y(u)\leq Y(u')$.   

\item In a {\em setting under uncertainty} this is no longer possible because $Y(u)$ and $Y'(u)$ have multiple possible outcomes and with different probabilities ($Y(u)$ and $Y'(u)$ are functions)

\item Concept of ``better" under uncertainty is ambiguous and mathematical statement $Y(u)\leq Y(u')$ does not even make sense. 

\item Does $Y(u)\leq Y(u')$ mean that all outcomes of $Y(u)$ are lower than those $Y(u')$? Does it mean that a subset of outcomes are lower? 
\end{itemize}

\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_example.m}}}
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item Can counteract variability in pressure $X$ by operating at low or high temp $u$
\item We compare empirical pdf and cdf for conversion at low $Y(u)$ and high $Y(u')$ temp
\item Should we operate at low or high temperature?
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/Matlab/gibbs_extent_temp.pdf}
\end{figure}

\end{frame}
%%%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Estimation}

Given data $x_\omega, y_\omega,\; \omega \in \mathcal{S}$ available, we now place our attention to the question:
\begin{block}{}
Is the data following a particular pattern (trend)? If there is a pattern, can we model it? 
\end{block}
In this context, by a model, we mean two things: 
\begin{itemize}
 \setlength{\itemsep}{5pt}
\item If we have empirical statistics (e.g., cdf, pdf, mean, variance) obtained from data $x_\omega,y_\omega$, do these match the statistics of a {\em known} RV?
\item If we do not know the system model $\varphi$ that relates $x_\omega$ and $y_\omega$, can we determine this by using input-output data? 
\end{itemize}
{\em Estimation} is task of determining models from data.  Having model will allow us:
\begin{block}{}
\begin{itemize}
\item Determine if the available data is sufficient to say something meaningful about events that have not been observed (e.g., need more data to make a decision?). 
\item Make predictions about other possible events and their respective probabilities (e.g., how likely is an extreme event from happening?)
\item Extract trends that help us summarize the data available (from data to knowledge). 
\item Conduct uncertainty quantification and ultimately make decisions. 
\end{itemize}
\end{block}
Our first step is to postulate an RV model and see if this fits the data. 
\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%
\section{Models of Random Variables}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Model of a Gaussian RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item A wide range of RV models have been developed over the years based on identification of common patterns that emerge in real-life phenomena. 

\item Many phenomena follow the behavior of a normal RV (a.k.a. Gaussian RV). 

\end{itemize}

\begin{block}{}
A Gaussian RV is continuous and has an associated pdf:
\begin{align*}
f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}, \quad  x\in \mathcal{D}_X=\{-\infty\leq x\leq \infty\}
\end{align*}
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Scalar values $\mu\in \mathbb{R},\sigma\in \mathbb{R}$ are parameters that are specific to application of interest.

\item We will seek to tune the parameters to match the RV model to the available data. 

\item We express the fact that an RV $X$ is Gaussian as $X\sim \mathcal{N}(\mu,\sigma^2)$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of a Gaussian RV}


\begin{itemize}
\setlength{\itemsep}{10pt}
\item Pdf tells us the behavior captured by a Gaussian RV: 

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Probability of an outcome $x$ decays exponentially fast as we move from $\mu$
\item Outcome of maximum probability (most likely outcome) is $\mu$
\item Speed of the decay is dictated by $\sigma$
\item Decay in probability is symmetric around $\mu$
\end{itemize}

\item Gaussian model assumes that an outcome $x$ can take any value in domain $(-\infty,\infty)$. 

\item This introduces complications, as many phenomena involve variables that cannot take negative values (e.g., mass) or infinite values (e.g., temperatures).   

\item Gaussian RVs can model a wide range of phenomena (e.g., diffusion). 

\item Moreover, many phenomena have the Gaussian RV as a limiting case (we will show this later). 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of a Gaussian RV \footnotesize{\texttt{compare\_gaussians.m}}}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Here are the pdfs for $\mathcal{N}(\mu,\sigma)$ for different values of $\mu$ and $\sigma$.
\item What do you observe?
\item What happens when $\sigma^2\to 0$?
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{figstats/Matlab/gaussians}
\end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Diffusion Phenomena}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Gaussian RV naturally emerges in diffusion phenomena
\item Consider question: What is probability of finding a particle in a particular location $x$ in the spatial domain $ [-\infty,\infty]$ and at a given time $t\in [0,T]$?
\item One can show that such probability, denotes as $f(x,t)$ solves the diffusion equation:
\begin{align*}
\frac{\partial f(x,t)}{\partial t}=D\frac{\partial^2 f(x,t)}{\partial x^2}
\end{align*}
with boundary conditions:
\begin{align*}
f(x,t)=0,\quad x\in [-\infty,\infty]\\
\int_{-\infty}^{\infty}f(x,t)dx=1,\quad t\in [0,T]
\end{align*}
\item The solution is:
\begin{align*}
f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{x^2}{2\sigma^2}},\quad  x\in [-\infty,\infty]
\end{align*}
with dispersion coefficient $\sigma^2=2Dt$.
\item In other words, the particle position is random and given by $X\sim\mathcal{N}(0,\sigma^2)$ 
\item How does behavior change with diffusivity $D$?
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

A Gaussian RV $X\sim \mathcal{N}(\mu,\sigma^2)$ has many useful properties. For instance:
\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item It expected value and variance are $\mathbb{E}_X=\mu$ and $\mathbb{V}[X]=\sigma^2$. 
\item Any linear transformation $Y=a+bX$ yields a Gaussian RV $Y\sim \mathcal{N}(a+b\mu,b^2\sigma^2)$. This implies that $\mathbb{E}_Y=a+b\mathbb{E}_X$ and $\mathbb{V}_Y=b^2\mathbb{V}_Y$. 
\item Cdf of $Y=a+bX$ satisfies $F_Y(y)=F_X(x)$ for all $y=a+bx$. 
\end{itemize}
\end{block}
Think about implications from an estimation and uncertainty propagation perspective: 
\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item We can estimate  $\mu$ and $\sigma$ from data as $\mu =\hat{\mathbb{E}}_X$ and $\sigma^2=
\hat{\mathbb{V}}[X]$. This is sufficient to create an empirical Gaussian model. 
\item Any linear system $\varphi(X)=a+bX$ will generate a Gaussian output. Moreover, system will shrink variability of $X$ if $b<1$ and will magnify if $b>1$. 
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Mixing Problem}
\begin{itemize}
\item We have input flow  $F_1=10$ (gpm) that can be measured with high accuracy so it is OK to assume this to be deterministic. 
\item We have another input flow $F_2$ (gpm) that cannot be measured with high accuracy and is thus modeled as an RV $\mathcal{N}(20,1)$. 
\item Uncertain flow $F_2$ can be controlled using a valve with coefficient $\kappa \in [0,1]$. 
\end{itemize}
\begin{block}{}
\begin{itemize}
\item What type of RV is output flow $F_3=F_1+\kappa\cdot F_2$? What is its mean and std dev?
\item How does uncertainty in $F_3$ change $\kappa\to 0$ and $\kappa\to 1$? Why?
\end{itemize}
\end{block}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/mixing_diagram}
\end{figure}
\pause
\begin{itemize}
\item We have that $F_3=10+\kappa F_2$ and is thus $F_3$ a linear transformation of $F_2$
\item We thus have that $F_3\sim\mathcal{N}(10+\kappa\cdot 20,\kappa^2\cdot 1)$
\end{itemize}
\end{frame}
%%%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

Cdf of a Gaussian RV is given by:
\begin{block}{}
\begin{align*}
F_X(x)=\frac{1}{2}\left(1+\textrm{erf}\left(\frac{(x-\mu)/\sigma}{\sqrt{2}}\right)\right)\; 
\end{align*}
where $\textrm{erf}:\mathbb{R}\to \mathbb{R}$ is the error function:
\begin{align*}
\textrm{erf}\left(\frac{(x-\mu)/\sigma}{\sqrt{2}}\right)=\frac{2}{\sqrt{\pi}}\int_0^{\frac{(x-\mu)/\sigma}{\sqrt{2}}}e^{-t^2}dt
\end{align*}
\end{block}
Computing cdf involves evaluating an integral that depends on $\mu$ and $\sigma$. 

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Fortunately, one can exploit properties of Gaussian RVs to avoid this issue. 

\item $Z=(X-\mu)/\sigma$ is a linear transformation of $X\sim \mathcal{N}(\mu,\sigma^2)$ and thus $Z\sim \mathcal{N}(0,1)$. 

\item Now note that pdf and cdf of $Z$ are simply:
\begin{block}{}
\begin{align*}
f_Z(z)&=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\\
F_Z\left(z\right)&=\frac{1}{2}\left(1+\textrm{erf}\left(\frac{z}{\sqrt{2}}\right)\right),\; \quad \textrm{erf}\left(\frac{z}{\sqrt{2}}\right)=\frac{2}{\sqrt{\pi}}\int_0^{\frac{z}{\sqrt{2}}}e^{-t^2}dt
\end{align*}
\end{block}
which do not depend on hyperparameters; $Z$ is known as the standard normal RV. 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item From linear transformation properties we have that $F_X(x)=F_Z(z)$ holds for any $z=(x-\mu)/\sigma$ and thus we can evaluate $F_X(x)$ at a given value $x$ by transforming $x$ into $z$ and then evaluate $F_Z(z)$. 

\item Since $F_Z(z)$ does not depend on any parameters, it can be precomputed (values of $F_Z(z)$ are available in software packages). 

\item If we want to compute $Q_X(\alpha)=F^{-1}_X(\alpha)$. As with cdf, we can compute this by using pre-computed quantiles of $Z$, which we denote as $z_\alpha:=F_Z^{-1}(\alpha)$. 

\item Relationship between the quantiles of $X$ and $Z$ is obtained directly from the linear transformation $x=\mu+\sigma z$:
\begin{align*}
Q_X(\alpha)=\mu+\sigma z_\alpha.
\end{align*}
The values $z_\alpha$ are known as the critical values of the standard normal. As with the cdf, these values have been precomputed and are available in software packages. 
\end{itemize}

\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Mixing Problem \footnotesize{\texttt{mixing\_gaussians.m}}}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Recall $F_3\sim\mathcal{N}(10+\kappa\cdot 20,\kappa^2\cdot 1)$
\item Consider $\kappa=1$ and thus flow $F_3\sim\mathcal{N}(30,1)$
\item Quantile functions for  $\mathcal{N}(0,1)$ and $\mathcal{N}(30,1)$ are shown below
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/mixing_gauss}
\end{figure}
\begin{itemize}
\item Quantile of $\mathcal{N}(0,1)$ at $\alpha=0.977$ is  $Q(\alpha)=2$
\item Quantile of  $\mathcal{N}(\mu,\sigma)$ should be $\mu+2\cdot \sigma=32$.  Confirm this is true from plot
\end{itemize}
\end{frame}
%%%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

\begin{itemize}
\setlength{\itemsep}{10pt}

\item Standarization allows us to easily determine probability that $X$ is in specific ranges.

\item Imagine that you precomputed $F_Z(k)=\mathbb{P}(Z\leq k)$ for $k=0,1,2,3,...$. We have:
\begin{block}{}
\begin{align*}
\mathbb{P}(Z\leq 0)=50.0\%&\Longleftrightarrow\mathbb{P}(X\leq \mu)=50.0\%\\
\mathbb{P}(Z\leq 1)=84.1\%&\Longleftrightarrow\mathbb{P}(X\leq \mu+\sigma)=84.1\%\\
\mathbb{P}(Z\leq 2)=97.7\%&\Longleftrightarrow\mathbb{P}(X\leq \mu+2\sigma)=97.7\%\\
\mathbb{P}(Z\leq 3)=99.9\%&\Longleftrightarrow\mathbb{P}(X\leq \mu+3\sigma)=99.9\%
\end{align*}
\end{block}
\item i.e., probability that $X$ is below its mean $\mu$ plus one $\sigma$ is always 84.1\%, probability that it is below $\mu$ plus three $\sigma$ is always 99.9\%, and so on.  
\end{itemize} 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Properties of a Gaussian RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Standarization allows us to easily determine {\em confidence regions} for $X$.

\item Assume probability level $\alpha \in [0,1]$; we can show that critical value $z$ satisfying
\begin{align*}
\mathbb{P}(-z\leq Z\leq z)=1-\alpha
\end{align*}
is $z=F_Z^{-1}(1-\frac{\alpha}{2})$ (we denote this as $z_{1-\frac{\alpha}{2}}$). 
\item In other words, we have that:
\begin{align*}
\mathbb{P}(-z_{1-\frac{\alpha}{2}}\leq Z\leq z_{1-\frac{\alpha}{2}})=1-\alpha
\end{align*} 
\item Using linear transformation property we obtain:
\begin{align*}
\mathbb{P}(\mu-z_{1-\frac{\alpha}{2}}\sigma\leq X\leq \mu+z_{1-\frac{\alpha}{2}}\sigma)=1-\alpha.
\end{align*}
\item i.e.; probability of finding $X\sim\mathcal{N}(\mu,\sigma^2)$ in region $\mu\pm z_{1-\frac{\alpha}{2}}\sigma$ is $1-\alpha$. 
\item This gives an idea of how confident we are of finding $X$ in a given region.

\item Concept of confidence region is important in many topics of statistics. 
\end{itemize}
\end{frame}
%------------------------------------------------

%%%------------------------------------------------
\begin{frame}{Example: Mixing Problem \footnotesize{\texttt{mixing\_gaussians.m}}}
\begin{itemize}
\item In what region do we expect flow $F_3\sim\mathcal{N}(30,1)$ to be with 90\% probability? 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/Matlab/conf_mixing}
\end{figure}
\begin{itemize}
\item We have $1-\alpha=0.9$ and thus $\alpha=0.1$ and quantile for $Q(1-\frac{\alpha}{2})=1.644$.
\item We thus have $F_3\in [30-1.64\cdot 1,30+1.64\cdot 1]=[28.36,31.64]$ with 90\% prob. 
\end{itemize}
\end{frame}
%%%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of an Exponential RV}

Another RV that often appears in applications is the exponential random variable.  

\begin{block}{}
An exponential RV is continuous and has a pdf of the form:
\begin{align*}
f_X(x)=\frac{1}{\beta}e^{-x/\beta},\quad x\in \mathcal{D}_X=\{0\leq x\leq \infty\}.
\end{align*}
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item The only parameter of this model is $\beta \in \mathbb{R}_+$ (a.k.a. scale value). 
\item Reciprocal $\eta=1/\beta$ is known as the intensity and thus the pdf can also be written as $f_X(x)=\eta e^{-\eta\cdot x}$. 
\item We express the fact that $X$ is exponential as $X\sim \textrm{Exp}(\beta)$.
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of an Exponential RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Pdf of exponential RV tells us that:

\begin{itemize} 
\setlength{\itemsep}{10pt}
\item Probability of finding $X$ away from zero decays exponentially fast at rate $\eta$
\item There is zero probability of finding $X$ below zero (pdf is asymmetric). One can think of an exponential RV as one side of a Gaussian RV. 
\end{itemize}
\item Cdf is $F_X(x)=1-e^{-x/\beta}$, expected value and variance are $\mathbb{E}_X=\beta$ and $\mathbb{V}_X=\beta^2$.
\item This RV is often used to model time-dependent phenomena (e.g., {\em failures}). 
\item For instance, $X$ can be used to model time that we have to wait until we observe first occurrence of an event (e.g., engine fails). 
\item In this context, we know average waiting time ($\mathbb{E}_X=\beta$) but actual time is unknown. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of an Exponential RV \footnotesize{\texttt{compare\_exponentials.m}}}
\begin{itemize}
\item Pdfs and cdfs for $\textrm{Exp}(\beta)$ for different values of $\beta$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/exponentials}
\end{figure}
\begin{block}{}
\begin{center}
How to determine $\beta$ from cdf? 
\end{center}
\end{block}
\pause
\begin{itemize}
\item Note that $F_X(x)=1-e^{-1}=0.63$ when $x=\beta$. 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Residence Time in Mixing System}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item  Mixed system with volume $V$ with input and output flow $F$
\item At time $t=0$ we inject particles so that input concentration is $C_0$.
\item The particle concentration in system at time $t$ is $C(t)$
\end{itemize}
\begin{block}{}
For how long will a particle reside in the system?  What factors influence this time? 
\end{block}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.3\textwidth]{figstats/particle_diagram}
\end{figure}
\pause
\begin{itemize}
\item Material balance reveals that:
\begin{align*}
f_T(t)=\frac{C(t)}{C_0}= \frac{1}{\tau}e^{-t/\tau}\;\; \textrm{with}\;\; \tau=V/F
\end{align*}
\item Interpret $T$ as time required for particle to exit system (residence time)
\item Pdf $f_T(t)$ is interpreted as fraction of particles exiting at time $T=t$
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Residence Time in Mixing System}

\begin{block}{}
\begin{align}
f_T(t)=\frac{C(t)}{C_0}= \frac{1}{\tau}e^{-t/\tau}\;\; \textrm{with}\;\; \tau=V/F
\end{align}
\end{block}
\begin{itemize}
\item Balance suggests that $T\sim\textrm{Exp}(\tau)$  
\item We thus have mean of residence time is $\mathbb{E}[T]=\tau=V/F$ and variance is $\mathbb{V}[T]=(V/F)^2$.  What is effect of $V$ and $F$? 
\item Fraction of particles that have exited up to time $t$ is $\mathbb{P}(T\leq t)=F_T(t)$. 
\item Note that $F_T(t)=\int_{0}^t(1/\tau)e^{-t/\tau}dt=(1-e^{-t/\tau})$ and thus $F(0)=0$ and $F(\infty)=1$.
\item Fraction of particles that are still in the system at time $t$  is $\mathbb{P}(T>t)=1-F_T(t)$. This function is known as the survival function. 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of a Gamma RV}
Gamma RV is a generalization of exponential RV that has a pdf of the form:
\begin{block}{}
\begin{align*}
f_X(x)=\frac{1}{\beta^\alpha \Gamma(\alpha)}e^{-x/\beta}x^{\alpha-1},\; x\in \mathcal{D}_X=\{0\leq x\leq \infty\}.
\end{align*}
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item The pdf has two hyperparameters $\alpha,\beta\in \mathbb{R}_+$
\item $\Gamma(\alpha)$ is the gamma function (for integer $\alpha\geq 1$ we have $\Gamma(\alpha)=(\alpha-1)!$). 
\item We express the fact that $X$ is a gamma RV as $X\sim \textrm{Gamma}(\alpha,\beta)$
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of a Gamma RV}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Pdf tells us that one recovers an exponential RV when $\alpha=1$. 
\item Term $x^{\alpha-1}$ introduces a competing (opposite) effect for the exponential decay, giving rise to peak in the pdf. The location of this peak is $x^*=(\alpha-1)\beta$ and is the mode of the RV (point that maximizes $f_X(x)$). 
\item Expected value and variance are $\mathbb{E}_X=\alpha\beta$ and $\mathbb{V}_X=\alpha\beta^2$ (i.e., the parameters can be estimated from data by solving a set of two equations). 
\item In the context of time phenomena, this RV generalizes exponential in that it models amount of time that we have to wait until we observe the $\alpha$-th occurrence of an event. 
\item Consequently, $\alpha$=1 means the first event (as in the exponential RV). 
\item This RV has applications not only in temporal but also in spatial phenomena. For instance, it can be used to model distance traveled until we find $\alpha$-th occurrence of certain type of atom in a molecule.  
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Model of an Gamma RV \footnotesize{\texttt{compare\_gammas.m}}}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Below at the pdfs and cdfs for $\textrm{Gamma}(\alpha,\beta)$ for different values of $\alpha,\beta$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/gamma}
\end{figure}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Note same as exponential for $\alpha=1$.
\item Note emergence of peaks due to competing effects for $\alpha=2$. 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of a Chi-Squared RV}

Chi-Square RV has pdf of the form:
\begin{block}{}
\begin{align*}
f_X(x)=\frac{1}{2^{r/2}\Gamma(r/2)}e^{-x/2}x^{r/2-1},\; x\in \mathcal{D}_X=\{0\leq x\leq \infty\}.
\end{align*}
\end{block}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Pdf has only one parameter $r\in \mathbb{Z}_+$ (a.k.a degrees of freedom).
\item We express fact that $X$ is a chi-squared RV as $X\sim \chi^2(r)$
\item This is a Gamma RV with $\beta=2$ and $\alpha=r/2$ (for a positive integer $r$). 
\item Expected value and variance can be derived from those of the Gamma RV.  
\item A crucial property of a chi-squared RV is that it is related to standard normal RV. In particular, one can show that: 
\begin{align*}
\sum_{i=1}^rX_i^2\sim \chi^2(r)
\end{align*}
if $X_i\sim \mathcal{N}(0,1)$ and $X_i$ are independent. This property will be useful later on. 
\item From relationship with $\mathcal{N}(0,1)$ we can show that $\mathbb{Q}(1-\alpha)$ of $\chi^2(1)$ is equal to $\mathbb{Q}(1-\alpha/2)^2$ of $\mathcal{N}(0,1)$. This is because $\mathbb{P}(-z\leq Z\leq z)=\mathbb{P}(Z^2\leq z)$.  
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of an Chi-Squared RV \footnotesize{\texttt{compare\_chisq.m}}}
\begin{itemize}
\item Compare quantiles $\mathbb{Q}(1-\alpha)$ of $\chi^2(1)$ and $\mathbb{Q}(1-\alpha/2)^2$ of $\mathcal{N}(0,1)$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/quantiles_norm_chi2}
\end{figure}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Model of a Weibull RV}

Weibull RV is a generalization of exponential RV that has pdf:
\begin{block}{}
\begin{align*}
f_X(x)=\frac{\xi}{\beta}\left(\frac{x}{\beta}\right)^{\xi-1}\exp\left[{-\left(\frac{x}{\beta}\right)^\xi}\right],\; x\in \mathcal{D}_X=\{0\leq x\leq \infty\}.
\end{align*}
\end{block}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Pdf has parameters $\xi,\beta\in \mathbb{R}_+$ (a.k.a scale and shape).
\item We express fact that $X$ is a Weibull RV as $X\sim \textrm{Weibull}(\xi,\beta)$.
\item One recovers an exponential RV when $\xi=1$. 
\item Expected value and variance are $\mathbb{E}_X=\beta\Gamma(1+1/\xi)$ and $\mathbb{V}_X=\beta^2\left(\Gamma(1+2/\xi)+\Gamma(1+1/\xi)^2\right)$. Dependence on gamma function makes it difficult to estimate $\xi,\beta$ from these relationships. 
\item Cdf has nice form $F_X(x)=1-e^{(-x/\beta)^\xi}$ ($\xi,\beta$ are often inferred from cdf). Note also that $\mathbb{P}(X\leq \beta)=0.632$ for any $\xi$. 
\item Weibull RV is {\em de facto} model used in failure analysis and was proposed by statisticians Fischer and Tippett. 
\item Turns out that, as in the case of the Gaussian RV, many phenomena have the Weibull RV as a limiting case (we will show this later). 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Model of Weibull RV \footnotesize{\texttt{compare\_weibull.m}}}
\begin{itemize}
\item Pdfs and cdfs for $\textrm{Weibull}(\beta,\xi)$ for different values of $\beta,\xi$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/weibull}
\end{figure}
\begin{itemize}
\item Same as exponential for $\xi=1$.
\item Note emergence of peaks due to competing effects for $\xi=2$. 
\item Note $\mathbb{P}(X\leq \beta)=0.632$ for any $\xi$ (this differentiates it from gamma RV).
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Families of Random Variables}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Exponential, gamma, chi-squared, and weibull RVs are interrelated. In fact, these are captured by generalized gamma model with pdf:
\begin{block}{}
\begin{align*}
F_X(x)=\frac{1}{\beta^{\alpha\xi}\Gamma(\alpha)}\exp\left[-\left(\frac{x-\delta}{\beta}\right)^\xi\right]\xi(x-\delta)^{\alpha\xi-1},\quad x\in \mathcal{D}_X=\{0\leq x\leq \infty\}.
\end{align*}
\end{block}
\item These RVs are known as the Gamma family. 
\item There are three major families of continuous RVs: 
\begin{itemize}
\item Gaussian family (includes Gaussian, LogNormal, and Raleigh)
\item Gamma family (includes exponential, Gamma, Chi-Squared, and Weibull)
\item Ratio family (includes Cauchy, Uniform, Beta, Fisher, Student)
\end{itemize}

\item There is one family for discrete RVs, which includes Uniform (discrete), Bernoulli, Binomial, and Poisson.

\item Each family models different type of phenomena and there exist relationships between RVs within families and across families. 

\end{itemize}
A detailed discussion of the modeling properties of RVs is beyond our scope. Here, we have only discussed the RVs that will be relevant in our subsequent discussion. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Families of Random Variables}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{figstats/familiesRV}
\end{figure}

\end{frame}
%------------------------------------------------


%%%%%%%%%%%%%%%%%
\section{Estimation Techniques}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Estimation Techniques}


\begin{itemize}
\setlength{\itemsep}{5pt}
\item We have now a basic idea of the types of RV models available to model phenomena. We proceed to develop procedures to determine if a RV model fits data at hand. 

\item We define an RV model in terms of its $f_X(x|\theta)$ (or $F_X(x|\theta)$), where $\theta$ are the model parameters. 

\item By estimating the RV model we mean that we seek to find $\theta$ that best fits data.

\end{itemize}

\begin{block}{}
We explore a couple of estimation methods:
\begin{itemize}
\item Point Estimation (Method of Moments and Least-Squares Method)

\item Maximum Likelihood Estimation (MLE)
\end{itemize}
\end{block}

\begin{itemize}
 \setlength{\itemsep}{5pt}
\item First step will be to explore our data and postulate an RV model (e.g., Gaussian or Exponential) based on any patterns exposed. 
\item Second step will be to tune $\theta$ to see if this fits the data satisfactorily. If fit is not adequate, we postulate another model.
\end{itemize}

\end{frame}
%------------------------------------------------

%%------------------------------------------------
%%
\begin{frame}{Method of Moments}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Recall the moments of $X$ (with pdf $f_X(x|\theta)$ and hyperprameters $\theta$) are given by:
\begin{align*}
m_k(\theta):=\mathbb{E}[(X-\mathbb{E}[X])^k], \qquad k=1,2,...,N
\end{align*}
\item Here, we highlight the dependence of the moments on the hyperparameters $\theta$. 

\item Method of moments uses data $x_\omega,\; \omega \in \mathcal{S}$ to obtain sample approximations: 
\begin{align*}
M_k&=\frac{1}{S}\sum_{\omega\in \mathcal{S}}(x_\omega-m)^k, \; k=1,2,...,N
\end{align*}
where $m=\frac{1}{S}\sum_{\omega\in \mathcal{S}}x_\omega$ is the sample mean.

\end{itemize}

\end{frame}
%------------------------------------------------


%%------------------------------------------------
%%
\begin{frame}{Method of Moments}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Our objective is to find $\theta$ that solve matching equations:
\begin{align*}
m_k(\theta)=M_k,\; k=1,2,...,N
\end{align*}
\item In other words, we want to find $\theta$ that matches model and sample moments. 

\item A solution tho these equations can also be found by solving minimization problem:
\begin{align*}
\min_\theta \frac{1}{2}\sum_{k=1}^N(m_k(\theta)-M_k)^2.
\end{align*}
This problem is known as a least-squares problem (seeks to minimize discrepancy between model and sample moments). 

\item Least-squares is preferred when the matching equations do not have a solution. In this case, minimization problem finds $\theta$ that is most compatible with data.  

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Method of Moments}

We illustrate how to estimate $\theta=(\mu,\sigma^2)$ using moment matching for $\mathcal{N}(0,1)$.
\begin{itemize}
\setlength{\itemsep}{10pt}
\item We are given observations $x_\omega,\; \omega \in \mathcal{S}$.
\item Sample mean is $\hat{E}_X=\frac{1}{S}\sum_{s\in\mathcal{S}}x_\omega$ and variance $\hat{V}_X=\frac{1}{S}\sum_{s\in\mathcal{S}}(x_\omega-\hat{\mathbb{E}}_X)^2$. 
\item We have that $m_1(\theta)=\mathbb{E}[X-\mathbb{E}_X]=\mathbb{E}_X-\mu=0$ and we thus estimate $\mu=\hat{\mathbb{E}}_X$.
\item We have that $m_2(\theta)=\mathbb{V}_X=\sigma^2$ and we thus estimate $\sigma^2=\hat{\mathbb{V}}_X$
\end{itemize}
Now consider we wish to estimate $\theta=(\xi,\beta)$ using moment matching for $\textrm{Weibull}(\xi,\beta)$. 
\begin{itemize}
\item We are given observations $x_\omega,\; \omega \in \mathcal{S}$.
\item Sample mean is $\hat{E}_X=\frac{1}{S}\sum_{s\in\mathcal{S}}x_\omega$ and variance $\hat{V}_X=\frac{1}{S}\sum_{s\in\mathcal{S}}(x_\omega-\hat{\mathbb{E}}_X)^2$. 
\item We find $\beta,\xi$ that match $m_1(\theta)$ and $m_2(\theta)$ by solving nonlinear equations:
\begin{align*}
\hat{\mathbb{E}}_X&=\beta\Gamma(1+1/\xi)\\
\hat{\mathbb{V}}_X&=\beta^2\left(\Gamma(1+2/\xi)+\Gamma(1+1/\xi)^2\right)
\end{align*}
\item This is challenging due to complex nature of $\Gamma$ function.  Is there another way?
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Least-Squares Method}

\begin{itemize}

\item Moment functions $m_k(\theta)$ might be too complex for some RVs (e.g., Weibull).

\item In such cases, we can use $F_X(t|\theta)$ to find parameters. For Weibull, we have $F_X(t|\theta)=(1-e^{-(t/\beta)^\xi})$ with $\theta=(\xi,\beta)$. 

\item In the least-squares method, we find $\theta$ that best matches empirical cdf (obtained from data $x_\omega,\; \omega \in \mathcal{S}$). 

\item We propose a set of threshold values $t_k, k=1,2,...,N$ and compute:
\begin{align*}
\hat{F}_k=\frac{1}{S}\sum_{\omega \in \mathcal{S}}\mathbf{1}[x_\omega\leq t_k],\; k=1,2,...,N
\end{align*}

\item  We use these approximations to solve least-squares problem:
\begin{align*}
\min_\theta \frac{1}{2}\sum_{k=1}^N(F_X(t_k|\theta)-\hat{F}_k)^2
\end{align*}
\item If cdf has an exponential form, it is convenient to use log transformations:
 \begin{align*}
\min_\theta \frac{1}{2}\sum_{k=1}^N(\log F_X(t_k|\theta)-\log \hat{F}_k)^2
\end{align*}
\item Least-squares method is general and can be used to match other statistics (e.g., empirical quantiles).

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Weibull using Least-Squares}

Consider we wish to estimate $\theta=(\xi,\beta)$ using least-squares for $\textrm{Weibull}(\xi,\beta)$. 
\begin{itemize}
\item We are given observations $x_\omega,\; \omega \in \mathcal{S}$ and obtain empirical cdfs $\hat{F}(t_k)$ for given $t_k,\, k=1,...,N$. 
\item Recall cdf of Weibull is $F_X(t|\theta)=(1-e^{-(t/\beta)^\xi})$ and notice that:
\begin{align*}
\log(1-F_X(t|\theta))=-(t/\beta)^\xi
\end{align*}
Moreover:
\begin{align*}
\log(-\log(1-F_X(t|\theta)))= \xi \log t-\xi \log \beta
\end{align*}
\item We have $\log t=a\log(-\log(1-F_X(t|\theta)))+b$ with $a=1/\xi$ and $b=\log\beta$. 
\item We want to find $a,b$ that best matches $\log t_k=a\cdot \log(-\log(1-\hat{F}(t_k))+b$. This is done by solving:
\begin{align*}
\min_{a,b}\; \sum_{k=1}^N\left (y_k-(a\cdot x_k+b)\right)^2
\end{align*}
where $x_k=\log(-\log(1-\hat{F}(t_k))$ and $y_k=\log t_k$. 
\item This is a simple linear optimization problem. We will see later on how to solve this. 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Weibull using Least-Squares \footnotesize{\texttt{weibull\_least\_squares.m}}}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/Matlab/lsfit_weibull}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Maximum Likelihood Method}

The maximum likelihood estimation (MLE) method is:

\begin{itemize}
\item Assume we have observations $x_\omega,\, \omega \in \mathcal{S}$ (selected at random).
\item We postulate $f_X(x|\theta)$ for RV $X$ and recall that $f(x_\omega|\theta)$ is probability (likelihood) that $X$ takes  value of observation $x_\omega$.  

\item Consequently, we find $\theta$ that maximize {\em joint} probability that $X$ takes observations $x_\omega,\,\omega \in \mathcal{S}$. This is done by solving maximization problem:
\begin{align*}
\max_\theta\; L(\theta)=\prod_{\omega \in \mathcal{S}}f(x_\omega|\theta).
\end{align*}
Here, $L(\theta)$ is known as the likelihood function. 

\item It is often convenient to solve the equivalent problem:
\begin{align*}
\max_\theta\; \log L(\theta)=\sum_{\omega \in \mathcal{S}}\log f(x_\omega|\theta).
\end{align*}
This problem can be solved by hand when pdf is simple but requires numerical techniques when complex. 
\item We will soon discuss what ``random observations" and ``joint probability" mean. 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Maximum Likelihood for Exponential RV}

\begin{itemize}
\item Want to estimate $\beta$ for $\textrm{Exp}(\beta)$ using observations $x_\omega$. 
\item Pdf of exponential is $f(x|\beta)=\frac{1}{\beta}e^{-x/\beta}$ and thus likelihood function is:
\begin{align*}
L(\beta)&=\left(\frac{1}{\beta}e^{-x_1/\beta}\right)\left(\frac{1}{\beta}e^{-x_2/\beta}\right)\cdots\left(\frac{1}{\beta}e^{-x_S/\beta}\right)\\
&=\frac{1}{\beta^S}\exp \left(-\frac{1}{\beta}\sum_{\omega=1}^Sx_\omega\right).
\end{align*}
\item We find $\beta$ that maximizes log likelihood $\log L(\beta)$:
\begin{align*}
\max_{\beta}\; \log L(\beta)=-S\log \beta -\frac{1}{\beta}\sum_{\omega=1}^Sx_\omega
\end{align*}
\item Solution of this problem is $\displaystyle\hat{\beta}=\frac{1}{S}\sum_{\omega=1}^Sx_\omega$.
\item i.e., best estimate $\hat{\beta}$ is sample mean (which makes sense because $\mathbb{E}[X]=\beta$). 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Maximum Likelihood for Exponential RV \footnotesize{\texttt{exp\_mle.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Plot of $\log L(\beta)$ (obtained with data $x_\omega$ generated from $\textrm{Exp}(2)$). 
\item Note that value of $\beta$ that maximizes $\log L(\beta)$ ($\hat{\beta}$) coincides with true value $\beta=2$. 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/loglike_exp}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Advanced Estimation Techniques}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item If data does not fit a known RV model, one can resort to determine pdf and cdf numerically by using kernel-based (functional) techniques. 

\item Kernel techniques use data $x_\omega,\; \omega \in \mathcal{S}$ to create functional approximation:
\begin{align*}
\hat{f}_h(x) = \frac{1}{S}\sum_{\omega=1}^S K_h (x - x_\omega) 
\end{align*}
where $K_h$ is the kernel function (with parameter $h$). 
\item Here are typical kernel functions and the type of complex behavior they can capture:
\begin{figure}[!htb]
    \centering
	\href{https://en.wikipedia.org/wiki/Kernel_(statistics)}{\includegraphics[width=0.3\textwidth]{figstats/kernelapprox}}
	\href{https://en.wikipedia.org/wiki/Kernel_(statistics)}{\includegraphics[width=0.3\textwidth]{figstats/kernelfunction}}
\end{figure}
\item We will explore later on the working principles behind kernel methods. 

\end{itemize}


\end{frame}
%------------------------------------------------



%------------------------------------------------
%
\begin{frame}{Sampling and Asymptotic Properties}

\begin{itemize}
\item So far, we have assumed that we have data $x_\omega,\; \omega \in \mathcal{S}$. 
\item However, we have not discussed how this data is being {\em collected}.  
\item We also want to know how sample approximations and estimates $\hat{\theta}$ behave as we accumulate data.  
\end{itemize}
We make the following observations:
\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Data samples $x_\omega \in \mathcal{S}$ is a set of observations of $X$ collected from a population $\Omega$ by a defined procedure; i.e., sampling is a data collection procedure. 

\item A data sample sequence $x_\omega \in \mathcal{S}$ is called {\em random} if each sample $x_\omega$ is drawn from the same underlying pdf $f_X(x)$ and if it is drawn {\em independently} from the others; i.e., samples are independent and identically distributed (i.i.d).

\item If sample $x_\omega$ is selected at random, the sample itself is an RV. Consequently, sometimes we denote data sample sequence as a sequence of RVs $X_\omega \in \mathcal{S}$.  

\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Sampling and Asymptotic Properties}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Random sample $X_\omega$ has same probability $1/S$ of being selected and is {\em unbiased}. 

\item Lack of bias indicates that $\mathbb{E}[X_\omega]=\mathbb{E}[X]$ (drawing sample many times and averaging results gives same expected value of actual RV $X$). 

\item Random samples can be used to construct {\em approximation} techniques with powerful asymptotic properties.

\item Collecting data at random is not as easy as it sounds, one must ensure that there is no bias in selecting a sample (i.e., there is no hidden mechanism).  As humans, it is strikingly difficult to pick something randomly. 

\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Sampling and Asymptotic Properties \footnotesize{\texttt{random\_sample.m}}}
\begin{itemize}
\item Below we show a random and nonrandom (biased) sample sequence for $\mathcal{N}(0,1)$ and their corresponding long-term  averages. 
\item For nonrandom, we select a particular element of the sample (this introduces a bias and thus $\mathbb{E}[X_\omega]\neq 0$). 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/random_sample}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Monte Carlo Approximations}

{\em Monte Carlo} (MC) is a set of computational techniques that use {\em random samples} to infer properties of $X$ and derived quantities (e.g., summarizing statistics).  For example, we can use random sample $x_\omega \in \mathcal{S}$ to compute sample approximations:
\begin{block}{}
\begin{itemize}
\item Expectation of $X$: $\hat{\mathbb{E}}_X^S:=\frac{1}{S}\sum_{\omega \in \mathcal{S}}x_\omega\approx \mathbb{E}[X]$
\item Variance of $X$: $\hat{\mathbb{V}}_X^S:=\frac{1}{S}\sum_{\omega \in \mathcal{S}}(x_\omega-\hat{E}_X^S)^2\approx \mathbb{V}[X]$
\item CDF of $X$: $\hat{F}_X^S(x):=\frac{1}{S}\sum_{\omega 
\in \mathcal{S}}\mathbf{1}[x_\omega \leq x]\approx F_X(x)=\mathbb{E}[{\mathbf{1}[X \leq x]}]$
\item Expectation of system $\hat{\mathbb{E}}_\varphi^S:=\frac{1}{S}\sum_{\omega 
\in \mathcal{S}}\varphi(x_\omega,u)\approx \mathbb{E}[{\varphi(X,u)}]$. 
\end{itemize}
\end{block}
Natural questions that emerge here are: 
\begin{block}{}
\begin{itemize}
\item Do the approximations become exact as $S\to \infty$?
\item How accurate are these approximations for finite $S$?
\end{itemize}
\end{block}
\begin{itemize}
\item Most approximations use an expectation function and we thus restrict our discussion to the behavior of $\hat{E}_X^S$. 
\item There are approximation techniques that use systematic (biased) sampling methods (data is not collected at random). 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Law of Large Numbers}

Lets assess quality of MC approximations as $S\to \infty$. 
\begin{itemize}
\item Consider i.i.d. random sample sequence $X_1,X_2,...,X_S$ for RV $X$. Since samples are i.i.d, they have same underlying pdf with expected value $\mathbb{E}[X]$. 
\item This implies that $\mathbb{E}[X_1]=\mathbb{E}[X_2]=\cdots=\mathbb{E}[X_S]=\mathbb{E}[X]$. 
\item Consider MC approximation of $\mathbb{E}[X]$:
\begin{align*}
\hat{\mathbb{E}}_X^S=\frac{1}{S}\sum_{\omega \in \mathcal{S}}X_\omega
\end{align*}
\end{itemize}
\begin{block}{}
The law of large numbers (LLN) states that:
\begin{align*}
\lim_{S\to \infty}\hat{\mathbb{E}}_X^S=\mathbb{E}[X]
\end{align*}
\end{block}
\begin{itemize}
\item LLN is a fundamental result in statistics and is important because it guarantees {\em stable long-run} behavior of random variables.  

\item In other words, if process is truly random, samples fluctuate around $\mathbb{E}[X]$ and average out.  Conversely, if there is a systematic bias, fluctuations will accumulate and process will drift. 

\item LLN implies that MC approximations become asymptotically exact as $S\to \infty$. Importantly, result holds for any random variable (e.g., discrete, continuous, univariate, multivariate). 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Monte Carlo Approximations \footnotesize{\texttt{mc\_approx.m}}}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Consider random samples $x_\omega,\; \omega \in \mathcal{S}$ obtained from $X\sim \textrm{Weibull}(2,1)$
\item MC approximations for $\mathbb{E}[X],\mathbb{E}[\log(X)]$ and $\mathbb{V}[\exp(X)+X^2]$  for different $S$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/convergence_mc}
\end{figure}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Central Limit Theorem}

Lets turn our attention to assessing quality of MC approximations for finite $S$. 
\begin{itemize}
\setlength{\itemsep}{10pt}
\item This can be addressed by using a powerful result in statistics known as the central limit theorem (CLT). 
\item Consider that sample sequence $X_1,X_2,...,X_S$ is i.i.d and has {\em known} expected value $e=\mathbb{E}[X]$ and variance $v^2=\mathbb{V}[X]$. 

\item We know that $X_\omega$ are RVs and so is $\hat{\mathbb{E}}_X^S$. 
\end{itemize}

CLT will answer the following question:
\begin{block}{}
\begin{itemize}
\item What is pdf of $\hat{\mathbb{E}}_X^S$ as $S\to \infty$?  
\end{itemize}
\end{block}
If we know it, we can say something about variability (accuracy) of MC  approximation. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Central Limit Theorem}

\begin{block}{}
CLT states that:
\begin{align*}
\lim_{S\to \infty}\hat{\mathbb{E}}_X^S\sim \mathcal{N}(e,v/\sqrt{S})
\end{align*}
\end{block}

This result is one of the most surprising and useful results in statistics. Let's explain why:

\begin{itemize}
\setlength{\itemsep}{10pt}
\item CLT says that, {\em regardless} of underlying nature of $X$ (e.g., Weibull, Exponential), its sample approximation $\hat{\mathbb{E}}_X^S$ will {\em always} become a Gaussian RV as $S$ increases.  

\item CLT also says that variance of $\hat{\mathbb{E}}_X^S\sim \mathcal{N}(e,v/\sqrt{S})$ shrinks with $S$. In other words, $\hat{\mathbb{E}}_X^S$ becomes more certain as $S$ increases.

\item Implication is that, since we have a cdf and pdf for $\hat{\mathbb{E}}_X^S$, we can compute all quantities of interest for it. For example, we can compute confidence regions:
\begin{align*}
\mathbb{P}\left(e-z_{1-\alpha/2}v/\sqrt{S}\leq \hat{\mathbb{E}}_X^S\leq e+z_{1-\alpha/2}v/\sqrt{S}\right)=1-\alpha.
\end{align*}
Consequently, for given $S$, we know how confident we are that $\hat{\mathbb{E}}_X^S$ is in a region.  

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Central Limit Theorem \footnotesize{\texttt{central\_limit\_theorem.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item We have samples $X_1,X_2...,,X_S$ obtained from $X\sim \textrm{Weibull}(2,1)$.
\item Want distribution of $\hat{\mathbb{E}}_X^S=\frac{1}{S}\sum_{\omega=1}X_\omega$ (denoted as $\bar{X}$) for different values of $S$.
\item Below we show pdf of $\textrm{Weibull}(2,1)$ and of $\hat{\mathbb{E}}_X^S$ for $S=10,100,1000$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/clt_weibull}
\end{figure}
\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Extreme Value Theorem}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item In CLT we are interested in the limiting behavior of {\em sample average} $\hat{\mathbb{E}}_X^S=\frac{1}{S}\sum_{\omega\in \mathcal{S}}X_\omega$. 

\item What if we are interested in a different statistic? For instance, {\em sample maximum}:
\begin{align*}
X_{max}^S=\max\{X_1,X_2,\cdots, X_S\}
\end{align*}

\item There exists a result (analogous to CLT) that characterizes pdf of $X_{max}^S$ as $S\to \infty$.  The result is known as the extreme value theorem (EVT).

\item Consider, as before, an i.i.d. sequence $X_1,X_2,...,X_S$ for RV $X$. 

\end{itemize}

\begin{block}{}
EVT states that:
\begin{align*}
\lim_{S\to \infty}X_{max}^S\sim \textrm{GEV}(a,b,c)
\end{align*}
\end{block}
where $GEV(a,b,c)$ is the generalized extreme value (GEV) RV with parameters $a,b,c$. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Extreme Value Theorem}

GEV RV has a pdf of the form:
\begin{block}{}
\begin{align*}
f(s;\xi) = \begin{cases}(1+c s)^{(-1/c)-1} \exp(-(1+c s)^{-1/c}) & c\neq0 \\
\exp(-s) \exp(-\exp(-s)) & c = 0\end{cases}
\end{align*}
and a cdf of the form:
\begin{align*}
F_X(x)= \begin{cases}\exp(-(1+c s)^{-1/c}) & c\neq0 \\ \exp(-\exp(-s)) & c = 0\end{cases}
\end{align*}
where $s=(x-a)/b$ is a standarized variable. 
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item GEV RV is a general RV that includes Weibull (for $c<0$), Frechet (for $c>0$), and Gumbel (for $c=0$) RVs. 

\item GEV RV is widely used in failure analysis because max operator characterizes peak (extreme) events. 

\item As with CLT,  EVT does not depend on the underlying nature of $X$. 

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Extreme Value Theorem \footnotesize{\texttt{extreme\_value\_theorem.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item We have samples $X_1,X_2...,,X_S$ obtained from $X\sim \mathcal{N}(2,1)$.
\item Want distribution of $X_{max}^S=\max\{X_1,X_2,\cdots, X_S\}$ for different values of $S$.
\item Below we show pdf of $X\sim \mathcal{N}(2,1)$ and of $X_{max}^S$ for $S=10,100,1000$.
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/evt_weibull}
\end{figure}

\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%
\section{Multivariate Statistics}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Multivariate Statistics}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item So far, we have assumed that RV $X$ is univariate and thus has scalar observations $x_\omega \in \mathbb{R}$.  

\item We have also informally mentioned concept of independent and joint RVs in the context of estimation and sampling.  What do these mean?

\item We now consider multivariate RV $X=(X_1,X_2,...,X_n)$ with observations $x_\omega=(x_{\omega,1},x_{\omega,2},...,x_{\omega,n})\in \mathbb{R}^n$; e.g., input-output pair $(X,Y)$ discussed in uncertainty propagation is a multivariate RV with $n=2$.
 
\end{itemize}

Questions that we are interested in answering are:
\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Are there any connections between RVs? Is there a pattern that suggests they vary together? Are they independent of one another?
\item If they are connected, how strong are connections?
\item If there are connected, how does knowledge of one affects uncertainty of the other? 
\item How to analyze connections between many RVs? (e.g., $n$ is in the hundreds)
\item How to generalize previous results from univariate case to multivariate case? 
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Joint PDFs and CDFs}

For simplicity, we consider bivariate RV $X=(X_1,X_2)$. The concepts presented easily generalize to larger $n$.

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Observation $\omega \in \Omega$ of RV $X$ generates observation pair $x_\omega=(x_{\omega,1},x_{\omega,2})$ 

\item We assume that domain of $X$ is a 2-D box: 
\begin{align*}
\mathcal{D}_1&=\{-\infty \leq x_1\leq \infty\}\\
\mathcal{D}_2&=\{-\infty \leq x_2\leq \infty\}\\
\mathcal{D}&=\mathcal{D}_1\times \mathcal{D}_2=\{-\infty \leq x_1\leq \infty,\;  -\infty \leq x_2\leq \infty\}
\end{align*}

\item {\em Joint} pdf and cdf of multivariate RV are:
\begin{align*}
f(x_1,x_2)&=\mathbb{P}(X_1=x_1\,\&\, X_2=x_2),\; (x_1,x_2)\in\mathcal{D}\\
F(x_1,x_2)&=\mathbb{P}(X_1\leq x_1\,\&\, X_2\leq x_2),\; (x_1,x_2)\in\mathcal{D}
\end{align*}
\item Note sign $\&$ inside measure; i.e., $f(x_1,x_2)$ is probability of event in which $X_1$ takes value  $x_1$ {\em and} $X_2$ takes value $x_2$.  

\item Pdf must satisfy $f(x_1,x_2)\geq 0,\; (x_1,x_2)\in \mathcal{D}$ and $\int_{(x_1,x_2)\in \mathcal{D}}f(x_1,x_2)dx_1dx_2=1$.

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Joint PDFs and CDFs}

Consider a box subdomain $\mathcal{A}\subseteq\mathcal{D}$ of the form:
\begin{align*}
\mathcal{A}=\{a_1 \leq x_1\leq b_1 \, \&\, a_2 \leq x_2\leq b_2\}
\end{align*}

\begin{itemize}

\item For discrete RV we have that pdf is discontinuous and:
\begin{align*}
\mathbb{P}(X\in \mathcal{A})&=\sum_{(x_1,x_2)\in \mathcal{A}}f(x_1,x_2)\\
&=\sum_{\omega \in \Omega}f(x_1,x_2)\mathbf{1}[(x_{\omega,1},x_{\omega,2})\in \mathcal{A}].
\end{align*}

\item For continuous RV we have that pdf is continuous and:
\begin{align*}
\mathbb{P}(X\in \mathcal{A})&=\int_{(x_1,x_2)\in \mathcal{A}}f(x_1,x_2)dx_1dx_2\\
&=\int_{a_1}^{b_1}\int_{a_2}^{b_2}f(x_1,x_2)dx_1dx_2\\
&=\int_{a_1}^{b_1}\int_{a_2}^{b_2}dF(x_1,x_2)
\end{align*}
The last expression implies that $\frac{dF(x_1,x_2)}{dx_1dx_2}=f(x_1,x_2)$.

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Control System Reliability}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Reliability of control system depends on lifetime of processor $X_1$ and actuator $X_2$.
\item Probability that lifetime of $X_1$  is $x_1$ and $X_2$ is $x_2$ is given by joint pdf of $X=(X_1,X_2)$:
\begin{align*}
f(x_1,x_2)=\frac{1}{50}e^{-(0.2x_1+0.1x_2)}
\end{align*} 
\item What is probability that system lasts more than 2 years?
\begin{align*}
\mathbb{P}(X_1\geq 2\,\&\,X_2\geq 2)=\int_{2}^{\infty}\int_{2}^{\infty}\frac{1}{50}e^{-(0.2x_1+0.1x_2)}dx_1dx_2=0.549
\end{align*}
\item What is probability that processor lasts more than 5 years and actuator lasts more than 10 years?
\begin{align*}
\mathbb{P}(X_1\geq 5\,\&\,X_2\geq 10)=\int_{5}^{\infty}\int_{10}^{\infty}\frac{1}{50}e^{-(0.2x_1+0.1x_2)}dx_1dx_2=0.135
\end{align*}
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Conditional PDFs and CDFs}

Joint pdf $f(x_1,x_2)$ tells us probability that $(X_1,X_2)$ takes value $(x_1,x_2)$.

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Imagine now we want to know probability that $X_1$ takes $x_1$ {\em given knowledge} that $X_2$ takes value $x_2$ (or other way around). 

\item These probabilities are obtained from {\em conditional pdfs}:
\begin{align*}
f(x_1|x_2)&=\frac{f(x_1,x_2)}{f_2(x_2)},\; x_1\in \mathcal{D}_1\\
f(x_2|x_1)&=\frac{f(x_1,x_2)}{f_1(x_1)},\; x_2\in \mathcal{D}_2
\end{align*}
i.e., $f(x_1|x_2)=\mathbb{P}(X_1=x_1|X_2=x_2)$ and $f(x_2|x_1)=\mathbb{P}(X_2=x_2|X_1=x_1)$.
\item These expressions can also be written as:
 \begin{align*}
{f_2(x_2)}f(x_1|x_2)&={f(x_1,x_2)},\; x_1\in \mathcal{D}_1\\
{f_1(x_1)}f(x_2|x_1)&={f(x_1,x_2)},\; x_2\in \mathcal{D}_2
\end{align*}
i.e., consider we want $\mathbb{P}(a_1\leq X_1\leq b_1|X_2=x_2)$:
\begin{align*}
\mathbb{P}(a_1\leq X_1\leq b_1|X_2=x_2)&=\int_{a_1}^{b_1}f(x_1|x_2)dx_1
\end{align*}

\item Joint pdfs have associated marginal cdfs $F(x_1|x_2)$ and $F(x_2|x_1)$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Marginal PDFs}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Imagine now we want to know probability that $X_1$ takes $x_1$ {\em regardless} of what value $X_2$ takes (or other way around). 

\item These probabilities are obtained from {\em} marginal pdfs. For a continuous RV:
\begin{align*}
f_1(x_1)&=\int_{x_2\in \mathcal{D}_2}f(x_1,x_2)dx_2,\; x_1\in \mathcal{D}_1\\
f_2(x_2)&=\int_{x_1\in \mathcal{D}_1}f(x_1,x_2)dx_1,\; x_2\in \mathcal{D}_2
\end{align*}
i.e., marginal pdfs integrate out effect of RV we ignore (for discrete RV we sum out)
\item Marginals represent:
\begin{align*}
f_1(x_1)&=\mathbb{P}(X_1=x_1|X_2\in \mathcal{D}_2)=\mathbb{P}(X_1=x_1)\\
f_2(x_2)&=\mathbb{P}(X_2=x_2|X_1\in \mathcal{D}_1)=\mathbb{P}(X_2=x_2).
\end{align*}
\item As an example, consider we want $\mathbb{P}(a_1\leq X_1\leq b_1)$; we have that:
\begin{align*}
\mathbb{P}(a_1\leq X_1\leq b_1)&=\int_{a_1}^{b_1}\int_{x_2\in \mathcal{D}_2} f(x_1,x_2)dx_2dx_1=\int_{a_1}^{b_1}f_1(x_1)dx_1
\end{align*}
\item Marginal pdfs have associated marginal cdfs $F_1(x_1)$ and $F_2(x_2)$.

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Independence}

So conditional pdfs tell us how knowledge in one RV resolves uncertainty in the other (i.e., how much knowledge of one is embedded in the other). 
\begin{block}{}
So what if knowledge of one does not resolve uncertainty of the other? 
\end{block}
This gives rise to concept of {\em independence}.

\begin{itemize}
\setlength{\itemsep}{10pt}
\item RVs $X_1$ and $X_2$ are said to be independent if:
\begin{align*}
f(x_1|x_2)&=f_1(x_1),\; x_1\in \mathcal{D}_1\\
f(x_2|x_1)&=f_2(x_2),\; x_2\in \mathcal{D}_2
\end{align*}
\item This implies that:
 \begin{align*}
{f(x_1,x_2)}={f_2(x_2)}f_1(x_1),\quad (x_1,x_2)\in \mathcal{D}
\end{align*}
\item Equivalently:
 \begin{align*}
\mathbb{P}(X_1=x_1\,\&\,X_2=x_2)=\mathbb{P}(X_1=x_1)\mathbb{P}(X_2=x_2)
\end{align*}

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Control System Reliability}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Recall joint pdf of $X=(X_1,X_2)$ is:
\begin{align*}
f(x_1,x_2)=\frac{1}{50}e^{-(0.2x_1+0.1x_2)}
\end{align*} 
\item Probability that $X_1=x_1$ regardless of knowledge that $X_2=x_2$ is:
\begin{align*}
f_1(x_1)=\int_{0}^{\infty}\frac{1}{50}e^{-(0.2x_1+0.1x_2)}dx_2=\frac{1}{5}e^{-0.2x_1}
\end{align*} 
\item Probability that $X_2=x_2$ regardless of knowledge that $X_1=x_1$ is:
\begin{align*}
f_2(x_2)=\int_{0}^{\infty}\frac{1}{50}e^{-(0.2x_1+0.1x_2)}dx_1=\frac{1}{10}e^{-0.1x_2}
\end{align*} 
\item Probability that $X_1=x_1$ given that we know $X_2=x_2$ is:
\begin{align*}
f(x_1|x_2)=\frac{\frac{1}{50}e^{-(0.2x_1+0.1x_2)}}{\frac{1}{10}e^{-0.1x_2}}=\frac{1}{5}e^{-0.2x_1}
\end{align*}
\item Probability that $X_2=x_2$ given that we know $X_1=x_1$ is:
\begin{align*}
f(x_2|x_1)=\frac{\frac{1}{50}e^{-(0.2x_1+0.1x_2)}}{\frac{1}{5}e^{-0.2x_1}}=\frac{1}{10}e^{-0.1x_2}
\end{align*}
\item We conclude that lifetime $X_1$ is independent of knowledge of lifetime $X_2$ (and viceversa). 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Summarizing Statistics for Multivariate RVs}

Computing summarizing statistics for multivariate RVs is similar to doing so for univariate RVs but there are a few key differences that we highlight:
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Joint expectation of $X=(X_1,X_2)$ is a vector $\mathbb{E}[X]=(\mathbb{E}[X_1],\mathbb{E}[X_2])$ with:
\begin{align*}
\mathbb{E}[X_1]=\int_{x_1\in \mathcal{D}_1}x_1f_1(x_1)dx_1,\qquad \mathbb{E}[X_2]=\int_{x_2\in \mathcal{D}_2}x_2f_2(x_2)dx_2.
\end{align*}
\item Joint expectation of $\varphi(X)=\varphi(X_1,X_2)$ is a scalar:
 \begin{align*}
\mathbb{E}[\varphi(X)]=\int_{x_1\in \mathcal{D}_1}\int_{x_2\in\mathcal{D}_2}\varphi(x_1,x_2)f(x_1,x_2)dx_1dx_2
\end{align*}
\item Conditional expectation of $X_1$ (given knowledge $X_2=x_2$) is:
 \begin{align*}
\mathbb{E}[X_1|X_2=x_2]&=\int_{x_1\in \mathcal{D}_1}x_1f(x_1|x_2)dx_1
\end{align*}
\item Conditional expectation of $\varphi(X)=\varphi(X_1,X_2)$ (given knowledge $X_2=x_2$) is:
 \begin{align*}
\mathbb{E}[\varphi(X)|X_2=x_2]&=\int_{x_1\in \mathcal{D}_1}\varphi(x_1,x_2)f(x_1|x_2)dx_1
\end{align*}
\item Expressions for discrete RVs are analogous (replace integrals for sums). 

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Summarizing Statistics for Multivariate RVs}

In multivariate case, concepts of {\em covariance} and {\em correlation} emerge:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Define marginal expectations and variances:
\begin{align*}
\mu_{1}&=\mathbb{E}[X_1]\\
\mu_{2}&=\mathbb{E}[X_2]\\
\sigma_1^2&=\mathbb{V}[X_1]=\mathbb{E}[(X_1-\mu_1)^2]\\
\sigma_2^2&=\mathbb{V}[X_2]=\mathbb{E}[(X_2-\mu_2)^2].
\end{align*}
\item Covariance between $X_1$ and $X_2$ is:
 \begin{align*}
\textrm{Cov}(X_1,X_2)&=\mathbb{E}[(X_1-\mu_1)(X_2-\mu_2)]\\
&=\int_{x_1\in \mathcal{D}_1}\int_{x_2\in\mathcal{D}_2}(x_1-\mu_1)(x_2-\mu_2)f(x_1,x_2)dx_1dx_2.
\end{align*}
\item Define $\sigma_{i,j}=\textrm{Cov}(X_i,X_j)$ and note $\sigma_{2,1}=\sigma_{1,2}$, $\textrm{Cov}(X_1,X_1)=\sigma_1^2$ and $\textrm{Cov}(X_2,X_2)=\sigma_2^2$. 
\item Correlation between $X_1$ and $X_2$ is:
 \begin{align*}
\textrm{Corr}(X_1,X_2)=\frac{\sigma_{1,2}}{\sigma_1\sigma_2}
\end{align*}
\item  Define $\rho_{i,j}=\textrm{Corr}(X_i,X_j)$ and note $\rho_{1,2}\in [-1,1]$, $\rho_{2,1}=\rho_{1,2}$, $\rho_{1,1}=\rho_{2,2}=1$. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Summarizing Statistics for Multivariate RVs}
Covariance and correlation tells us how, {\em on average}, $X_1$ varies with $X_2$: 

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item If $\sigma_{1,2}>0$ ($\rho_{1,2}>0$) we have {\em positive correlation}: The event $X_1>\mu_1$  is, on average, associated with $X_2>\mu_2$ and event $X_1<\mu_1$  is, on average, associated with $X_2<\mu_2$.  In other words, $X_1$ and $X_2$ move in the same direction (on average).

\item If $\sigma_{1,2}<0$  ($\rho_{1,2}<0$)we have negative correlation: The event $X_1>\mu_1$  is, on average, associated with $X_2<\mu_2$ and event $X_1<\mu_1$  is, on average, associated with $X_2>\mu_2$. In other words, $X_1$ and $X_2$ move in opposite directions (on average). 

\item If $\sigma_{1,2}=0$ ($\rho_{1,2}=0$) we have no correlation: On average, changes in $(X_1-\mu_1)$ are not associated with changes in $(X_2-\mu_2)$. If $X_1$ and$X_2$ are independent then $\sigma_{1,2}=0$. However, $\sigma_{1,2}=0$ does not imply independence because variables might move together (but not on average).
\end{itemize}
\end{block}
Presence of correlation reveals emergent trends. For instance, if $X_1$ and $X_2$ are related as $X_2=\alpha X_1$ then $\textrm{Cov}(X_1,X_2)=\alpha \mathbb{V}[X_1]$. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Covariance and Correlation for Reactor}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Consider reactor under which reaction $CO+2H_2\leftrightarrow CH_3OH$ takes place
\item Equilibrium is favored by high pressure ($P$) and low temperature ($T$)
\item Have data for pressure, conversion, and output flow of $CO$, $H_2$, and $CH_3OH$
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{figstats/gibbs_diagram}
\end{figure}

\begin{block}{}
\begin{itemize}
\item Do you expect a positive or negative correlation between conversion and pressure?
 \item How are output flows for $CO$, $H_2$, and $CH_3OH$ related?
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Covariance and Correlation for Gibbs Reactor \footnotesize{\texttt{gibbs\_covariance.m}}}


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/correlation_gibbs}
\end{figure}



\end{frame}
%------------------------------------------------




%------------------------------------------------
%
\begin{frame}{Covariance and Correlation Matrices}

\begin{itemize}
\item Covariance between variables is often expressed in matrix form as: 
\begin{align*}
\textrm{Cov}[X]=\left[\begin{array}{cc}\sigma_{1,1}&\sigma_{1,2}\\ \sigma_{2,1}&\sigma_{2,2}\end{array}\right]
\end{align*}
Matrix is symmetric ($\sigma_{1,2}=\sigma_{2,1}$) and has positive eigenv (it is positive definite).  

\item Covariance matrix (for any dimension $n$) can be computed as:
\begin{align*}
\textrm{Cov}[X]=\mathbb{E}\left[(X-\mathbb{E}[X])(X-\mathbb{E}[X])^T\right]
\end{align*}

\item Correlation between variables is often expressed in matrix form as: 
\begin{align*}
\textrm{Corr}[X]=\left[\begin{array}{cc}1&\rho_{1,2}\\ \rho_{2,1}&1\end{array}\right]
\end{align*}
Matrix is symmetric ($\rho_{1,2}=\rho_{2,1}$) and is positive definite.  

\item Correlation matrix (for any dimension $n$) can be computed as:
\begin{align*}
\textrm{Corr}[X]=D^{-1}\textrm{Cov}(X) D^{-1}
\end{align*}
where $D=\textrm{diag}(\textrm{Cov}[X])$ contains the diagonal elements of $\textrm{Cov}[X]$. 
\item Sample covariance and correlation matrices can be computed from data. 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Covariance and Correlation for Gibbs Reactor \footnotesize{\texttt{gibbs\_covariance.m}}}

\begin{itemize}
\setlength{\itemsep}{3pt}
\item Consider reactor under which reaction $CO+2H_2\leftrightarrow CH_3OH$ takes place
\item Have data for pressure, conversion, and output flow of $CO$, $H_2$, and $CH_3OH$ ($n=5$)
\item Sample covariance and correlation matrices are shown below;
\end{itemize}

\begin{align*}
\hat{\textrm{Cov}}[X]=\left[\begin{array}{ccccc}
 		  1.28   &       1.03   &      -0.97    &     -0.90    &      0.96\\
          1.03     &     1.38    &     -1.05    &     -1.00    &      1.04\\
         -0.97      &   -1.05    &      1.20    &      0.98    &     -0.97\\
         -0.90   &      -1.00    &      0.98   &       1.23   &      -0.97\\
          0.96     &     1.04     &    -0.97    &     -0.97   &       1.21
\end{array}\right]
\end{align*}

\begin{align*}
\hat{\textrm{Corr}}[X]=\left[\begin{array}{ccccc}
1.00   &       0.77   &      -0.78   &      -0.71    &      0.77\\
          0.77    &      1.00    &     -0.81    &     -0.77    &      0.81\\
         -0.78   &      -0.81   &       1.00   &       0.81   &      -0.80\\
         -0.71    &     -0.77   &       0.81    &      1.00     &    -0.79\\
          0.77     &     0.81    &     -0.80    &     -0.79    &      1.00
\end{array}\right]
\end{align*}

\begin{block}{}
\begin{itemize}
\item Do you expect a positive or negative correlation between conversion and pressure?
 \item How are output flows for $CO$, $H_2$, and $CH_3OH$ related?
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Multivariate Gaussian Variables}

Surprisingly enough, there are actually few established models for multivariate RVs.  The most used model is that of the Gaussian RV, which has a wide range of properties:

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Consider a multivariate RV vector $X=(X_1,X_2,...,X_n)$.

\item Denoted as $X\sim \mathcal{N}(\mu,\Sigma)$, where $\mu\in \mathbb{R}^n$ and $\Sigma\in \mathbb{R}^{n\times n}$ are parameters. 

\item RV $X\sim \mathcal{N}(\mu,\Sigma)$ has joint pdf of the form:
\begin{block}{}
\begin{align*}
f_X(x)=f_X(x_1,x_2,...,x_n)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
\end{align*}
\end{block}
\item $|\Sigma|$ is determinant of  $\Sigma$ (product of its eigenvalues) and $\Sigma$ is positive definite. 

\item Domain of $X$ is $\mathcal{D}=[-\infty,\infty]^n$. 

\item Parameters are given by expected value $\mu=\mathbb{E}[X]$ and covariance $\Sigma=\textrm{Cov}[X]$.

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Properties of Multivariate Gaussian RVs}

Consider the case with $n=2$:
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Can show that marginal pdfs of $X$ are Gaussian:
\begin{block}{}
\begin{align*}
f_1(x_1)&=\int_{x_2\in \mathcal{D}_2}f(x_1,x_2)dx_2=\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp \left({\frac{-(x_1-\mu_1)^2}{2\sigma_1^2}}\right)\\
f_2(x_2)&=\int_{x_1\in \mathcal{D}_1}f(x_1,x_2)dx_1=\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp \left({\frac{-(x_2-\mu_2)^2}{2\sigma_2^2}}\right)
\end{align*} 
\end{block}
\item In other words, $X_1\sim\mathcal{N}(\mu_1,\sigma_1^2)$ and $X_2\sim\mathcal{N}(\mu_2,\sigma_2^2)$.
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Properties of Multivariate Gaussian RVs}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Can show that conditional pdfs of $X$ are Gaussian:
\begin{block}{}
\begin{align*}
f(x_1|x_2)&=\frac{1}{\sqrt{2\pi\sigma_{1|2}^2}}\exp \left({\frac{-(x_1-\mu_{1|2})^2}{2\sigma_{1|2}^2}}\right)\\
f(x_2|x_1)&=\frac{1}{\sqrt{2\pi\sigma_{2|1}^2}}\exp \left({\frac{-(x_2-\mu_{2|1})^2}{2\sigma_{2|1}^2}}\right).
\end{align*} 
\end{block}
\item That is, $X_1|X_2\sim \mathcal{N}(\mu_{1|2},\sigma_{1|2})$ and $X_2|X_1\sim \mathcal{N}(\mu_{2|1},\sigma_{2|1})$ with hyperparameters:
\begin{align*}
\mu_{1|2}&=\mu_1+\sigma_{1,2}\sigma_{22}^{-1}(x_2-\mu_2)\\
\sigma_{1|2}&=\sigma_{1,1}-\sigma_{1,2}\sigma_{2,2}^{-1}\sigma_{2,1}\\
\mu_{2|1}&=\mu_2+\sigma_{2,1}\sigma_{1,1}^{-1}(x_1-\mu_1)\\
\sigma_{2|1}&=\sigma_{2,2}-\sigma_{2,1}\sigma_{1,1}^{-1}\sigma_{1,2}
\end{align*}
\item What  if $X_1$ and $X_2$ are independent? What  if they are strongly correlated?
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Uncertainty Reduction in Gibbs Reactor \footnotesize{\texttt{gibbs\_joint.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Consider covariance for pressure $X_1$ and conversion $X_2$ and assume Gaussian.
\item Marginal means are $\mu_1=147$, $\mu_2=0.69$.
\item Covariance matrix is:
\begin{align*}
\textrm{Cov}[X]=\left[\begin{array}{cc}  641.31    &      1.84\\
          1.84     &     0.24\end{array}\right]
\end{align*}
\item Marginal variance for conversion is $\sigma_{2,2}=0.24$ (use this as measure of uncertainty).

\item Since pressure and conversion are correlated, we expect that having knowledge of pressure decreases uncertainty in  conversion. To verify this, we compute the variance of conditional density $f(x_2|x_1)$
\begin{align*}
\sigma_{2|1}^2&=(0.24-1.84\cdot (641.31)^{-1}\cdot 1.84)^2\\
&=0.05
\end{align*}
\item Consequently, uncertainty in conversion is {\em reduced by 80\%} if we know pressure.  
\item In other words,  since pressure and conversion are correlated, data on pressure carries information on conversion (and the other way around). 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Properties of Multivariate Gaussian RVs}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Linear transformation of a multivariate Gaussian is Gaussian: 
\begin{itemize}
\setlength{\itemsep}{5pt}
\item If $X\sim\mathcal{N}(\mu,\Sigma)$, then $Y=AX+b$ is $Y\sim \mathcal{N}(A\mu+b,A\Sigma A^T)$. 
\end{itemize}
\vspace{0.1in}
\item Linear transformation property is used to establish useful properties:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Mixture model:

 If $X_i\sim \mathcal{N}(\mu_i,\sigma_i^2)$, then $Y=\sum_{i=1}^nX_i$ is Gaussian with $Y\sim\mathcal{N}(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2)$. 
 \vspace{0.1in}
\item Multivariate standard normal: 

If $X\sim\mathcal{N}(0,{I})$ then $Y=\sqrt{\Sigma}X+\mu$ is Gaussian with $Y\sim\mathcal{N}(\mu,\Sigma)$.
\end{itemize}
\item Standardization result used to generate samples of $\mathcal{N}(\mu,\Sigma)$ from samples of $\mathcal{N}(0,I)$.
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gaussian Mixture}
\begin{block}{}
\begin{itemize}
\item Have system with random revenue streams $F_1\sim\mathcal{N}(\mu_1,\sigma_1)$ and $F_2\sim\mathcal{N}(\mu_2,\sigma_2)$
\item Can control amount of revenue using $\kappa_1\in [0,1]$ and $\kappa_2\in [0,1]$
\item What is uncertainty of total revenue $F_3$? How is this affected by $\kappa_1$ and $\kappa_2$? 
\end{itemize}
\end{block}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/gaussian_revenue_diagram}
\end{figure}
\pause 
\begin{itemize}
\item We can write $F_3$ as a linear combination of $F_1$ and $F_2$:
\begin{align*}
F_3=\left[\begin{array}{c} \kappa_1\, \kappa_2
\end{array}\right]\left[\begin{array}{c} F_1\\ F_2
\end{array}\right]=\kappa_1F_1+\kappa_2F_2
\end{align*}
\item We thus have that $F_3\sim\mathcal{N}(\mu_3,\sigma_3)$ with:
\begin{align*}
\mu_3&=\left[\begin{array}{c} \kappa_1\; \kappa_2
\end{array}\right]\left[\begin{array}{c} \mu_1\\ \mu_2
\end{array}\right]=\kappa_1\mu_1+\kappa_2\mu_2\\
\sigma_3^2&=\left[\begin{array}{c} \kappa_1\; \kappa_2
\end{array}\right]\left[\begin{array}{cc} \sigma_1^2&0\\0&\sigma_2^2
\end{array}\right]
\left[\begin{array}{c} \kappa_1\\ \kappa_2
\end{array}\right]=\kappa_1^2\sigma_1^2+\kappa_2^2\sigma_2^2
\end{align*}
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Geometry of Multivariate Gaussian Variables}
Understanding geometry of multivariate Gaussian facilitates {\em data visualization}.  

\begin{itemize}
\setlength{\itemsep}{5pt}
\item For $n=2$ we write joint pdf of $X=(X_1,X_2)$ as:
\begin{align*}
f_X(x_1,x_2)=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-U(x_1,x_2)\right)
\end{align*}
where 
\begin{align*}
U(x_1,x_2)=\frac{1}{2(1-\rho^2)}\left[\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}-2\rho\frac{(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2}\right]
\end{align*}

\item If we fix probability $f_X(x_1,x_2)=\alpha$ with $\alpha\in [0,1]$, pdf defines an equation in variables $(x_1,x_2)$.  This is equation of ellipse centered at ($\mu_1,\mu_2$).  This ellipse is known as the $\alpha$-level set of pdf.  

\item Correlation coefficient $\rho=\textrm{Corr}(X_1,X_2)$ dictates {\em orientation of ellipse}: 
\begin{itemize}
\item If $\rho>0$ this is tilted to right
\item If $\rho<0$ this is tilted to  left
\item If $\rho=0$ (e.g., $X_1$ and $X_2$ are independent) ellipse has no tilt. 
\end{itemize}
\item {\em Length of axes} are dictated by $\sigma_1^2$ and $\sigma_2^2$ (variances of $X_1$ and $X_2$).

\item Maximum value of $f_X(x_1,x_2)$ is achieved at $x_1=\mu_1$ and $x_2=\mu_2$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Geometry of Multivariate Gaussian Variables}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item We are interested in box confidence regions $\mathcal{B}$ satisfying:
\begin{align*}
\mathbb{P}(X\in \mathcal{B})=1-\alpha.
\end{align*}
\item For $X\sim \mathcal{N}(\mu,\sigma^2)$, box is:
\begin{align*}
\mathcal{B}=\{x\,|\,x\in [\mu\pm \sqrt{\mathbb{Q}(1-\alpha)} \sigma]\} 
\end{align*}
where $\mathbb{Q}(1-\alpha)$ is the $(1-\alpha)$-quantile of $\chi^2(1)$. 
\item For $X=(X_1,X_2)$ with $X_1\sim \mathcal{N}(\mu_1,\sigma_1^2)$ and $X_2\sim \mathcal{N}(\mu_2,\sigma_2^2)$, box is:
\begin{align*}
\mathcal{B}=\{(x_1,x_2)\,|\,x_1\in [\mu_1\pm \sqrt{\mathbb{Q}(1-\alpha)}\sigma_1]\,\&\,x_2\in [\mu_2\pm \sqrt{\mathbb{Q}(1-\alpha)}\sigma_2]\}.
\end{align*}
\item This box (a.k.a. marginal box) does not capture correlations in $X_1$ and $X_2$. 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Geometry of Multivariate Gaussian Variables}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item  In multivariate Gaussians, observations concentrate in ellipses. We are thus interested in finding {\em ellipsoidal} confidence region $\mathcal{E}$ satisfying:
\begin{align*}
\mathbb{P}(X\in \mathcal{E})=1-\alpha.
\end{align*}
\item For $X\sim \mathcal{N}(\mu,\Sigma)$, the ellipsoidal region is given by:
\begin{align*}
\mathcal{E}=\{x\,|\,(x-\mu)^T\Sigma^{-1}(x-\mu)\leq \mathbb{Q}(1-\alpha)\}.
\end{align*}
where $\mathbb{Q}(1-\alpha)$ is the $(1-\alpha)$-quantile of $\chi^2(n)$.  
\item  Interpretation of region is:
\begin{itemize}
\item If draw sample from $\mathcal{N}(\mu,\Sigma)$, there is probability $1-\alpha$ that it will land in $\mathcal{E}$
\item The larger the $1-\alpha$, the larger the ellipsoid (more likely it is to land in $\mathcal{E}$)
\end{itemize}
\item Tighest box that encloses $\mathcal{E}$ is:
\begin{align*}
\mathcal{B}=\{(x_1,x_2)\,|\,x_1\in [\mu_1\pm \sqrt{\mathbb{Q}(1-\alpha)}\sigma_1]\,\&\,x_2\in [\mu_2\pm \sqrt{\mathbb{Q}(1-\alpha)}\sigma_2]\}.
\end{align*}
where $\mathbb{Q}(1-\alpha)$ is  $(1-\alpha)$-quantile of $\chi^2(n)$ (note difference with marginal box). 
\item Ellipsoidal and enclosing box capture correlations in $X_1$ and $X_2$.
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Geometry of Multivariate Gaussian Variables}

Marginal pdfs, joint pdf and confidence ellipse and boxes for Gaussian with correlation. 
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/gaussian_geom_corre}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Geometry of Multivariate Gaussian Variables \footnotesize{\texttt{geometry\_gaussian.m}}}

Marginal pdfs, joint pdf and confidence ellipse and boxes for Gaussian with no correlation. 
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/gaussian_geom_indep}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Geometry of Multivariate Gaussian Variables \footnotesize{\texttt{samples\_ellipse.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Here are $S=1,000$ observations for Gaussian RV and $95\%$ confidence ellipsoid
\item Total of 942 samples lie inside ellipsoid 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/ellipsoid_points}
\end{figure}

\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%
\section{Data-Driven Modeling}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

When $X,Y$ are correlated, variations in $Y$ align with those in $X$ (there is a {\em trend}). 
\begin{block}{}
What if connection is deep (e.g., variations in $Y$ are {\em caused} by variations in $X$)? 
\end{block}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Consider univariate RVs $Y$ and $X$ and postulate that any behavior in $Y$ is due to a systematic dependence of $X$:
\begin{align*}
Y=\theta X
\end{align*}
Here, $\theta$ is a parameter that captures a {\em linear} dependence between $X$ and $Y$ . 

\item Use samples $(y_\omega,x_\omega)$ and, based on postulated model, assume they are related as:
\begin{align*}
y_\omega=\theta x_\omega + \epsilon_\omega,\; \omega \in \mathcal{S}.
\end{align*}
\item We introduce hidden RV $\epsilon_\omega\in \mathcal{N}(0,\sigma^2)$ with known $\sigma^2$ to capture behavior that cannot be explained by model (the unknown).  

\item  Note $y_\omega$ is an RV because $\epsilon_\omega$ is an RV. Moreover, $y_\omega$ is linear transformation of $\epsilon_\omega$ and thus $y_\omega$ is Gaussian with $\mathbb{E}[y_\omega|x_\omega,\theta]=\theta x_\omega$ and  $\mathbb{V}[y_\omega|x_\omega,\theta]=\sigma^2$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Recall $f(y_\omega|x_\omega,\theta)$ is probability that $Y=y_\omega$ given that we know $X=x_\omega$ and $\theta$. This is equivalent to assume that $\theta$ and $x_\omega$ are {\em deterministic} (more on this later). 

\item We use a maximum likelihood approach and seek to find $\theta$ that maximizes joint likelihood $\prod_{\omega\in \mathcal{S}}f(y_\omega|x_\omega,\theta)$. This gives:
\begin{block}{}
\begin{align*}
\max_\theta\; \log L(\theta)=\sum_{\omega \in \mathcal{S}} \log f(y_\omega|x_\omega,\theta)
\end{align*}
\end{block}
\item Since $y_\omega\sim \mathcal{N}(\theta x_\omega,\sigma^2)$, we know that:
\begin{block}{}
\begin{align*}
\log f(y_\omega|x_\omega,\theta)=-\log {\sqrt{2\pi\sigma^2}}-{\frac{(y_\omega-\theta x_\omega)^2}{2\sigma^2}}
\end{align*}
\end{block}
\item Terms $\log {\sqrt{2\pi\sigma^2}}$ and $2\sigma^2$ are constants and we thus obtain:
\begin{align*}
\min_\theta \frac{1}{2}\sum_{\omega \in \mathcal{S}} (y_\omega-\theta x_\omega)^2
\end{align*}
This is a least-squares problem and aims to find the estimate ${\theta}$ that minimizes discrepancy between the observed output $y_\omega$ and model prediction ${\theta} x_\theta$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

We denote best parameter that is {\em learned} from data as $\hat{\theta}$. 
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Recall $\hat{\theta}$ minimizes $S(\theta)=\frac{1}{2}\sum_{\omega \in \mathcal{S}} (y_\omega-\theta x_\omega)^2$ and thus satisfies:
\begin{align*}
\frac{\partial S(\hat{\theta})}{\partial \theta}=-\sum_{\omega \in \mathcal{S}}x_\omega (y_\omega-\hat{\theta} x_\omega)=0,\qquad  \frac{\partial^2 S(\hat{\theta})}{\partial \theta^2}>0
\end{align*}
These conditions lead to:
\begin{align*}
\hat{\theta}=\frac{\sum_{\omega \in \mathcal{S}}x_\omega y_\omega}{\sum_{\omega \in \mathcal{S}}x_\omega^2},\qquad \sum_{\omega \in \mathcal{S}}x_\omega^2>0 
\end{align*}
\item Estimate $\hat{\theta}$ captures {\em interactions} in data $x_\omega,y_\omega$. 
\item Estimate $\hat{\theta}$ is {\em unique} and becomes better defined as we add more data. 
\item Recall $y_\omega \sim \mathcal{N}(\hat{\theta} x_\omega,\sigma^2)$, implying that prediction $\hat{\theta} x_\omega$ is the most likely outcome and that the larger $\sigma^2$, the more uncertainty we have in $y_\omega$. 
\item Estimate gives {\em residual noise estimates} $\hat{\epsilon}_\omega =y_\omega-\hat{\theta}x_\omega$. If these estimates follow our assumption $\mathcal{N}(0,\sigma^2)$ then the available data and postulated model is satisfactory. If not, more data or another model is needed (e.g., nonlinear). 

\item From $y_\omega=\hat{\theta}x_\omega+\epsilon_\omega$ we note that model $\hat{\theta}x_\omega$ represents what we know about $y_\omega$ while $\epsilon_\omega$ represents the unknown. Estimation problem thus seeks to extract maximum knowledge from data (minimize the unknown). 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/knowledge_extraction_diagram}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Data-Driven Modeling \footnotesize{\texttt{linear\_est\_example.m}}}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Consider input-output pair data $(Y,X)$ with true relationship $Y=\theta X$ and $\theta=2$
\item Observations $y_\omega$ are corrupted by noise $\epsilon_\omega  \sim \mathcal{N}(0,0.25)$
\item If we use $S=100$ observations, best estimate is $\hat{\theta}=1.96$ 
\end{itemize}
\end{block}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/results_lin_est}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Data-Driven Modeling \footnotesize{\texttt{linear\_est\_example.m}}}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Below we show function $S(\theta)$ for different amounts of data $S=10,100,1000$
\item Note how surface becomes better defined as we add data
\end{itemize}
\end{block}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/Matlab/sharpness_lin_est}
\end{figure}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Data-Driven Modeling \footnotesize{\texttt{linear\_est\_wrongmodel.m}}}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Assume now true relationship $Y=\theta X^2$ and $\theta=2$
\item Below we show model predictions if we postulate wrong model $Y=\theta X$
\end{itemize}
\end{block}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/results_lin_est_wrong}
\end{figure}

\begin{itemize}
\item Residual statistics play key role in determining appropriateness of postulated model.
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

Now generalize linear model to account for influence of multiple inputs. We postulate:
\begin{align*}
Y&=\theta_0+\sum_{i=1}^n\theta_iX_i
\end{align*}
Use samples (data) pairs $(y_\omega,x_\omega)$ and, based on postulated model, we have that:
\begin{align*}
y_\omega=\theta_0+\sum_{i=1}^n\theta_i x_{i,\omega} + \epsilon_\omega,\; \omega \in \mathcal{S}.
\end{align*}
This set of equations can be expressed compactly using matrix notation:
\begin{align*}
\mathbf{y}=\mathbf{X}\theta + \epsilon
\end{align*}
where:
\begin{align*}
\mathbf{y}=\left[\begin{array}{c}y_1\\y_2\\\vdots \\ y_S\end{array}\right]\quad 
\mathbf{X}=\left[\begin{array}{ccccc}1&x_{1,1}&x_{1,2}&\hdots&x_{1,n}\\
1&x_{2,1}&x_{2,2}&\hdots&x_{2,n}\\
\vdots&&&\vdots\\
1&x_{S,1}&x_{S,2}&\hdots&x_{S,n}\\
\end{array}\right]\quad \mathbf{\theta}=\left[\begin{array}{c}\theta_0\\\theta_1\\\vdots \\ \theta_n\end{array}\right]\quad 
\quad \mathbf{\epsilon}=\left[\begin{array}{c}\epsilon_1\\\epsilon_2\\\vdots \\ \epsilon_S\end{array}\right]\quad 
\end{align*}
We assume unknown noise is $\epsilon\sim \mathcal{N}(0,\Sigma)$ with $\Sigma_{\omega,\omega}=\sigma^2$ for $\omega\in\mathcal{S}$. 

\end{frame}
%------------------------------------------------



%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

Best estimate $\hat{\theta}$ is found as the solution of max likelihood problem:
\begin{align*}
\min_{\theta}&\quad \frac{1}{2}(\mathbf{y}-\mathbf{X}\theta)^T (\mathbf{y}-\mathbf{X}\theta)
\end{align*}
Problem can also be written as:
\begin{align*}
\min_{\theta}&\quad \frac{1}{2}\|\mathbf{y}-\mathbf{X}\theta\|^2
\end{align*}
Solution of this problem must satisfy:
\begin{align*}
\frac{\partial S(\theta)}{\partial \theta} &= -\mathbf{X}^T(\mathbf{y}-\mathbf{X}\theta)=0\\
\left|\frac{\partial^2 S(\theta)}{\partial \theta^2}\right| &= \left|\mathbf{X}^T\mathbf{X}\right|>0
\end{align*}
First condition yields:
\begin{align*}
\hat{\theta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.
\end{align*}
Second condition indicates that $\hat{\theta}$ is unique if matrix $\mathbf{X}^T\mathbf{X}$ is positive definite. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

Now note that $\mathbf{y}=\mathbf{X}\theta+\epsilon$ is an RV ($\theta$ is true parameter). Consequently: 
\begin{align*}
\hat{\theta}&=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\theta+\epsilon)\\
&=(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{X})\theta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\epsilon\\
&=\theta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}\epsilon
\end{align*}
Estimate $\hat{\theta}$ is thus an RV (linear transformation of $\epsilon\sim\mathcal{N}(0,\Sigma)$)  and thus:
\begin{align*}
\hat{\theta}\sim\mathcal{N}(\theta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)
\end{align*}
Some observations:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Expected value of estimate is $\mathbb{E}[\hat{\theta}]=\theta$ (estimate is unbiased).
\item Covariance of estimate is $\textrm{Cov}[\theta]=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2$. 
\item As $\sigma^2$ increases so does variance of $\theta$ (covariance is sensitivity of estimates to noise).
\item Matrix $(\mathbf{X}^T\mathbf{X})^{-1}$ can dampen or magnify this sensitivity (more on this later).
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{block}{}
How much of the variability in data $\mathbf{y}$ can be explained by our gained knowledge (model $\hat{\mathbf{y}}=\mathbf{X}^T\hat{\theta}$) and how much of it cannot be explained (unknown $\epsilon$)?
\end{block}
This can be addressed using {\em analysis of variance} (ANOVA).  Total variability of data is:
\begin{align*}
S_{y}=\sum_{\omega\in \mathcal{S}}(y_\omega -\bar{{y}})^2\quad \textrm{with}
\quad \bar{{y}}=\frac{1}{S}\sum_{\omega \in \mathcal{S}}y_\omega 
\end{align*}
Total variability can be decomposed into contributions as:
\begin{align*}
S_{y}=\underbrace{\sum_{\omega\in \mathcal{S}}(\hat{y}_\omega -\bar{y})^2}_{S_m}+\underbrace{\sum_{\omega\in \mathcal{S}}(y_\omega -\hat{y}_\omega)^2}_{S_e}
\end{align*}
Here, $S_m$ is known as model sum of squares and $S_e$ is known as sum of squared errors.  Based on these quantities we define index: 
\begin{align*}
R^2&=\frac{S_m}{S_y}=1-\frac{S_e}{S_y}
\end{align*}
This is faction of variability captured by model. Fraction that is left unexplained is $1-R^2={S_e}/{S_y}$; consequently, $R^2\to 1$ indicates model fully explains variability. 
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{block}{}
How confident are we in estimate $\hat{\theta}$ and in model predictions $\hat{\mathbf{y}}=\mathbf{X}\hat{\theta}$?
\end{block}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item We have established that $\hat{\theta}\sim\mathcal{N}(\theta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)$.  We thus have that true $\theta$ lies in ellipsoid $\mathcal{E}$ defined by:
\begin{align*}
(\hat{\theta}-\theta)^T\left(\frac{\mathbf{X}^T\mathbf{X}}{\sigma^2}\right)(\hat{\theta}-\theta)\leq \mathbb{Q}(1-\alpha)
\end{align*}
where $\mathbb{Q}(1-\alpha)$ is $(1-\alpha)$-quantile of $\chi^2(n+1)$.  

\item In most scientific literature, confidence in estimates is reported using marginals:
\begin{align*}
\hat{\theta}_i\sim \mathcal{N}(\theta_i,\sigma^2(\mathbf{X}\mathbf{X})^{-1}_{ii}),\; i=0,...,n
\end{align*}
as:
\begin{align*}
\theta_i=\hat{\theta}_i\pm m_i\;\qquad m_i=\sqrt{\mathbb{Q}(1-\alpha)\sigma^2(\mathbf{X}\mathbf{X})^{-1}_{ii}}\;
\end{align*}
where  $\mathbb{Q}(1-\alpha)$ is $(1-\alpha)$-quantile of $\chi^2(1)$ (this disregards correlations in parameters). 
\item We can construct confidence intervals for model predictions by noticing that $\mathbf{\hat{y}}=\mathbf{X}\hat{\theta}$ and thus:
\begin{align*}
\hat{\mathbf{y}}\sim \mathcal{N}(\mathbf{y},\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\sigma^2)
\end{align*} 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Catalytic Reactor \footnotesize{\texttt{catalytic\_reactor\_lin\_est.m}}}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Given experimental data for pressure $X_1$, temperature $X_2$, and conversion $Y$ 
\item Total of $S=32$ observations
\end{itemize}
\end{block}


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/data_reactor}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Catalytic Reactor \footnotesize{\texttt{catalytic\_reactor\_lin\_est.m}}}

\begin{block}{}
\begin{itemize}
\item We postulate model $Y=\theta_0+\theta_1X_1 + \theta_2 X_2$ with noise model $\epsilon \sim \mathcal{N}(0,\sigma)$
\item Model fit and residual pdf is shown below 
\end{itemize}
\end{block}


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/model_reactor}
\end{figure}

\begin{itemize}
\item $R^2=0.55$ (55\% of total variability is explained by model)
\item Parameters $95\%$ confidence intervals (marginals) are:
\begin{align*}
 \theta_0&\in [69.8858, \,81.8434]\\ 
 \theta_1&\in  [0.0149,\,    0.1366]\\
  \theta_2&\in [1.9946,\,   4.4299]
\end{align*}
\end{itemize}
\begin{block}{}
\begin{center}
Model seems satisfactory, low $R^2$ suggests that measurements are inaccurate. 
\end{center}
\end{block}

\end{frame}
%------------------------------------------------



%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{block}{}
Is the available data sufficient (or too much) to construct model?
\end{block}
Some observations on having sufficient data:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Matrix $\mathbf{X}^T\mathbf{X}$ plays a fundamental role as it contains all input data and determines  sharpness of minimum and variance and sensitivity of estimate $\hat{\theta}$. 

\begin{itemize}
\setlength{\itemsep}{5pt}
\item If one or more eigenvalues of $\mathbf{X}^T\mathbf{X}$ are large, $\hat{\theta}$ is well-defined by the data and its variance is small.  This manifests as a sharp minimum $S(\hat{\theta})$ and low sensitivity.
\item If one or more eigenvalues of $\mathbf{X}^T\mathbf{X}$ are close to zero, $\hat{\theta}$ is ill-defined by the data and variance is large. This manifests as a flat minimum $S(\hat{\theta})$ and high sensitivity.
\item If one eigenvalue of $\mathbf{X}^T\mathbf{X}$ is zero, $\hat{\theta}$ cannot be obtained uniquely from data.  
\end{itemize}
\item Volume of data is not sufficient, we also require {\em quality of data}. 

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Observations do not provide information if redundant ($\mathbf{X}^T\mathbf{X}$ has dependent rows). 
\item If selected inputs $X$ do not explain output $\mathbf{y}$ estimates $\hat{\theta}$ might exhibit high variability (regardless of number of observations). 
\item Using knowledge of application to select input variables is important.  
\end{itemize}
\item Selection of input variables and observations is a topic of {\em design of experiments}. 
\item Inputs $X$ are a.k.a. {\em regressor variables}, {\em explanatory variables}, {\em features}, or {\em descriptors}. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling}

\begin{block}{}
Is the available data sufficient (or too much) to construct model?
\end{block}
Some observations on using excessive data:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item A common issue is that we use too many inputs $X$ to explain output $Y$. This can result in a large number of parameters and {\em overfitting}. 
\item Check that $\hat{\theta}$ obtained with observations $(y_\omega,x_\omega),\; \omega \in \mathcal{S}$ predicts well in an independent set of observations $(y_\omega,x_\omega),\; \omega \in \mathcal{T}$. This procedure is called {\em cross-validation} or out-of-sample testing. 
\item Cross validation will ensure that model is {\em generalizable}. 
\item In linear models, and adjusted $R^2$ index is used to account for number of parameters:
\begin{align*}
R^2_{adj}=1-\frac{S_e}{S_y}\frac{(S-1)}{(S-n)}
\end{align*}
As number of parameters $n$ increases, we have that $R^2_{adj}$ decreases. 
\item Note that the size of confidence ellipsoid $\mathcal{E}(\theta)$ depends on $n$.
\item A strategy to deal with many parameters is {\em regularization} (will be covered later). Regularization seeks to embed {\em prior} knowledge in estimation problem. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Catalytic Reactor \footnotesize{\texttt{catalytic\_reactor\_ellipse.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item $R^2=0.55$, $R^2_{adj}=0.52$ (model does not seem overparameterized)
\item Below we show confidence ellipses for $S=32$ (all data) and $S=16$ (reduced data). 
\item Low impact of data reduction on uncertainty tells us that there redundancy in data. 
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/Matlab/ellipsoids_reactor}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling (Nonlinear)}

Now generalize linear model to general form:
\begin{align*}
y_\omega=g(\theta, x_{\omega}) + \epsilon_\omega,\; \omega \in \mathcal{S}.
\end{align*}
where $g:\mathbb{R}^{n}\times \mathbb{R}^S\to \mathbb{R}$ is the model (function of parameters and inputs). 

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Model can be nonlinear and capture mechanistic relationships between inputs, parameters, and outputs. 

\item In linear case we have $g(\theta,x_\omega)=\sum_{i=1}^n\theta_i x_{i,\omega} + \epsilon_\omega$. 

\item Use an MLE framework to estimate $\theta$:
\begin{align*}
\min_{\theta} \; S(\theta)=\frac{1}{2}\sum_{\omega \in \mathcal{S}}(y_\omega - m_\omega(\theta))
\end{align*}
where $m_\omega(\theta)=g(\theta, x_{\omega})$.  Problem can be expressed in vector form:
\begin{align*}
\min_{\theta} \; \frac{1}{2}\|\mathbf{y} - \mathbf{m}(\theta)\|^2
\end{align*}
\item Solution $\hat{\theta}$ satisfies following set of $n$ nonlinear equations (a.k.a. score functions):
\begin{align*}
\nabla_\theta S(\theta)=0\qquad \Longleftrightarrow\qquad  \nabla_\theta m(\theta)^T(\mathbf{y}-\mathbf{m}(\theta))=0
\end{align*}
\end{itemize}
\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Data-Driven Modeling (Nonlinear)}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Solution $\hat{\theta}$ also satisfies $|\mathbf{H}(\hat{\theta})|>0$ where:
\begin{footnotesize}
\begin{align*}
\mathbf{H}(\theta)=\frac{\partial^2 S(\theta)}{\partial \theta^2}=
\left[\begin{array}{ccccccccc}
\frac{\partial^2 S(\theta)}{\partial \theta_1\partial \theta_1}&\frac{\partial^2 S(\theta)}{\partial \theta_1\partial \theta_2}&\cdots& \frac{\partial^2 S(\theta)}{\partial \theta_1\partial \theta_n}\\
\frac{\partial^2 S(\theta)}{\partial \theta_1\partial \theta_2}&\frac{\partial^2 S(\theta)}{\partial \theta_2\partial \theta_2}&\cdots&\frac{\partial^2 S(\theta)}{\partial \theta_n\partial \theta_2}\\
\vdots & \vdots &\vdots &\vdots\\
\frac{\partial^2 S(\theta)}{\partial \theta_1\partial \theta_n}&\frac{\partial^2 S(\theta)}{\partial \theta_2\partial \theta_n}&\cdots& \frac{\partial^2 S(\theta)}{\partial \theta_n\partial \theta_n}
\end{array}\right]
\end{align*}
\end{footnotesize}
This matrix is known as the {\em Hessian} matrix. 
\item For linear models the Hessian is:
\begin{align*}
\mathbf{H}(\theta)=\mathbf{X}^T\mathbf{X}
\end{align*}
because  $\nabla_\theta m(\theta)=\mathbf{X}$.  
\item As in linear case, eigenvalues of Hessian encode information about data quality and quantity. 
\end{itemize}
\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Nonlinear Estimation}

What is different about nonlinear estimation?
\begin{itemize}
\setlength{\itemsep}{10pt}
\item  Difficult to obtain pdf of $\hat{\theta}$. Typically, this is approximated as:
\begin{align*}
\hat{\theta}\sim\mathcal{N}(\theta,\mathbf{H}(\hat{\theta})^{-1}\sigma^2)
\end{align*}
The approximation is accurate if nonlinearity of model $m(\theta)$ is not too strong. 
\item Function $S(\theta)$ might have multiple points satisfying optimality conditions.  
\item Problems are often solved using local search algorithms (based on Newton's method) and thus initial guess of $\hat{\theta}$ influences estimate found.  One can also resort to using global search algorithms. 
\item Hessian can be difficult to compute but one can approximate it as $H(\theta)\approx \nabla_\theta m(\theta)^T\nabla_\theta m(\theta)$ (a.k.a. Gauss-Newton approximation).
\item Modern modeling languages can compute exact Hessians (e.g., JuMP, Pyomo, Casadi). 
\item Surprisingly, standard tools such as Matlab do not provide efficient capabilities to compute Hessians. 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hessian of Heat Capacity Problem}

\begin{itemize}
\item You would like to create a model to predict heat capacity as a function of temperature:
\begin{align*}
C=\theta_0+\theta_1 T
\end{align*}
\item Assume that you have one experimental data point available $(T_1,C_1)$
\end{itemize}
\begin{block}{}
\begin{center}
Can $\theta_0,\theta_1$ be estimated {\em uniquely} from $(T_1,C_1)$? Why? 
\end{center}
\end{block}
\pause
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/heatcapacity_diag}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hessian of Heat Capacity Problem}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Lets analyze how to reach conclusion by analyzing Hessian. Estimation problem is:
\begin{align*}
\min_{\theta_0,\theta_1} \; S(\theta_0,\theta_1)=\frac{1}{2}(C_1-\theta_0-\theta_1T_1)^2
\end{align*}
\item First-order derivatives of $S(\theta)$ are:
\begin{align*}
\frac{\partial S}{\partial \theta_0}&=-(C_1-\theta_0-\theta_1 T_1)\\
\frac{\partial S}{\partial \theta_1}&=-T_1(C_1-\theta_0-\theta_1 T_1)\\
\end{align*}
\item Second derivatives of $S(\theta)$ are:
\begin{align*}
\frac{\partial^2 S}{\partial \theta_0 \partial \theta_0}&=1,\quad \frac{\partial^2 S}{\partial \theta_0\partial \theta_1}=T_1\\
\frac{\partial^2 S}{\partial \theta_1 \partial \theta_0}&=T_1,\quad \frac{\partial^2 S}{\partial \theta_1 \partial \theta_1}=T_1^2.
\end{align*}
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hessian of Heat Capacity Problem}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Hessian matrix is thus:
\begin{align*}
H(\theta)=\left[\begin{array}{cc}1&T_1\\ T_1 & T_1^2\end{array}\right]
\end{align*}
\item Determinant of this matrix is:
\begin{align*}
|H(\theta)|=T_1^2-T_1^2=0
\end{align*}
\item Consequently, matrix is singular (it has eigenvalues that are zero)
\item Parameters $\theta_0,\theta_1$ cannot be estimated uniquely, as expected. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hessian of Heat Capacity Problem}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Now imagine that you obtain another experimental data point $(T_2,C_2)$ to estimate parameters. Problem becomes: 
\begin{align*}
\min_{\theta_0,\theta_1} \; S(\theta_0,\theta_1)=\frac{1}{2}(C_1-\theta_0-\theta_1T_1)^2+\frac{1}{2}(C_2-\theta_0-\theta_1T_2)^2
\end{align*}
\item Hessian is:
\begin{align*}
H(\theta)=\left[\begin{array}{cc}2&T_1+T_2\\ T_1+T_2 & T_1^2+T_2^2\end{array}\right]
\end{align*}
\item Determinant is:
\begin{align*}
|H(\theta)|&=2(T_1^2+T_2^2)-(T_1+T_2)^2\\
&=T_1^2-2T_1T_2+T_2^2\\
&=(T_1-T_2)^2
\end{align*}
\item Under what conditions are parameters unique? 
\pause
\item Parameters are unique as long as $T_1\neq T_2$ (data is not redundant). 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction}

\begin{itemize}
\item You would like to find parameters for the Hougen-Watson function:
\begin{align*}
Y=\frac{(\theta_0X_2-X_3/\theta_4)}{(1+\theta_1X_1+\theta_2X_2+\theta_3X_3)}
\end{align*}
\item Model captures effect of competitive reaction and adsorption rates, $Y$ is production rate and $X$ are species concentrations. 
\item Parameters $\theta$ have physical meaning (represent adsorption and reaction rates). 
\item We have $S=13$ data pairs $(y_\omega,x_\omega)$ available and $m=5$ parameters to estimate. 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction \footnotesize{\texttt{hougen\_watson\_reaction.m}}}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Model is highly nonlinear and this can result in high nonlinearity of $S(\theta)$.
\item Below we show behavior of $S(\theta)$ as we span range for $\theta_1$ and $\theta_2$.
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/hougen_span1}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/hougen_span2}
\end{figure}
\end{column}
\end{columns}
\item In nonlinear models, we might see emergence of maxima and minima in $S(\theta)$.

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction \footnotesize{\texttt{hougen\_watson\_reaction.m}}}

\begin{itemize}
\setlength{\itemsep}{3pt}
\item Solution of problem with Matlab built-in function $\texttt{fitnlm}$ gives following fit:
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{figstats/Matlab/hougen_fit_good}
\end{figure}
\item Parameters 95\% confidence intervals are:
\begin{align*}
\theta_0&\in [-0.75, 3.25],\qquad \theta_1\in [-0.04, 0.16]\\
\theta_2&\in [-0.03 ,0.11],\qquad \theta_3\in [-0.06, 0.29],\qquad \theta_4\in [ -0.74,3.12].
\end{align*}
\item Do these intervals make physical sense?
\pause 
\item If we analyze output of Matlab, we realize that algorithm does not report Hessian. Because of this, it is impossible to conclude if estimates are unique. 
\item Check also what happens if you provide a bad initial guess to parameters. 
\end{itemize}


\end{frame}
%------------------------------------------------



%------------------------------------------------
%
\begin{frame}{Prior Knowledge}

\begin{block}{}
How can we incorporate prior (expert, physical) knowledge about parameters in  estimation problem? 
\end{block}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item Prior knowledge helps us eliminate spurious estimates $\hat{\theta}$ (e.g., avoid estimates with no physical meaning). 
\item Prior knowledge helps us reduce number of parameters or narrow space over we search on.
\end{itemize}
We can incorporate prior knowledge in estimation (a.k.a. regularization) by using: 
\begin{itemize}
\item Bounds on parameters (e.g., kinetic parameters are positive):
\begin{align*}
\min_{\theta}& \; \frac{1}{2}\|\mathbf{y}- \mathbf{m}(\theta)\|^2\\ 
\textrm{s.t.}&\; \theta_L\leq \theta\leq\theta_U 
\end{align*}
\item Constraints to fix parameters (e.g., sum of parameters must be equal to some value):
\begin{align*}
\min_{\theta}& \; \frac{1}{2}\|\mathbf{y}- \mathbf{m}(\theta)\|^2\\ 
\textrm{s.t.}&\; \Pi \theta = r
\end{align*}

\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Prior Knowledge}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Penalty term to control parameter behavior (e.g., penalize movements from reference):
\begin{align*}
\min_{\theta}& \; \frac{1}{2}\|\mathbf{y}- \mathbf{m}(\theta)\|^2+\kappa\cdot  \rho(\theta)
\end{align*}
Where $\kappa\geq 0$ is constant that trades-off fit and allowed movement. 
\item Common choices for penalty function $\rho(\theta)$ are:
\begin{itemize}
\setlength{\itemsep}{5pt}
\item  $\ell$-2 norm (a.k.a ridge or Tikhonov penalty): $\rho(\theta)=\frac{1}{2}\|\theta-\bar{\theta}\|_2^2=\frac{1}{2}\sum_{i=1}^n(\theta_i-\bar{\theta}_i)^2$
\item $\ell$-1 norm (a.k.a. lasso penalty): $\rho(\theta)=\|\theta-\bar{\theta}\|_1=\sum_{i=1}^n
|\theta_i-\bar{\theta}_i|$
\item Bayes penalty: $\rho(\theta)=\frac{1}{2}(\theta-\bar{\theta})^T\Sigma_\theta^{-1} (\theta -\bar{\theta})$
\end{itemize}
\item Different penalties  (a.k.a. regularizers) induce different behavior and some of them can be derived from statistical principles. There are many regularizers in the literature (seeking to embed different type of knowledge).
\item Statistics help us to understand type of {\em prior knowledge} that regularizers convey. 
\item Understanding statistical principles of constraints is a more difficult question but one can often reformulate constraints as penalty functions. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction \footnotesize{\texttt{hougen\_watson\_regularized.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Below we show behavior of $S(\theta)+\frac{1}{2}(\theta-\bar{\theta})^T\Sigma_\theta^{-1} (\theta -\bar{\theta})$ 
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/hougen_span_reg1}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/hougen_span_reg2}
\end{figure}
\end{column}
\end{columns}
\item Bayes regularizer helps better define minima but in this case does not avoid presence of non-physical regions (this can only be achieved with constraints). 
\item The presence of non-physical regions can introduce numerical instability (nothing prevents solvers from visiting such regions). 
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction \footnotesize{\texttt{hougen\_watson\_constraints.m}}}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Instead, filter out parameters with non-physical meaning by directly imposing constraints. We formulate problem:
\begin{align*}
\min_{\theta}& \; \frac{1}{2}\|\mathbf{y}- \mathbf{m}(\theta)\|^2\\ 
\textrm{s.t.}&\; \theta_L\leq \theta\leq\theta_U 
\end{align*}
where
\begin{align*}
m_\omega(\theta)=\frac{(\theta_0x_{2,\omega}-x_{3,\omega}/\theta_4)}{(1+\theta_1x_{1,\omega}+\theta_2x_{2,\omega}+\theta_3x_{3,\omega})}
\end{align*}
and we set $\theta_L=(0,0,0,0,0)$ and $\theta_U=(2,2,2,2,2)$. 
\item This problem can be solved using Matlab's bulit-in tool \textrm{lsqnonlin}. 
\item This tool does not provide Hessian but provides Jacobian $\nabla_\theta m(\theta)$. 
\item We can approximate Hessian as $H(\theta)\approx \nabla_\theta m(\theta)^T\nabla_\theta m(\theta)$.
\item All eigenvalues of Hessian are positive (parameters are unique).
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Bayesian Estimation}

Bayes theorem provides a statistical basis to derive a wide range of estimation formulations. 

\begin{itemize}
\setlength{\itemsep}{10pt}
\item In the context of our estimation problem of interest, Bayes theorem states that:
\begin{align*}
f(\theta|\mathbf{y})=\frac{f(\mathbf{y}|\theta)f(\theta)}{f(\mathbf{y})}
\end{align*}
\item $f(\theta|\mathbf{y})$ is probability that parameters take value $\theta$ given knowledge that output takes value $\mathbf{y}$ (a.k.a. posterior pdf)
\item $f(\mathbf{y}|\theta)$ is probability that output takes value $\mathbf{y}$ given knowledge that parameters take value $\theta$ (a.k.a. likelihood)
\item $f(\theta)$ is marginal probability of parameters (a.k.a prior pdf)
\item $f(y)$ is marginal probability of outputs (this is irrelevant as it does not carry knowledge on parameters)
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Bayesian Estimation}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Goal in Bayesian estimation is to maximize probability of parameters $f(\theta|\mathbf{y})$ (not of outputs as in MLE). Bayes theorem tells us that:
\begin{align*}
f(\theta|\mathbf{y})\propto f(\mathbf{y}|\theta)f(\theta)
\end{align*}
\item This approach carries prior knowledge of $\theta$.  Recall that in MLE we find estimate $\hat{\theta}$ that maximizes $f(\mathbf{y}|\theta)$ (it is assumed that $\theta$ is deterministic). 

\item We thus find estimate $\hat{\theta}$ by solving:
\begin{align*}
\max_\theta\quad  \log f(\mathbf{y}|\theta)+\log f(\theta)
\end{align*}
This problem is equivalent to that of MLE but we incorporate prior term $\log f(\theta)$. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Bayesian Estimation}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item If we have prior knowledge that $\theta\sim \mathcal{N}(\bar{\theta},\Sigma_\theta)$, then: 
\begin{align*}
\min_{\theta}& \quad \frac{1}{2}\|\mathbf{y}- \mathbf{m}(\theta)\|^2+\frac{1}{2}(\theta-\bar{\theta})\Sigma_\theta^{-1}(\theta-\bar{\theta})  
\end{align*}
\item Gaussian prior $f(\theta)$ achieves its maximum at $\theta=\bar{\theta}$ while $f(\mathbf{y}|\theta)$ achieves its maximum when $\theta$ fits data. 
\item Estimation problem seeks to find {\em balance} between what we previously knew about $\theta$ and new knowledge gained through observations $\mathbf{y}$. 
\item If ignore prior knowledge, all that we know about $\theta$ is through $\mathbf{y}$ (which might lead to ambiguity if data is insufficient). 
\item Adding prior knowledge reduces {\em ambiguity}. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Bayesian Estimation \footnotesize{\texttt{bayes\_example.m}}}

\begin{itemize}
\item Consider estimation problem for model $Y=\theta X$ and assume single observation $(x,y)$
\item Evidence provided by prior knowledge is $\mathcal{N}(0,1)$ with pdf $f(\theta)$
\item Evidence provided by data $(x,y)$ is $\mathcal{N}(y-\theta x,2)$ with pdf $f(y|\theta)$
\item Combined evidence is pdf $f(\theta|y)\propto f(\mathbf{y}|\theta)f(\theta)$
\item Below we show pdfs in log and original scale:
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/bayes_log.eps}
\end{figure}
\vspace{-0.2in}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/bayes.eps}
\end{figure}
\pause
\begin{itemize}
\item According to prior evidence $f(\theta)$, $\theta=0$ is most likely value
\item According to data evidence $f(y|\theta)$, $\theta=2$ is most likely value 
\item According to combined evidence $f(\theta|y)$, $\theta=1$ is most likely value 
\end{itemize}

\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%
\section{Statistical Learning}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Statistical Learning}

\begin{block}{}
Machine learning (ML) is fast growing field that combines techniques from diverse branches of science and engineering to perform tasks such as:
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Data Analysis (e.g., dimension reduction, clustering, computer vision)
\item Data-Driven Modeling (e.g., neural nets, kriging, support vector machines)
\item Artificial Intelligence (e.g., data collection, experimentation, learning, control)
\end{itemize}
\begin{block}{}
Statistical learning is a subset of ML that provides tools derived from {\em statistical principles}. 
\end{block}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Some tools of machine learning are derived from other mathematical principles (e.g., geometry, topology, optimization, linear algebra). 
\item Our focus here is not to provide an extensive review of all tools. Instead, we focus on general statistical principles behind such tools.  
\end{itemize}
\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Data Analysis}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item How can I interpret and extract knowledge from high-dimensional data? 

\item How can I reduce (compress) my data? 
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Analysis of Multivariate Data}

Consider the following problem:
\begin{itemize}
\setlength{\itemsep}{10pt}
\item You have multiple input RVs $X=(X_1,X_2,...,X_n)$ entering a system.
 
\item You want to create a product that is a {\em mixture} (blend) of these RVs:
\begin{align*}
t=\sum_{i=1}^nw_iX_i=w^TX
\end{align*}
where $w_i\in \mathbb{R}$ are  mixture proportions satisfying $\|w\|=1$, with $w=(w_1, w_2,...,w_n)$. 

\item What proportions $w$ give product that contains maximum information about $X$?
\item What proportions $w$ give product that contains second most information about $X$?
\item What proportions $w$ give product that contains minimum information about $X$?

\end{itemize}
\vspace{0.1in}
Think about analogy of this problem with a physical blending (mixing) process: 
\begin{block}{}
We want to mix a set of input flows in a way that product is as valuable as possible.  
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Analysis of Multivariate Data}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/pca_mixing_diagram}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Analysis of Multivariate Data}

Data mixing problem can be solved using {\em principal component analysis} (PCA). 

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Collect observations for $x_\omega\in \mathbb{R}^n,\; \omega \in 1,...,S$ for the RV $X=(X_1,X_2,...,X_n)$.  
\item Store the observations in $S\times n$ data matrix: 
\begin{align*}
\mathbf{X}=\left[\begin{array}{ccccccc}x_{1,1}&x_{1,2}&\cdots &x_{1,n}\\
x_{2,1}&x_{2,2}&\cdots &x_{2,n}\\
\vdots&\vdots&\vdots &\vdots\\
x_{S,1}&x_{S,2}&\cdots&x_{S,n}
\end{array}
\right]
\end{align*}
\item Normalize columns in such a way that $\frac{1}{S}\sum_{i=1}^S\mathbf{X}_{i,j}=0$ for all $j=1,...,n$.  This centers the data around zero. 
\item After normalization, the sample covariance matrix of $X$ is $\textrm{Cov}(X)\propto \mathbf{X}^T\mathbf{X}$ (denote this as $\Sigma$ and note this is $n\times n$). 
\item Covariance matrix contains all information (variance) of inputs $X$. 


\end{itemize}


\end{frame}
%------------------------------------------------

\begin{frame}{Analysis of Multivariate Data}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item For mixture $t=w^TX$ we can show that $\hat{\mathbb{V}}[t]=w^T\Sigma w$. 
\item Consequently, the mix proportions that contain max variance of $X$ can be found as:
\begin{align*}
\max_{w}\; w^T\Sigma w\; \textrm{s.t.}\; \|w\|=1
\end{align*}
\item Solution of this problem is eigenvector $w_1$ of $\Sigma$ associated with largest eigenvalue $\lambda_1$. Consequently, proportions $w_1$ give mixture  $t_1=w_1^TX$ that contains maximum information about $X$. 
\item Mixture that contains minimum information about $X$ is found by solving:
\begin{align*}
\min_{w}\; w^T\Sigma w\; \textrm{s.t.}\; \|w\|=1
\end{align*}
this gives eigenvector $w_n$ associated with smallest eigenvalue $\lambda_n=w^T_n\Sigma w_n$

\item We can find and rank mixtures based on information content by performing an eigendecomposition: 
\begin{align*}
\Sigma = W\Lambda W^T=\lambda_1w_1w_1^T+\lambda_2w_2w_2^T+\cdots + \lambda_n w_nw_n^T
\end{align*}
\item Eigenvalues are arranged as $\lambda_1\geq \lambda_2\geq \cdots \lambda _n$. Consequently, mix $t_1=w_1^TX$ contains most info while $t_2=w_2^TX$ contains second most info and so on. 


\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Analysis of Multivariate Data}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Mixtures (a.k.a. principal components) $t_1,t_2,...,t_n$ contain information about $X$. Consequently, select a few of them (typically two or three) to visualize data $X$ in a small dimensional space (PCA is a dimensionality reduction technique). 

\item Truncating the eigendecomposition series enables compression of data matrix $\Sigma$. 

\item Key property of principal components is that they retain structure of high-dimensional data. Visualizing data by dropping variables destroys structure.

\item We can show that principal components are uncorrelated and thus $\textrm{Cov}(t_i,t_j)=0$ for all $i\neq j$ (i.e., mixtures contain complementary types of information). 

\item We can use eigenvector matrix $W=[w_1\,|\,w_2\,|\,\cdots|w_n]\in \mathbb{R}^{n\times n}$, to can {\em project} data matrix $\mathbf{X}$ to the principal component (information) space as: 
\begin{align*}
\mathbf{T}=\mathbf{X}W
\end{align*}
where $\mathbf{T}\in \mathbb{R}^{S\times n}$ is a matrix with entries $t_{i,j},\,i=1,...,S\, j=1,...,n$. 

 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_pca.m}}}

\begin{itemize}
\setlength{\itemsep}{3pt}
\item Have data set with pressure, temperature, and conversion $(P,T,C)$.
\item If we visualize data in a 3D $(P,T,C)$ space, there are clearly two clusters revealed (one is reactor operating under failure mode and one under normal mode).
\item Imagine we drop $T$ data because we believe this is unimportant and visualize $(P,C)$ space alone. We can see that   failure clusters cannot be spotted. This illustrates how improper reduction of data can miss significant information. 
\item By using PCA, we retain structure and can visualize in 2D space $(t_1,t_2)$.
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/pca_gibbs.pdf}
\end{figure}

\begin{itemize}
\item Eigenvalues obtained with PCA are $\lambda=(3.19\times 10^5,1.12\times 10^5,4)$. 
\item This indicates that one of the principal components carries small information (there is a redundant dimension). This is because of high correlation between $P$ and $C$ (i.e., if you know $P$, you know $C$ and vice versa). 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Singular Value Decomposition}

\begin{itemize}
\setlength{\itemsep}{3pt}
\item PCA decomposes covariance $\Sigma=\mathbf{X}^T\mathbf{X}$ as $\Sigma=W\Lambda W^T$

\item One can also implement PCA by decomposing data matrix $\mathbf{X}\in\mathbb{R}^{S\times n}$ directly. This is done by using a technique called singular value decomposition (SVD):
\begin{align*}
\mathbf{X}=USV^T
\end{align*}
\item Here, $U\in \mathbb{R}^{S\times S}$ is left eigenvector matrix, $V\in \mathbb{R}^{n\times n}$ is right eigenvector matrix and $\mathbf{S}\in \mathbb{R}^{S\times n}$ is singular value matrix (a diagonal matrix). 
\item Matrices $U,V$ are orthogonal and thus satisfy $U^TU=I$ and $V^TV=I$:
\begin{align*}
\mathbf{X}^T\mathbf{X}&=(U\mathbf{S}V^T)^T(U\mathbf{S}V^T)\\
&=V\mathbf{S}^T\mathbf{S}V^T
\end{align*}
\item By defining $W=V$ and $\Lambda=\mathbf{S}^T\mathbf{S}$, we obtain standard PCA. 
\item SVD of $\mathbf{X}$ can be written as an expansion of the form:
\begin{align*}
\mathbf{X}=\sum_{j=1}^ns_ju_jv_j^T
\end{align*}
where $s_j=\mathbf{S}_{j,j}$ and $u_j$ and $v_j$ are columns of $U$ and $V$, respectively. 
\item Why would you prefer to decompose $\mathbf{X}$ over decomposing $\Sigma$?
 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Image Compression \footnotesize{\texttt{svd\_reconstruction.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item An image is a matrix $\mathbf{X}$ (each entry represents a pixel and the number the intensity)
\item We decompose the image using SVD as $\mathbf{X}=\sum_{j=1}^ns_ju_jv_j^T$ (elements of series are arranged in order of information content). 
\item Large components tend to contain broad (global) features of image while small components contain granular (local) features. 
\item We can truncate series to compress image (i.e., ignore granular behavior and noise).


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{figstats/Matlab/babyviking}
\end{figure}

\item This image is a matrix $\mathbf{X}$ of dimension 747$\times$ 941
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}{Example: Image Compression \footnotesize{\texttt{svd\_reconstruction.m}}}

\begin{itemize}
\item Approximate images (matrices) obtained using truncated series $\mathbf{X}_k=\sum_{j=1}^ks_ju_jv_j^T$ for $k=1,3,10,100$:


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/Matlab/approximation_viking}
\end{figure}

\item Note how coarse features develop quickly while granular develop slowly
\item Each element $s_ju_jv_j^T$ encodes different ``features" of the image
\item Nearly perfect image obtained with $k=100$ (13\% of original data)
\item Truncating series cuts down non-informative features
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Image Compression \footnotesize{\texttt{svd\_components.m}}}

\begin{itemize}
\item Here are first nine elements $s_ju_jv_j^T$ of series $\mathbf{X}=\sum_{j=1}^ns_ju_jv_j^T$. 
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/singularvalues_viking}
\end{figure}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Analysis of Space-Time Data}

\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{5pt}
\item PCA assumes observations $x_\omega,\; \omega \in \mathcal{S}$ are independent
\item But what if there is correlation between observations $x_\omega$ and $x_{\omega'}$?
\item For instance, what if there is a temporal correlation (i.e., $x_\omega$, $x_{\omega+1}$ are correlated)? 
\end{itemize}
\end{block}

\begin{itemize}
\item Consider $x_\omega\in \mathbb{R}^n$ are observations at time $t_\omega$ and consider mixture:
\begin{align*}
t_\omega=w^Tx_\omega,\quad \omega \in \mathcal{S}
\end{align*}
\item This forms a time series $t_1,t_2,...,t_S$ and we assume temporal model of the form:
\begin{align*}
\hat{t}_k=\beta_1t_{k-1}+\beta_2t_{k}+\cdots+\beta_rt_{k-r}
\end{align*}
\item This linear model is known as a time-series model (a whole topic on its own).
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Analysis of Space-Time Data}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item Our goal is to find parameters $w$ and $\beta$ such that $t_\omega$ capture most information of $x_\omega$.
\item This can be done by solving the following optimization problem:
\begin{align*}
\max_{w,\beta}& \quad \sum_{\omega=1}^{S}t_k\hat{t}_\omega\\
\textrm{s.t.}& \quad t_\omega=w^Tx_\omega,\; \omega=1,...,S\\
& \quad \hat{t}_\omega=\beta_1t_{\omega-1}+\beta_2t_{\omega}+\cdots+\beta_rt_{\omega-r},\; \omega=1,...,S
\end{align*}
with $\|\beta\|=1$ and $\|w\|=1$. 
\item This can be interpreted as finding $w,\beta$ that maximize predictability of $\hat{t}_\omega$. 
\item One can show that problem is a special type of an eigenvalue problem that captures static and temporal variance of $x_\omega$. 
\item Time series $\hat{t}_\omega,\; \omega\in\mathcal{S}$ contains maximum information about $x_\omega$ (these are known as dynamic principal components).
\item As in PCA, we find a time series that contains second most information and so on.
\item This provides powerful approach to visualize dynamics of high-dimensional data. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Dynamic Image Analysis \footnotesize{\texttt{dynamic\_image\_analysis.m}}}
\begin{figure}[!htb]
    \centering
  \includegraphics[width=0.5\textwidth]{figstats/lc-raw.pdf}
  \end{figure}
  \vspace{-0.1in}
  \begin{itemize}
  \setlength{\itemsep}{5pt}
  \item Chemical sensor responds to different contaminants, seek to spot differences in space-dynamic response to identify contaminant type.
  \item Each observation $x_\omega$ is an image and we have a sequence $\omega=1,...,S$.
  \item Below we show dynamics for first two principal components extracted from data.
  \end{itemize}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.4\textwidth]{figstats/Matlab/DiPCA/output/pc_plot_1}
    \end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Data-Driven Modeling (Classification)}


Consider following problem:
\begin{itemize}
\item You have input $X=(X_1,...,X_n)$ with observations $x_\omega\in \mathbb{R}^n,\; \omega \in \mathcal{S}$. Domain $\mathcal{D}_X$ can be discrete or continuous.
\item You have output $Y$ with observations $y_{\omega}\in \mathbb{R}$ and discrete domain $\mathcal{D}_Y=\{0,1\}$. 
 
\item We postulate that there exists a relationship between $X$ and $Y$ of the form:
\begin{align*}
Y=g(X,\theta)
\end{align*}
where $\theta \in \mathbb{R}^n$ are parameters and $g(X,\theta)$ is model.  

\item Our objective is to find model that best explains observations:
\begin{align*}
y_\omega=g(x_\omega,\theta)+\epsilon_\omega
\end{align*}
\item This estimation problem is known as a {\em classification} problem.  In this context, inputs $X$ are known as features or descriptors while $Y$ are known as labels or classes.  
\item What is different (and difficult) here is the binary nature of output $Y$. In standard estimation problems, $Y$ is assumed to be continuous. 

\end{itemize}

Think about chemical classification problem: 
\begin{block}{}
Given a set of features for a chemical, can we predict if this is toxic or not? 
\end{block}


\end{frame}
%------------------------------------------------


%------------------------------------------------

\begin{frame}{Classification}


\begin{itemize}
  \setlength{\itemsep}{10pt}
\item To solve problem, we postulate a model function of the form:
\begin{align*}
g(x,\theta)=\frac{1}{1+e^{-\theta^Tx}}
\end{align*}
 
\item The mixture $\theta^Tx=\sum_{i=1}^n\theta_ix_i$ is known as {\em evidence}. Parameters reflect weights that we place on different features.

\item Postulated model is known as the logistic function and seeks to capture 0-1 logic. 
\item Logistic function is a sigmoidal function that satisfies:

\begin{itemize}
  \setlength{\itemsep}{10pt}
 \item $g(x,\theta)\to 1$ as $\theta^Tx\to +\infty$ 
 \item $g(x,\theta)\to 0$ as $\theta^Tx\to -\infty$ 
 \item $g(x,\theta)=\frac{1}{2}$ if $\theta^Tx=0$
\end{itemize}

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Classification \footnotesize{\texttt{plot\_logistic\_function.m}}}

Here is a visualization of the logistic function for different values of $\theta$ and for scalar $x$. 
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/logistic.eps}
\end{figure}
\end{frame}
%------------------------------------------------


%------------------------------------------------

\begin{frame}{Classification}


\begin{itemize}
  \setlength{\itemsep}{10pt}
\item Logistic function $g(x,\theta)$ can model probabilities $\mathbb{P}(Y=1\,|\,x,\theta)$ and $\mathbb{P}(Y=0\,|\,x,\theta)=1-\mathbb{P}(Y=1\,|\,x,\theta)$:

\vspace{0.1in}
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item If evidence is strongly positive, there is a high probability that $Y=1$
\item If evidence is strongly negative, there is a high probability that  $Y=0$
\item If evidence is weak, there is ambiguity (it is equally probable that $Y=0$ or $Y=1$)
\end{itemize}

\item This logic can be captured by defining a conditional pdf of the form:
\begin{align*}
f(y|x,\theta)=\mathbb{P}(Y=y|x,\theta)=g(x,\theta)^y(1-g(x,\theta))^{1-y}
\end{align*}
\item Goal is to find estimate $\hat{\theta}$ that maximizes joint probability $\prod_{\omega \in \mathcal{S}}f(y_\omega,|x_\omega,\theta)$:
\begin{align*}
\max_{\theta}\; \sum_{\omega \in \mathcal{S}}y_\omega\log(g(x_\omega,\theta))+(1-y_\omega)\log(1-g(x_\omega,\theta))
\end{align*}

\item Having estimate $\hat{\theta}$ we use model $g(x,\hat{\theta})$ to predict class of input with features $x$. 

\item This approach does not minimize sum of squared errors (and is standard estimation).  The objective function is called {\em log loss}. 
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_logistic.m}}}

\begin{itemize}
  \setlength{\itemsep}{10pt}
\item Have data $(P_\omega,C_\omega,T_\omega)$ for reactor.
\item Reactor is in normal mode when $T_\omega$ is low ($y_\omega=1$) and in failure mode when $T_\omega$ is high ($y_\omega=0$).
\item We seek to develop a model that predicts mode by using data $x_\omega=(P_\omega,C_\omega)$. 
\item Data tells us that this might be achievable due to strong cluster separation.
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/Matlab/gibbs_logistic_data.eps}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_logistic.m}}}

\begin{itemize}
  \setlength{\itemsep}{10pt}
\item After estimation, we find that model correctly classifies failure 85\% of the time. 
\item Logistic classification also gives us an estimate of the probability that predictions are in a given class and which observations are in a ``fuzzy" (confusion) region. 
\item If data noise is increased, cluster separation is not as strong and accuracy decreases. 
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/gibbs_logistic_confusion.eps}
\end{figure}



\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Data-Driven Modeling (Kernel Methods)}
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item A flexible approach to constructing models is to {\em mix} different types of models.  
\item Assume that $Y$ and $X=(X_1,...,X_n)$ follow a relationship of the form:
\begin{align*}
Y=\sum_{j=1}^m\theta_j \phi_j(X)=\theta^T\phi(X)
\end{align*}
\item $\phi_j(X),\; j=1,...,m$ is collection of basic models (a.k.a. basis functions). 

\item We define vector $\phi(X)=(\phi_1(X),...,\phi_m(X))$. 

\item Basis function $\phi_j(X)$ can be nonlinear (e.g., polynomial, sigmoidal, exponential) or linear (in which case $\phi(X)=X$). 

\item Parameters $\theta_j\in \mathbb{R}$ are mixing coefficients of basis functions.  

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Data-Driven Modeling (Kernel Methods)}
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item Assume we determine estimate $\hat{\theta}$ by solving regularized MLE problem:
\begin{align*}
\min_{\theta}\; S(\theta)= \frac{1}{2}\sum_{\omega \in \mathcal{S}}(y_\omega-\theta^T\phi(x_\omega)))^2+\lambda \theta^T\theta
\end{align*}
In vector form:
\begin{align*}
\min_{\theta}\; S(\theta)= \frac{1}{2}(\Phi\theta -\mathbf{y})^T(\Phi\theta-\mathbf{y})+\lambda\theta^T\theta
\end{align*}
\item $\Phi\in \mathbb{R}^{S\times n}$ is input data matrix with $\Phi_{\omega,j}=\phi_{j}(x_\omega)$ and $\mathbf{y}$ is output data vector

\item If basis functions are linear then $\Phi=X$. 

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------

\begin{frame}{Kernel Methods}
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item Optimality conditions indicate that best estimate satisfies:
\begin{align*}
\theta=\frac{1}{\lambda}\Phi^T\mathbf{r}
\end{align*}
where we define residuals (mismatch errors) $\mathbf{r}=(\Phi\theta-\mathbf{y})$.  

\item By substituting in $S(\theta)$ we obtain:
\begin{align*}
S(\theta)= \frac{1}{2}\mathbf{r}^T\Phi\Phi^T\Phi\Phi^T\mathbf{r}-\mathbf{r}^T\Phi\Phi^T\mathbf{y}+\frac{1}{2}\mathbf{y}^T\mathbf{y}+\frac{\lambda}{2}\mathbf{r}^T\Phi\Phi^T\mathbf{r}
\end{align*}
We define matrix $K=\Phi\Phi^T\in \mathbb{R}^{S\times S}$ and simplify:
\begin{align*}
S(\theta)= \frac{1}{2}\mathbf{r}^TKK\mathbf{r}-\mathbf{r}^TK\mathbf{y}+\frac{1}{2}\mathbf{y}^T\mathbf{y}+\frac{\lambda}{2}\mathbf{r}^TK\mathbf{r}
\end{align*}
\item Note that $S(\theta)$ can be defined entirely in terms of $\mathbf{r}$. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kernel Methods}
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item This suggests an alternative strategy to find estimate $\hat{\theta}$. 
\begin{itemize}
\item We solve the following problem to find an optimal $\hat{\mathbf{r}}$: 
\begin{align*}
\min_{\mathbf{r}} \frac{1}{2}\mathbf{r}^TKK\mathbf{r}-\mathbf{r}^TK\mathbf{y}+\frac{1}{2}\mathbf{y}^T\mathbf{y}+\frac{\lambda}{2}\mathbf{r}^TK\mathbf{r}
\end{align*}
\item We recover estimate $\hat{\theta}=\Phi^T\hat{\mathbf{r}}$
\end{itemize}
\item It turns out, however, that parameters $\hat{\theta}$ are {\em not needed at all}. 
\item To see this, note that $\hat{\mathbf{r}}$ satisfies $\hat{\mathbf{r}}=(K+\lambda)^{-1}\mathbf{y}$ and thus optimal prediction is:
\begin{align*}
\hat{\mathbf{y}}&= K(K+\lambda I)^{-1}\mathbf{y}
\end{align*}
\item Optimal prediction only depends on input data (in matrix $K$) and output data (in $\mathbf{y}$).  

\item We thus see that $\theta$ are not needed to make predictions (these are just intermediary). 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kernel Methods}
\begin{itemize}
\item Matrix $K$ is known as kernel matrix and captures interactions in input variables. 
\item Given input data $x_\omega,\; \omega \in \mathcal{S}$, the kernel matrix can be constructed as:
\begin{align*}
K_{i,j}=k(x_i,x_j)\; i,j\in \mathcal{S}
\end{align*}
where $k(x_i,x_j)$ is the {\em kernel function}. 
\item The case for linear estimation corresponds to defining kernel function:
\begin{align*}
k(x_i,x_j)=x_i^Tx_j
\end{align*}
In this case, note that $K=\textrm{Cov}(X)$ (after normalization). 
\item The case of basis functions corresponds to defining a kernel function:
\begin{align*}
k(x_i,x_j)=\phi(x_i)^T\phi(x_j)
\end{align*}
\item Typically, the kernel function is chosen to be the radial basis function (RBF):
\end{itemize}
\begin{align*}
k(x_i,x_j)=\textrm{exp}(-\gamma\|x_i-x_j\|^2)
\end{align*}
where $\gamma$ is a parameter. This kernel is also known as the Gaussian kernel.  
\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kernel Methods}

RBF kernel is simple but captures a wide range of nonlinear behavior. 
\begin{block}{}
Consider case where $x_\omega$ is a scalar and notice that:
\begin{align*}
\exp(-\gamma(x_i-x_j)^2)&=\exp(-\gamma x_i^2+2\gamma x_ix_j-\gamma x_j^2)\\
&=\exp(-\gamma x_i^2-\gamma x_j^2)\left(1+\frac{2\gamma x_ix_j}{1\, !}+\frac{(2\gamma x_ix_j)^2}{2\,!}+...\right)
\end{align*}
We thus have that RBF can be written as:
\begin{align*}
\exp(-\gamma(x_i-x_j)^2)&=\phi(x_i)^T\phi(x_j)
\end{align*}
where $\phi(x)$ is an {\em infinite} collection of polynomial basis functions: 
\begin{align*}
\phi(x)=e^{-\gamma x}(1,x\sqrt{2\gamma/1\,!},x^2\sqrt{(2\gamma)^2/2\,!},x^3\sqrt{(2\gamma)^3/3\,!},...). 
\end{align*}
\end{block}
\begin{itemize}
\item In kernel methods, we do not need to postulate an input-output model (e.g., $Y=\theta^T\phi(X))$ and estimate its parameters ($\theta$).  
\item Instead, we postulate a kernel model and estimate its parameters (e.g., $\gamma$).  
\item Number of parameters of a kernel function is often small (typically a handful) . 
\item Small number of parameters is a key advantage over standard estimation approaches. 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Hougen-Watson Reaction \footnotesize{\texttt{hougen\_watson\_kernel.m}}}

\begin{itemize}
  \setlength{\itemsep}{5pt}
\item Mechanistic model fits data well but is highly nonlinear and has five parameters:
\begin{align*}
Y=\frac{(\theta_0X_2-X_3/\theta_4)}{(1+\theta_1X_1+\theta_2X_2+\theta_3X_3)}
\end{align*}
\item Instead of using this model, we use data $(x_\omega,y_\omega)$ to construct a kernel model. 
\item We use Gaussian kernel $k(x_\omega,x_{\omega'})=\exp(-\gamma\|x_\omega-x_{\omega'}\|^2)$ with $\gamma=0.01$. 
\item Kernel model only requires one parameter ($\gamma$). 
\item Below we show the fit of the data:


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{figstats/Matlab/hougen_fit_kernel.eps}
\end{figure}
\item The kernel model can indeed capture the high nonlinearity of mechanistic model.
\end{itemize}
\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Data-Driven Modeling (Kriging)}

\begin{block}{}
How do we estimate kernel function hyperparameters?
\end{block}

This can be done by using a technique called kriging. 
\begin{itemize}
  \setlength{\itemsep}{10pt}
\item In kriging we postulate a model of the form:
\begin{align*}
{y}_\omega = g(x_\omega)+\epsilon_\omega,\; \omega \in \mathcal{S}
\end{align*}
\item In vector form:
\begin{align*}
\mathbf{y}= \mathbf{g}+\epsilon
\end{align*}
\item If we assume that $\epsilon_\omega \sim \mathcal{N}(0,\sigma)$ then we have that $f(\mathbf{y}|\mathbf{g})$ corresponds to $\mathcal{N}(\mathbf{g},\sigma \mathbb{I})$. 
\item Note absence of parameters in postulated model. 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kriging}

\begin{itemize}
  \setlength{\itemsep}{10pt}
\item What is unique about kriging is that $\mathbf{g}$ is treated as a random function (non-parametric). 

\item In the techniques that we have discussed we defined parametric functions $\mathbf{g}(\theta)$.

\item Assume that the random function $\mathbf{g}$ has pdf $f(\mathbf{g}|\gamma)$ corresponding to $\mathcal{N}(0,K(\gamma))$.  

\item Here, $K(\gamma)=\textrm{Cov}(\mathbf{g})$ is a covariance function and $\gamma$ are its parameters.  

\item Think about $K(\gamma)$ as a function that generates samples of random model $\mathbf{g}$. 

\item Kernel matrix has entries:
\begin{align}
K_{i,j}(\gamma)=k(x_i,x_j,\gamma)
\end{align}
where $k(x_i,x_j,\gamma)$ is kernel function. 

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kriging}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Marginal of $\mathbf{y}$ is:
\begin{align*}
f(\mathbf{y}|\gamma)=\int f(\mathbf{y}|\mathbf{g})f(\mathbf{g}|\gamma)d\mathbf{g}
\end{align*}
\item One can show that this corresponds to pdf of $\mathcal{N}(0,C(\gamma))$ with entries:
\begin{align*}
C_{i,j}(\gamma)=K_{i,j}(\gamma)+\sigma^2\cdot  \delta_{i,j},\quad i,j\in \mathcal{S}
\end{align*}
where $\delta_{i,j}=1$ if $x_i=x_j$ and zero otherwise. 

\item One approach to estimate $\hat{\gamma}$ consists of maximizing $\log f(\mathbf{y}|\gamma)$:
\begin{align*}
\max_\gamma \; -\frac{1}{2}\log|C(\gamma)|-\frac{1}{2}\mathbf{y}^TC(\gamma)\mathbf{y}-\frac{S}{2}\log 2\pi
\end{align*}

\item As a kernel method, in kriging one computes predictions by using kernel function (which only depends on $\hat{\gamma}$ and input data).

\item This is done by using conditional pdf $f(y|\mathbf{y})$  where $y$ is predicted output at a new point $x$ and $\mathbf{y}$ are observations used to determine $\hat{\gamma}$.

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Kriging}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Conditional pdf $f(y|\mathbf{y})$ is Gaussian $\mathcal{N}(m(x),\sigma^2(x))$ with:
\begin{align*}
m(x)&=\mathbf{k}^TC^{-1}\mathbf{y}\\
\sigma^2(x)&=c-\mathbf{k}^TC\mathbf{k}
\end{align*}
and:
\begin{align*}
C&=C(\hat{\gamma})\\
c&=k(x,x,\hat{\gamma})\\
\mathbf{k}_i&=k(x_i,x,\hat{\gamma}),\; i=1,...,S
\end{align*}
\item Mean prediction of $y$ is $m(x)$ and variance is $\sigma^2(x)$.
\item There is a wide range of kernel functions that can be used in kriging. The generalized Gaussian kernel resembles the RBF and takes the form:
\begin{align*}
k(x_i,x_k,\gamma)=\gamma_0\exp\left(-\frac{\gamma_1}{2}\|x_i-x_j\|^2\right)+\gamma_2
\end{align*}
\item It is possible to combine parametric and non-parametric estimation within kriging. 
\item This can be done by defining $f(\mathbf{y}|\mathbf{g},\theta)$ as $\mathcal{N}(\mathbf{g}+\Phi\theta,\sigma \mathbb{I})$ where $\Phi$ is an input data matrix with entries $\Phi_{\omega,j}=\phi_j(x_\omega)$ and $\phi_j(x)$ is a collection of basis functions. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_kriging.m}}}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item We compare performance of kriging (nonlinear) and linear models 
\item We construct models with inputs $X=(P,T)$ and output $Y=C$ 
\item Below we show fit obtained with both models: 
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/fit_gibbs_kriging_linear.eps}
\end{figure}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item The kriging model can predict better over entire domain
\item Linear model predicts accurately in the middle of the domain but fails on the extremes (where nonlinearity is more marked)
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_kriging.m}}}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item We can benchmark models by comparing pdfs and cdfs of their residuals
\item Here are the empirical cdfs for residuals $|\epsilon|=|y-\hat{y}|$ for kriging and linear models:
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{figstats/Matlab/benchmark_gibbs_kriging_linear_pdf.eps}
\end{figure}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.3\textwidth]{figstats/Matlab/benchmark_gibbs_kriging_linear.eps}
\end{figure}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Kriging never exceeds threshold $t=0.06$ while linear reach levels of $t=0.1$
\item How do you interpret above graphs? 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_kriging.m}}}

\begin{itemize}
\item An important feature of kriging is that it provides uncertainty on predictions
\end{itemize}

%\begin{figure}[!htb]
%    \centering
%	\includegraphics[width=0.7\textwidth]{figstats/Matlab/benchmark_gibbs_kriging_linear_pdf.eps}
%\end{figure}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{figstats/Matlab/conf_gibbs.eps}
\end{figure}


\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Data-Driven Modeling (Neural Networks)}

\begin{block}{}
How do humans learn? How do they establish connections between variables to make predictions and decisions? 
\end{block}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Neural networks (NNs) are nature-inspired parametric models of the form:
\begin{align*}
Y=g(X,\theta)
\end{align*}
\item What is different about NNs is that the model function $g(X,\theta)$ is automatically constructed by using a set of activation (basis) functions that are mixed (combined) in a hierarchical manner.  This seeks to mimic how the brain works. 

\item NNs provide a flexible approach to capture virtually any type of relationship between $X$ and $Y$. A key advantage of this is that we do not need to postulate a model (e.g., nonlinear, linear, logistic, mechanistic).  

\item As with estimation and classification, the objective is to build a NN model (estimate parameters $\theta$) to fit the  observations:
\begin{align*}
y_\omega=g(x_\omega,\theta)+\epsilon_\omega,\; \omega \in \mathcal{S}
\end{align*}
\item Determination of $\theta$ from data mimics the {\em learning} process of a human. 

\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------

\begin{frame}{Neural Networks}

\begin{itemize}
   \setlength{\itemsep}{5pt}
\item NNs seek to mimic how the brain responds when exposed to data and how we learn and accumulate knowledge. 

\item The brain is a highly sophisticated network that has neurons as basic processing (decision) units that interact with one another through signals. 

\item  The mathematical description of a neuron is called a {\em perceptron}. A perceptron generates a signal if evidence $\sum_{i=1}^n\theta_ix_i+\theta_0$ is strong enough. 

\item Parameter $\theta_0$ is called the {\em bias}; this captures situations in which we make decisions based on our inherent biases (ignoring evidence). 

\item Binary behavior of perceptron is similar to that logistic classification. In fact, perceptrons are often modeled using logistic functions. 

\item Perceptron is a decision unit that decides to ``fire" or not when exposed to evidence. How to respond to different pieces of evidence is captured by parameters $\theta$.   
 
\item An NN is a hierarchical architecture of perceptrons that constructs {\em complex logic}. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}

\begin{figure}[!htb]
    \centering
	\href{http://dataskunkworks.com/2018/03/22/building-an-artificial-neuron-in-python-the-perceptron}{\includegraphics[width=0.9\textwidth]{figstats/neuron_perceptron}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}

The components of a NN hierarchy are: input layers,  hidden layers, and output layers.
\begin{block}{}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Input layer contains perceptrons that take input data $X$ to generate signals. This layer captures basic logic (i.e., I decided  this because of that). 
\item Hidden layer contains perceptrons that take signals from input layer to generate additional signals. This layer captures abstract logic. This layer captures aspects that are difficult for humans to rationalize (i.e., why did I decide that?). 
\item Output layer contains perceptrons that take signals from hidden layer to generate a final output signal $Y$. Signal can be continuous or discrete. 
\end{itemize}
\end{block}
We now proceed to show how to train NNs. For simplicity, we will assume a NN architecture with one input layer, one hidden layer, and one output layer. 

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}

\begin{figure}[!htb]
    \centering
	\href{https://www.neuraldesigner.com/learning/tutorials/neural-network}{\includegraphics[width=0.8\textwidth]{figstats/neuralnetwork}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}

\begin{itemize}
   \setlength{\itemsep}{5pt}
\item {\em Input layer} is composed of $j=1,...,n_I$ perceptrons. Evidence in perceptron $j$ is :
\begin{align*}
a_j=\sum_{i=1}^n\theta_{j,i}^{I}x_i+\theta_{j,0}^I
\end{align*}
Here, $\theta_{j,i}^I$ are parameters of perceptron ($\theta_{j,0}^I$ is bias) and $x_i$ is input data (for an observation $\omega$).  Given evidence, perceptron $j$ generates output signal:
\begin{align*}
z_j=h(a_j)
\end{align*}
here, $h(a_j)$ is known as activation function and is often modeled using a sigmoidal function (tanh and max functions are also often used). 
\item {\em Hidden layer} is composed of $k=1,...,n_H$ perceptrons. Evidence in perceptron $k$ is:
\begin{align*}
a_k=\sum_{j=1}^{n_I}\theta_{k,j}^{H}z_j+\theta_{k,0}^H
\end{align*}
Given evidence, output signal is:
\begin{align*}
w_k=h(a_k)
\end{align*}

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}
\begin{itemize}
   \setlength{\itemsep}{5pt}
\item {\em Output layer} takes signals of hidden layer and can have one or multiple perceptrons (depending on how many outputs $Y$ we have). If there is one output,  evidence is:
\begin{align*}
a=\sum_{k=1}^{n_H}\theta_{k}^{O}w_k+\theta_{0}^O
\end{align*}
Final output (model prediction) is $m=\sigma(a)$ where $\sigma(a)$ is a sigmoidal function if output $Y$ is binary (e.g., classification) or $\sigma(a)=a$ if continuous (e.g., regression). 

\item Given parameters $\theta=(\theta^I,\theta^H,\theta^O)$, NN propagates input $x_\omega$ into output $m_{\omega}(\theta)$. This forward propagation can be written as:
\begin{block}{}
\begin{align*}
m_{\omega}(\theta)
=\sigma\left(\sum_{k=1}^{n_H}\theta_{k}^Oh\left(\sum_{j=1}g\left(\sum_{i=1}^n\theta_{j,i}^Ix_i^\omega+\theta_{j,0}^I\right)+\theta_{k,0}^H\right)+\theta_0^O\right),\; \omega \in \mathcal{S}
\end{align*}
\end{block}
\item Derivatives $\nabla_\theta m_\omega(\theta)$ are computed using a technique called {\em back propagation}.
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}{Neural Networks}
\begin{itemize}
   \setlength{\itemsep}{5pt}
\item We can use an MLE framework to estimate $\theta$. For regression problems:
\begin{align*}
\min_{\theta}\; S(\theta)=\frac{1}{2}\sum_{\omega\in \mathcal{S}}(y_\omega - m_\omega(\theta))^2
\end{align*}
In the context of NNs, $S(\theta)$ is also known as the loss function.  
\item For classification problems we find $\theta$ that maximizes the log loss. 

\item Main advantage of NNs is that they are completely data-driven (no mechanistic understanding is needed). However, this is also the main disadvantage of NNs. 

\item Parameters and hidden variables often have no physical meaning and it is thus difficult to convey prior knowledge. 

\item This often manifests as a need for large amounts of data to determine parameters which are also many (on the order of thousands to millions). In the architecture discussed with have $n_I\cdot n+n_H\cdot n_I+n_H$ parameters.

\item As a result, a typical issue with NNs is that of {\em overfitting}.  Regularization terms (e.g., $\rho(\theta)=\lambda \theta^T\theta$) are often added to tne loss function but there is often no mechanistic basis to construct more sophisticated strategies (e.g., Bayesian). 

\item Conducting cross-validation in NNs is particularly critical. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Gibbs Reactor \footnotesize{\texttt{gibbs\_kriging.m}}}

\begin{itemize}
   \setlength{\itemsep}{5pt}
\item We now compare the performance of NN against kriging
\item NN contains one hidden layer (with two perceptrons) 
\end{itemize}


\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/Matlab/benchmark_gibbs_kriging_nn.eps}
\end{figure}

\begin{itemize}
   \setlength{\itemsep}{5pt}
\item We see that performance is virtually the same (NN can capture high nonlinearity)
\item Reducing perceptrons worsens predictions (but not by much)
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Sigmoids as Basis Functions \footnotesize{\texttt{gibbs\_kriging.m}}}

\begin{itemize}
\item So why are NNs so effective at capturing nonlinear behavior? 
\item NNs are sophisticated basis function models (use activation functions)
\item To see this, construct a simple NN model with sigmoidal activation functions:
\begin{align*}
\phi_k(z)=\frac{1}{1+e^{-z}},\; k=1,...,N
\end{align*}
where $z=\theta_k\cdot x + b_k$ 
\item NN output is $y=\sum_{k=1}^Nw_k\phi_k(z)$ (there are no hidden layers).  
\item Fit of NN model to function $y=\sin(4x)+\sin(8x)+\exp(-x)$ is shown below:

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/basis_nn_fit.eps}
\end{figure}

\item The Hessian $H(\hat{\theta})$ has five eigenvalues below $1\times 10^{-5}$ (surface of $S(\theta)$ is flat). 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Sigmoids as Basis Functions \footnotesize{\texttt{nn\_basis.m}}}

\begin{itemize}
\item Basis functions (with estimated parameters) are shown below. 



\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.9\textwidth]{figstats/Matlab/basis_nn.eps}
\end{figure}

\item NN model $y=\sum_{k=1}^Nw_k\phi_k(z)$ thus captures nonlinear behavior.  
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Convolutional Neural Nets (CNNs) are NNs that are specialized to take images (matrices $\mathbf{X}_\omega$) as inputs to make predictions $y_\omega$ (e.g., classify or predict value). 
\item In principle, we could create a large vector $\textrm{vec}(\mathbf{X}_\omega)$ and feed this into a NN to make a prediction. However, this approach can lead to a large number of parameters. 
\item Instead, CNNs seek to summarize (compress) images by extracting relevant features. Such features are extracted by using filters (a filter is a small matrix $\mathbf{W}$ that encodes a pattern). 
\item Extracted features are then used as evidence in the perceptrons that generate activation signals (also matrices).
\item In a CNN, the filter matrices and the parameters of the activation functions are learned from data.
\item Different filters extract different features (patterns) of an image (analogous to SVD). 
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
\item Here is a typical architecture of a CNN:
\end{itemize}

\begin{figure}[!htb]
    \centering
	\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.9\textwidth]{figstats/cnn_architecture}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
\item Images can be expressed by multiple matrices (e.g., color channels).
\end{itemize}

\begin{figure}[!htb]
    \centering
		\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.9\textwidth]{figstats/cnn_channels}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Convolution is a pattern matching operation 
\item In signal processing, convolutions are used to identify if a signal ``looks like" a another reference signal (filter)
\item Procedure generates a filtered image (convolved feature)
\end{itemize}


\begin{figure}[!htb]
    \centering
		\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.6\textwidth]{figstats/cnn_convolution1}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}


\begin{figure}[!htb]
    \centering
		\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.9\textwidth]{figstats/cnn_convolution2}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
\item Pooling is a procedure to compress filtered images into smaller sizes
\end{itemize}

\begin{figure}[!htb]
    \centering
		\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.7\textwidth]{figstats/cnn_pooling}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Convolutional Neural Networks}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Subsequent convolutions of the image generate output features that are used by a standard NN to perform predictions
\item This NN layer is called the fully connected layer
\end{itemize}

\begin{figure}[!htb]
    \centering
		\href{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}{\includegraphics[width=0.7\textwidth]{figstats/cnn_fullyconnected}}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Classifying Contaminants \footnotesize{\texttt{cnn\_contaminants.m}}}

\begin{itemize}
\item Here are some example images of a sensor exposed to DMMP and Water 
\item Have a total set of $S=500$ for training 
\end{itemize}

\begin{figure}[!htb]
    \centering
\includegraphics[width=0.6\textwidth]{figstats/Matlab/CNN/images_contaminants}
\end{figure}

\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}{Example: Classifying Contaminants \footnotesize{\texttt{cnn\_contaminants.m}}}

\begin{itemize}
\item Here are the filtered images (activations) obtained from filters for DMMP
\end{itemize}

\begin{figure}[!htb]
    \centering
\includegraphics[width=0.7\textwidth]{figstats/Matlab/CNN/images_contaminants_activations_water}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Classifying Contaminants \footnotesize{\texttt{cnn\_contaminants.m}}}

\begin{itemize}
\item Here are the filtered images (activations) obtained from filters for Water
\end{itemize}

\begin{figure}[!htb]
    \centering
\includegraphics[width=0.7\textwidth]{figstats/Matlab/CNN/images_contaminants_activations_dmmp}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}{Example: Classifying Contaminants \footnotesize{\texttt{cnn\_contaminants.m}}}

\begin{itemize}
\item Here is the confusion matrix obtained from classification for an independent set of images (accuracy of 77\%)
\end{itemize}

\begin{figure}[!htb]
    \centering
\includegraphics[width=0.6\textwidth]{figstats/Matlab/CNN/images_contaminants_confusion}
\end{figure}

\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%
\section{Decision-Making Under Uncertainty}
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%

%------------------------------------------------
%
\begin{frame}{Decision-Making under Uncertainty}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item We have learned to use data to model uncertainty as RVs and to build models to capture connections between RVs. 
\item We have also learned how to characterize uncertainty for functions of RVs using Monte Carlo simulations and RV transformations. 

\item We now shift our attention on how to use these capabilities to {\em make decisions}. 

\item Fundamental issue with making decisions under uncertainty is that humans take different attitudes towards risk and tend to severely under/overestimate uncertainty. Also, ``risk" means different things to different people. 

\item Consider setting in which we would like to chose between decisions $u$ and $u'$ with associated univariate RVs $Y(u)$ and $Y(u')$ (e.g., cost). 

\item Another setting is that in which we want to find a decision $u$ that manipulates RV $Y(u)$ in a particular way (e.g., it maximizes it or reduces its variability). 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Decision-Making under Uncertainty}

Consider now the following questions:

\begin{block}{}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item What is risk? How do I measure risk?
\item How can I make a decision that withstands uncertainty?
\item How can I take optimal proactive actions in the face of uncertainty?
\end{itemize}
\end{block}
Concepts and techniques that answer these questions are studied in the area of {\em stochastic optimization} (a.k.a. stochastic programming). 


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Defining and Measuring Risk}

At this point, we have all fundamentals of statistics needed to properly define risk and figure out how to measure it.

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Consider an input $X$ with pdf $f_X(x)$ and cdf $F_X(x)$ and utility function $\varphi(X,u)$ that depends on the input RV and a decision $u$. 

\item Since $X$ is an RV, utility is also an RV that we represent as univariate RV $Y(u)=\varphi(X,u)$ with pdf $f_{Y(u)}(y)$ and cdf $F_{Y(u)}(y)$. 

\item The pdf and cdf of $Y(u)$ depend on the decision $u$; consequently, $u$ can be used to manipulate these and to manipulate the statistics of $Y(u)$. 

\item In popular culture, risk is associated with a decision or situation (e.g., ``this is a risky investment", ``the reactor is operating at risky conditions") and is associated with extreme events or large/catastrophic losses.    

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Defining and Measuring Risk}

\begin{itemize}
   \setlength{\itemsep}{10pt}
\item When making decisions, however, we want precise measures that tell us exactly what constitutes a risky decision and how risky it is. 

\item Unfortunately, there is no unique mathematical definition of risk because humans tend to value different aspects of  uncertainty (e.g., probability of failure vs. magnitude of failure vs. worst-case failure).  

\item Moreover, humans tend to disagree on what probability levels are acceptable (e.g., what seems risky to me might not seem risky to you).

\item Disagreements arise because decision-makers (DMs) take different risk attitudes. 

\item As a result, what we want to explore here is not what risk is but rather what {\em definitions} of risk exist. 

\item Ultimately, establishing a definition of risk for a particular situation at hand should be based on consensus between DMs. 

\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Defining and Measuring Risk \footnotesize{\texttt{mean\_variance\_comparison.m}}}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Consider options $Y(u_1)$ and $Y(u_2)$  
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item $Y(u_1)$ has low mean cost but high variance 
\item $Y(u_2)$ has high mean cost and but low variance
\end{itemize}
\item Which one is better?
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/mean_var_options.eps}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Defining and Measuring Risk}

To motivate potential definitions of risk that we might consider, we explore different attitudes towards risk that DMs might take: 

\begin{block}{}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item {\em Risk-Neutral}: DM prefers $Y(u)$ over $Y(u')$ if $\mathbb{E}_{Y(u)}\leq \mathbb{E}_{Y(u')}$.  

\begin{itemize}
\item This DM worries about performance on average and is not concerned with the fact that $Y(u)$ might have outcomes with large values compared to those of $Y'(u)$. 
\end{itemize}
\item {\em Risk-Conscious}: DM prefers $Y(u)$ over $Y(u')$ if $\mathbb{P}(Y(u)\leq y)\geq \mathbb{P}(Y(u')\leq y)$.  

\begin{itemize}
\item This DM wants a decision that will likely achieve lower outcomes. This type of DM might also prefer $\mathbb{V}_{Y(u)}\leq \mathbb{V}_{Y(u')}$ because $u$ has lower variability than $u'$.
\end{itemize}
\item {\em Risk-Averse}: DM prefers $Y(u)$ over $Y(u')$ if $\max  Y(u)\leq \max Y(u')$.  

\begin{itemize}
\item This DM only worries about the worst possible outcome of $u$ and $u'$.
\end{itemize}
\item {\em Risk-Taker}: DM prefers $Y(u)$ over $Y(u')$ if $\min Y(u)\leq \min Y(u')$.  

\begin{itemize}
\item This DM only worries about the best possible outcome of $u$ and $u$.
\end{itemize}

\end{itemize}
\end{block}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Defining and Measuring Risk \footnotesize{\texttt{mean\_variance\_comparison.m}}}
\begin{itemize}
\item Which option to select if you are risk-neutral, risk-conscious, risk-averse, or risk-taker DM?
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/mean_var_options_pdf_cdf.eps}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Defining and Measuring Risk}
\begin{block}{}
So, how do we measure risk? 
\end{block}

A risk measure is a function $\rho(Y(u))$ that maps a univariate RV $Y(u)$ to a scalar quantity (it is a summarizing statistic). Common risk measures used in practice are:
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Expected Value: $\mathbb{E}[Y(u)]$
\item Variance: $\mathbb{V}[Y(u)]=\mathbb{E}[(Y(u)-\mathbb{E}[Y(u)])^2]$
\item Mean-Variance: $\mathbb{MV}_\kappa[Y(u)]=\mathbb{E}[Y(u)]+\kappa \mathbb{V}[Y(u)]$ (for some $\kappa$)
\item Probability of Loss: $\mathbb{P}(Y(u)>y)$ or $\mathbb{P}(Y(u)\leq y)$ (a.k.a. probability of failure)
\item Value-at-Risk: $\mathbb{Q}_{Y(u)}(\alpha)$ (also written as $\textrm{VaR}_\alpha$)
\item Conditional Value-at-Risk: $\mathbb{E}[Y(u)|Y(u)\geq \textrm{VaR}_\alpha]$ (a.k.a. expected short fall) 
\item Mean Deviation:  $\mathbb{E}[|Y(u)-\mathbb{E}[Y(u)]|]$
\item Worst/Best-Case: $\max Y(u)$ or $\min Y(u)$
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Defining and Measuring Risk}

Some observations:
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Different measures are used to model different risk attitudes. 

\item Probability of loss is what is colloquially known as risk because it has a natural interpretation that DMs find useful in practice. This measure has important its caveats. 

\item In principle, we have the freedom of using any statistic of $Y(u)$ (e.g., moments, entropy) as risk measure but, as expected, not all measures are expected to be adequate.   
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Defining and Measuring Risk \footnotesize{\texttt{mean\_variance\_comparison.m}}}

For previous example we have:
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.6\textwidth]{figstats/Matlab/mean_var_options_pdf_cdf.eps}
\end{figure}
\vspace{-0.1in}
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item Expected Value: $\mathbb{E}[Y(u_1)]=3$, $\mathbb{E}[Y(u_2)]=4$
\item Variance: $\mathbb{V}[Y(u_1)]=2.2$, $\mathbb{V}[Y(u_2)]=0.2$
\item Mean-Variance: $\mathbb{MV}_\kappa[Y(u_1)]=5.2$, $\mathbb{MV}_\kappa[Y(u_2)]=4.2$ for $\kappa=1$
\item Probability of Loss: 
\begin{itemize}
\item $\mathbb{P}(Y(u_1)>y)=9\%$, $\mathbb{P}(Y(u_2)> y)=2\%$  for $y=5$
\item $\mathbb{P}(Y(u_1)>y)=2\%$, $\mathbb{P}(Y(u_2)> y)=5\%$  for $y=4$
\end{itemize}
\item Value-at-Risk: $\mathbb{Q}_{Y(u_1)}(\alpha)=4.92$, $\mathbb{Q}_{Y(u_2)}(\alpha)=4.64$ for $\alpha=0.9$
\item Conditional Value-at-Risk: $\textrm{CVaR}_\alpha[Y(u_1)]=5$, $\textrm{CVaR}_\alpha[Y(u_2)]=4.9$  for $\alpha=0.9$
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Stochastic Dominance}
\begin{block}{}
So, what are desirable features of a risk measure? What is an adequate risk measure?
\end{block}
\begin{itemize}
\item To answer these questions, it is necessary to first discuss what makes an RV better than another RV (how do we compare RVs?). 

\item An important concept that arises here is that of {\em stochastic dominance} (SD).

\end{itemize}

\begin{block}{}
We say that $Y$ stochastically dominates $Y'$ (written as $Y\preceq Y'$) if:
\begin{align*}
\mathbb{P}(Y\leq y)\geq \mathbb{P}(Y'\leq y)\; \textrm{for all}\;  y\in \mathcal{D}
\end{align*}
where $\mathcal{D}$ is the domain of $Y$ and $Y'$. 
\end{block}
Some observations:
\begin{itemize}
   \setlength{\itemsep}{10pt}
\item SD says that $Y$ is better (dominates) $Y'$ if probability of $Y$ taking a value less than a threshold $y$ is higher than probability of $Y'$ being below same threshold. Moreover, the probability is higher or equal for any threshold value. 
\item Can also express SD as $F_{Y}(y)\geq F_{Y'}(y)$ for all $y\in\mathcal{D}$. We thus have that cdf curve of $Y$ is always above that of $Y'$. 

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Stochastic Dominance}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Note how SD is different than the traditional dominance concept we are familiar with (a decision dominates another one if it is {\em always} better). 

\item In statistical terms, this would imply that outcomes of $Y$ are always lower or equal than those of $Y'$:
\begin{align*}
y_{\omega}\leq y'_\omega,\quad \omega \in \Omega
\end{align*}
\item This requirement is strict and it is unlikely to occur in practice. SD is a more flexible requirement. 
\item A risk-conscious DM would require that $\mathbb{P}(Y\leq y)\geq \mathbb{P}(Y'\leq y)$ holds {\em for a single} $y$. 

\item Note that SD is a stricter requirement because it requires $\mathbb{P}(Y\leq y)\geq \mathbb{P}(Y'\leq y)$ to hold {\em for all possible} $y$. 

\item As such, there are weakened (relaxed) versions of SD that are also often used in practice (e.g., holds {\em for some} $y$).  
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Stochastic Dominance \footnotesize{\texttt{mean\_variance\_comparison.m}}}

\begin{itemize}
\item Does $Y(u_1)$ dominate $Y(u_2)$ (i.e., $Y(u_1)\preceq Y(u_2)$)? 
\end{itemize}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/mean_var_options_pdf_cdf.eps}
\end{figure}
\pause 
\begin{itemize}
\item No, $Y(u_1)$ dominates $Y(u_2)$ only for some values of threshold $y$.
\end{itemize}


\end{frame}
%------------------------------------------------



%------------------------------------------------
%
\begin{frame}{Coherency Properties of Risk Measures}

There are a number of fundamental properties that an adequate (a.k.a. coherent) risk measure should satisfy. These properties have been proposed based on their usefulness in actual practical applications and on mathematical consistency. 
\begin{block}{}
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Translation Invariance: $\rho(Y+c)=\rho(Y)+c$ for $c\in \mathbb{R}$. 
\begin{itemize}
\item Adding a constant value to an RV should result in adding same constant to the risk measure (i.e., adding a constant does not alter inherent properties of RV).
\end{itemize}
\item Subadditivity: $\rho(Y+Y')\leq \rho(Y)+\rho(Y')$. 
\begin{itemize}
\item The risk of a combined pair of RVs cannot exceed sum of individual risks. 
\end{itemize}
\item Monotonicity:  If $Y\preceq Y'$ then $\rho(Y)\leq \rho(Y')$. 
\begin{itemize}
\item If $Y$ stochastically dominates $Y'$, then its risk should also be lower. This indicates that risk measure reflects dominance. 
\end{itemize}
\item Positive Homogeneity: $\rho(c\cdot Y)=c\rho(Y)$ for $c\in \mathbb{R}_+$. 
\begin{itemize}
\item Scaling RV by a constant does not affect inherent properties of RV.
\end{itemize}
\end{itemize}
\end{block}
We will see later that some of these properties resemble those of vector norms. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Risk Measures as Norms}

The risk measure of an RV is analogous to a norm of a vector. To see this, we ask the question:
\begin{block}{}
How can we say that vector $\mathbf{y}=(y_1,y_2,...,y_S)$ is better than $\mathbf{y}'=(y_1',y_2',...,y_S')$?
\end{block}
This reveals similar difficulties that arise when comparing RVs. Consider:
\begin{itemize}

\item We can say that $\mathbf{y} $ is better than $\mathbf{y}'$ if the total magnitude of its entries is lower ($\sum_{i=1}^Sy_i\leq \sum_{i=1}^Sy_i'$). This can be written as $(1/S)\sum_{i=1}^Sy_i\leq (1/S)\sum_{i=1}^Sy_i'$  and is analogous to expected value.

\item We can say that $\mathbf{y} $ is better than $\mathbf{y}'$ if $\max_i\; y_i\leq \max_i\; y_i'$. This is analogous to worst-case risk measure. 

\item  We can  say that $\mathbf{y}$ is better than $\mathbf{y}'$ if $y_i\leq y_i'$ for all $i=1,...,S$. This is analogous to say that $Y$ is always better than $Y'$.  Note, however, that the entries are not arranged in order so maybe an entry of $\mathbf{y}$ is much larger than any entry of $\mathbf{y}'$.  

\item Consequently, perhaps a better approach would be arranging entries in decreasing order and then compare the rearranged entries.  This is analogous to stochastic dominance (we establish a threshold on the magnitude of the entries) and counts how many entries of the vectors are below that threshold value.  

\end{itemize}



\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Risk Measures as Norms}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Recall that the norm $\rho(\mathbf{y})$ of a vector $\mathbf{y}$ is a measure of its size. Norms are used to compare vectors and to establish bounding properties.   

\item Recall that most used vector norm is $\ell-p$ norm:
\begin{align*}
\|\mathbf{y}\|_p=\left(\sum_{i=1}^S|y_i|^p\right)^{1/p}.
\end{align*}
This norm has the following special cases:
\begin{itemize}
\setlength{\itemsep}{10pt}
\item For $p=1$, $\|\mathbf{y}\|_1=\sum_{i=1}^S|y_i|$ and note that this is $S$ times the average magnitude of the entries (analogous of expected value). 
\item For $p=\infty$, $\|\mathbf{y}\|_\infty=\max_{i} |y_i|$ and note this is largest value (analogous of worst-case). 
\end{itemize}
\item As in the case of risk measures, one can define many types of norms to measure and compare vectors in different forms. 

\item For instance, an interesting norm is $k$-max norm. This norm is the sum of $k$-largest elements of vector $\mathbf{y}$. This norm is analogous to expected shortfall.   
\end{itemize}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Risk Measures as Norms}

As with risk measures, one must ensure that any norm that we define satisfy basic properties.  A {\em proper} norm of a vector must satisfy the following properties:

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Homogeneity: $\rho(c\cdot \mathbf{y})=c\cdot \rho(\mathbf{y})$ for $c\in \mathbb{R}_+$
\item Subadditivity (Triangle Inequality): $\rho(\mathbf{y}+\mathbf{y}')\leq \rho(\mathbf{y})+\rho(\mathbf{y}')$
\item Normalization: $\rho(0)=0$.
\end{itemize}
The homogeneity property is analogous to that of a proper risk measure. Moreover, the triangle inequality is analogous to the  subadditivity property. 

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Risk Measures as Norms}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Consider vectors $y_1=(100,70,50,20,10)$ and $y_2=(110,80,40,10,0)$

\item What is mean and worst-case for such vectors?
\begin{itemize}
\setlength{\itemsep}{10pt}
\item Mean is $(1/5)\cdot (100+70+50+20+10)=50$ and $(1/5)\cdot(110+80+40+10+0)=48$
\item Worst-Case is $100$ and $110$
\end{itemize}
\item What is CVaR for such vectors? 
\begin{itemize}
\setlength{\itemsep}{10pt}
\item If $\alpha=5/5$ then $(1/5)\cdot (100+70+50+20+10)=50$ and $(1/5)\cdot(110+80+40+10+0)=48$
\item If $\alpha=4/5$ then $(1/4)\cdot (100+70+50+20)=60$ and $(1/4)\cdot(110+80+40+10)=60$
\item If $\alpha=3/5$ then $(1/3)\cdot (100+70+50)=73$ and $(1/3)\cdot(110+80+40)=76$
\item If $\alpha=1/5$ then $(1/1)\cdot (100)=100$ and $(1/1)\cdot(110)=60$
\end{itemize}
\item We thus have that CVaR has mean and worst-case as extremes.
\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Expected Value Properties}

We now have all elements needed to judge whether a risk measure is adequate or not.  We begin by discussing the properties of expected value $\mathbb{E}[Y]$.

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Expected value is a measure of the magnitude of $Y$ (i.e., $\mathbb{E}[Y]\leq \mathbb{E}[Y']$ indicates that $Y$ is, on average, smaller than $Y'$) and therefore has a natural intuitive interpretation.  Because of this, this is a common measure used in practice. 

\item Expected value is a coherent risk measure.

\item Expected value ignores outcomes with extreme values. For instance, even if $\mathbb{E}[Y]\leq \mathbb{E}[Y']$ holds, $Y$ can have outcomes of large magnitude that $Y'$ does not have. 

\end{itemize}


\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Variance Properties}


We now discuss the properties of variance $\mathbb{V}[Y]=\mathbb{E}[(Y-\mathbb{E}[Y])^2]$.

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Variance is a measure of variability (i.e., $\mathbb{V}[Y]\leq \mathbb{V}[Y']$ indicates that $Y$ has less variability than $Y'$) and therefore has a natural and intuitive interpretation.  

\item An issue with the variance is that it does not capture magnitude of RV. To remedy this issue, variance is often used in conjunction with expected value by using mean-variance measure $\mathbb{E}[Y]+\kappa\mathbb{V}[Y]$ where $\kappa$ is a weighting vector that helps span a range of attitudes towards risk (from risk neutral to risk conscious). 

\item Mean-variance was proposed by Harry Markowitz in the 1950s and became the standard in the finance industry for many years (Markowitz earned a Nobel prize). 

\item Variance takes into consideration outcomes of large values (in upper tail of pdf) but this connection is not direct and this results in several important inefficiencies.   Specifically, the variance is centered around the expected value and penalizes deviations from it symmetrically. 

\item Symmetry is undesirable in applications because we are often only interested in accounting for outcomes of large magnitude (and not with those of low magnitude).  Moreover, in many applications,  RV does not have a symmetric pdf and therefore variance can fail to properly capture tail effects.  

\end{itemize}
. 


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Variance Properties}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Importantly, the variance is not a proper risk measure. Specifically, it does not satisfy monotonicity (it is not consistent with SD). 

\item The variance and expected value do not have the same units. Consequently, it is often preferred to use the standard deviation $\mathbb{SV}[Y]=\sqrt{\mathbb{V}[Y]}$. 

\item The variance remains widely used in industry because it is an easily-interpretable measure of variability and uncertainty. This measure, however, has important deficiencies. 

\end{itemize}


\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Expected Shortfall Properties}


The expected shortfall overcomes many of deficiencies of the variance and has recently become the standard risk measure.  This measure has many important properties and connections with other risk measures that are worth highlighting.  

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Recall that the expected shortfall of $Y$ at probability $\alpha$ is $\mathbb{E}[Y|Y\geq \mathbb{Q}(\alpha)]$, where $\mathbb{Q}(\alpha)$ is the $\alpha$-quantile of $Y$. 

\item Expected loss takes expected value losses above quantile $\mathbb{Q}(\alpha)$. The  $\alpha$-quantile is threshold value $t$ at which $\mathbb{P}(Y\leq t)=\alpha$. 

\item Expected loss captures magnitude of outcomes of high value while ignoring those of small magnitude (it is an asymmetric risk measure). This becomes obvious if we write the expected loss as $\mathbb{E}[(Y-\mathbb{Q}(\alpha))_+]$. 

\end{itemize}



\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Expected Shortfall Properties}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/cvar_diag}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Expected Shortfall Properties}

\begin{itemize}
\setlength{\itemsep}{10pt}
\item If we set $\alpha=0$, quantile $\mathbb{Q}(\alpha)$ is minimum value of $Y$ and therefore the expected loss is the expected value $\mathbb{E}[Y]$. 

\item If we set $\alpha=1$, quantile is maximum value and therefore the expected loss is the worst-case value $\max Y$.

\item Consequently, expected loss captures a range of risk attitudes (from risk neutral to conscious to averse).

\item Expected loss is a coherent risk measure. 

\item A caveat of the expected loss is that it offers no direct control on the probability of loss, which is a measure of interest to many DMs. In other words, reducing the expected loss does not necessarily imply reducing the probability of loss. 

\item Moreover, DM needs to specify $\alpha$ and this selection can lead to disagreement. 

\end{itemize}



\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Probability of Loss Properties}


Probability of loss is one of most widely used measures of risk.  

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Recall that the probability of loss is simply $\mathbb{P}(Y>y)$ and this is also often expressed as probability of no loss as $\mathbb{P}(Y\leq y)=1-\mathbb{P}(Y>y)$. 

\item What constitutes a loss is defined by threshold value $y$. Consequently, it is more appropriate to call this measure the  ``probability of unacceptable loss". 

\item Meaning of probability of loss is intuitive. 

\item Biggest caveat of probability of loss is that it says nothing about actual magnitude of the losses.  For example, even if $\mathbb{P}(Y>y)\leq \mathbb{P}(Y'>y)$, one might still prefer $Y'$ because losses incurred are not that large compared to those of $Y$.  In other words, $Y'$ is less catastrophic than $Y$. 

\item Probability of loss is not coherent risk measure. 

\item Finally, one needs to specify a threshold value to express what constitutes an unacceptable loss, and this selection often leads to disagreement of DMs.

\end{itemize}

\end{frame}


%------------------------------------------------
%
\begin{frame}{Risk Measures}

The different advantages and disadvantages encountered with risk measures highlight the difficulty in controlling and comparing RVs. In particular:

\begin{itemize}
\setlength{\itemsep}{10pt}
\item Risk measures are often {\em conflicting} (e.g., reducing one measure often results in increasing another measure). 
\item Measuring risk leads to {\em ambiguity} (expressing mathematically what a DM might be looking for is challenging). This is analogous to measuring happiness or fairness. 
\end{itemize}
Despite these limitations, risk measures are an essential component of decision-making under uncertainty.

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Investment Options \footnotesize{\texttt{npv\_comparison.m}}}
\begin{itemize}
\item We have investment options:
\begin{itemize}
\item Initial capital $C_1=\$25,000$ with annual rate of return $R_1\sim \mathcal{N}(0.5,0.1)$
\item Initial capital $C_2=\$50,000$ with annual rate of return $R_2\sim \mathcal{N}(0.4,0.2)$
\end{itemize}
\item We compare net present value for both options (10 yr, interest rate of 5\%):
\begin{align*}
NPV_1=C_1 - R_1\cdot C_1\cdot \frac{(1-(1+i)^{-10})}{i}\\
NPV_2=C_2 - R_2\cdot C_2\cdot \frac{(1-(1+i)^{-10})}{i}.
\end{align*}
\item Note we want NPV to be as {\em negative} as possible. 
\end{itemize}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.5\textwidth]{figstats/npv_diag}
\end{figure}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Investment Options \footnotesize{\texttt{npv\_comparison.m}}}

\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{figstats/Matlab/comparison_NPV}
\end{figure}

\end{frame}
%------------------------------------------------


%------------------------------------------------
%
\begin{frame}{Making Optimal Decisions}
Instead of comparing decisions, a DM might also want to directly find best decision possible. Such a decision is also influenced by the attitude towards risk and gives rise to different optimization problems (depending on the risk measure used):
\begin{block}{}
\begin{itemize}
\item Expected value and variance:
\begin{align*}
u^*=\mathop{\textrm{argmin}}_u\; \mathbb{E}[Y(u)]+ \kappa \mathbb{V}[Y(u)]
\end{align*}
\item Probability of loss
\begin{align*}
u^*=\mathop{\textrm{argmin}}_u\; \mathbb{P}(Y(u)>y)
\end{align*}
\item Expected shortfall: 
\begin{align*}
u^*=\mathop{\textrm{argmin}}_u\; \mathbb{E}[Y(u)\,|\,Y(u)\geq \mathbb{Q}(\alpha)]
\end{align*}
\end{itemize}
\end{block}
Another possibility is to find a decision that dominates a given benchmark. In other words, we seek $u$ such that $Y(u)\preceq Y'$ for a given $Y'$.  An issue with this approach is that the problem might have no solution. 
\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Deterministic Decision-Making}
DMs often make decisions in real-life by ignoring uncertainty all-together. This {\em deterministic} DM approach follows the logic:
\begin{block}{}
\begin{itemize}
\item Assume input $X$ takes a single value $x={\mathbb{E}}[X]$ (i.e., average historical value). This assumes that input is deterministic and thus output $Y$ is deterministic with value:
\begin{align*}
y=\varphi(x,u)
\end{align*}
\item Based on this assumed behavior, we can obtain a decision: 
\begin{align*}
u_D^*=\mathop{\textrm{argmin}}_{u}\varphi (x,u)
\end{align*}
\end{itemize}
\end{block}
\begin{itemize}
\item The assumed deterministic behavior simplifies the decision-making process (removes ambiguity) but fails to ignore the inherent variability of $X$ seen in real-life. 

\item Decision $u_D^*$ might not be capable of controlling real output $Y(u_D^*)$, which is characterized by outcomes $y_\omega=\varphi(x_\omega,u_D^*),\; \omega \in \mathcal{S}$. 

\item As a result, decision $u_D^*$ might be vulnerable to uncertainty and incur large losses. 
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%
\begin{frame}{Example: Deterministic vs. Stochastic Solutions \footnotesize{\texttt{stoch\_prog\_example.m}}}

\begin{itemize}
\setlength{\itemsep}{5pt}
\item We consider the minimization of the cost function:
\begin{align*}
\varphi(u,X)=(u-X)^2-e^{\alpha X}
\end{align*}
with  input uncertainty $X\in \mathcal{N}(2,1)$
\item We obtain solution using deterministic approach, which finds decision by solving:
\begin{align*}
u_D^*=\mathop{\textrm{argmin}}_{u}\varphi (\mathbb{E}[X],u)
\end{align*}
\item We obtain solution using stochastic approach, which finds decision by solving:
\begin{align*}
u^*=\mathop{\textrm{argmin}}_u\; \mathbb{Q}_{\varphi(u,X)}(\alpha)
\end{align*}
\item Pdfs and cdfs for costs $\varphi(u^*_D,X)$ and $\varphi(u^*,X)$ are shown below:
\end{itemize}
\vspace{-0.1in}
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{figstats/Matlab/det_stoch_comp}
\end{figure}

\end{frame}
%------------------------------------------------

%\bibliography{statsbook.bib} 

\end{document}
